{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation already completed.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "    🗿 READ ME 🗿\n",
    "    - This Only Needs To Run Once.\n",
    "    - This Needs to Restart the Kernel after Installing the Dependencies.  \n",
    "    - To Avoid Unintended Restart: This adds an init.flag file in root folder after successful installation.\n",
    "    - To Rerun: Delete init.flag file in root folder.\n",
    "    \n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\"\"\"\n",
    "# Import OS Dependency\n",
    "import os, subprocess, sys\n",
    "\n",
    "# Check If This Already Runned Once\n",
    "if os.path.exists(\"init.flag\"):\n",
    "    print(\"Installation already completed.\")\n",
    "else:\n",
    "    print(\"Starting Installation...\")\n",
    "\n",
    "    # Ensure pip is Installed\n",
    "    subprocess.check_call([sys.executable, '-m', 'ensurepip'], shell=True)\n",
    "\n",
    "    # Install Main Dependencies\n",
    "    subprocess.check_call(f'python -m pip install nltk \"ffmpeg-python\" \"openai-whisper\" pandas pytubefix bertopic \"scikit-learn\"', shell=True)\n",
    "    \n",
    "    # Install Other Dependencies\n",
    "    subprocess.check_call(f'python -m pip install torch tqdm', shell=True)\n",
    "    \n",
    "    # Install BERTopic Dependencies\n",
    "    subprocess.check_call(f'python -m pip install \"numpy<2\" \"tf-keras\"', shell=True)\n",
    "\n",
    "    # Add Flag File to Set that this Already Runned Once\n",
    "    open(\"init.flag\", 'w').close()\n",
    "\n",
    "    # Restart the Kernel to Load Installed Dependences\n",
    "    os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\MSI\n",
      "[nltk_data]     Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\MSI Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\MSI\n",
      "[nltk_data]     Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Main Dependencies\n",
    "import os, re, nltk, ffmpeg, whisper\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pytubefix import YouTube, Stream\n",
    "from pytubefix.cli import on_progress\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import Other Dependencies\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Additional Downloads\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename: str) -> str:\n",
    "    # Escape Double Quotes\n",
    "    filename = filename.replace('\"', '\\\\\"')\n",
    "\n",
    "    # Replace Invalid Characters with \"_\"\n",
    "    invalid_chars = re.compile(r'[<>:\"/\\\\|?*]')\n",
    "    sanitized_filename = invalid_chars.sub(\"_\", filename)\n",
    "\n",
    "    return sanitized_filename\n",
    "def read_items_from_file(file: str) -> list:\n",
    "    with open(file, \"r\") as f:\n",
    "        return f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Set Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Michigan': ['Southgate',\n",
       "  'Wayne',\n",
       "  'Escanaba',\n",
       "  'Zeeland',\n",
       "  'Taylor',\n",
       "  'South Lyon',\n",
       "  'Grosse Pointe Farms',\n",
       "  'Eaton Rapids',\n",
       "  'Belding',\n",
       "  'Kalamazoo',\n",
       "  'St. Clair',\n",
       "  'Highland Park',\n",
       "  'Okemos',\n",
       "  'Mason',\n",
       "  'Ecorse',\n",
       "  'Davison',\n",
       "  'Ludington',\n",
       "  'Muskegon Heights',\n",
       "  'Muskegon',\n",
       "  'Midland',\n",
       "  'Hillsdale',\n",
       "  'Haslett',\n",
       "  'Marshall',\n",
       "  'Kentwood',\n",
       "  'Wixom',\n",
       "  'Adrian',\n",
       "  'Dearborn',\n",
       "  'East Lansing',\n",
       "  'Huntington Woods',\n",
       "  'Pontiac',\n",
       "  'Sterling Heights',\n",
       "  'Rockford',\n",
       "  'Farmington Hills',\n",
       "  'Jenison',\n",
       "  'Grand Ledge',\n",
       "  'Hazel Park',\n",
       "  'Petoskey',\n",
       "  'Woodhaven',\n",
       "  'Eastwood',\n",
       "  'Flint',\n",
       "  'St. Clair Shores',\n",
       "  'Comstock Park',\n",
       "  'Alma',\n",
       "  'Grand Rapids',\n",
       "  'Ironwood',\n",
       "  'Lincoln Park',\n",
       "  'Manistee',\n",
       "  'East Grand Rapids',\n",
       "  'Clawson',\n",
       "  'St. Louis',\n",
       "  'Dearborn Heights',\n",
       "  'Byron Center',\n",
       "  'Traverse City',\n",
       "  'Marysville',\n",
       "  'Rochester Hills',\n",
       "  'Benton Harbor',\n",
       "  'Waverly',\n",
       "  'Kingsford',\n",
       "  'Bay City',\n",
       "  'Mount Clemens',\n",
       "  'Fenton',\n",
       "  'Fraser',\n",
       "  'Lambertville',\n",
       "  'Cadillac',\n",
       "  'Holt',\n",
       "  'Westland',\n",
       "  'Sault Ste. Marie',\n",
       "  'Wyandotte',\n",
       "  'Ann Arbor',\n",
       "  'Buena Vista',\n",
       "  'Brighton',\n",
       "  'Freeland',\n",
       "  'Northville',\n",
       "  'Farmington',\n",
       "  'Monroe',\n",
       "  'Springfield',\n",
       "  'Temperance',\n",
       "  'Walled Lake',\n",
       "  'Three Rivers',\n",
       "  'Wyoming',\n",
       "  'Walker',\n",
       "  'Melvindale',\n",
       "  'Albion',\n",
       "  'Richmond',\n",
       "  'Romulus',\n",
       "  'Riverview',\n",
       "  'Saline',\n",
       "  'Grandville',\n",
       "  'Hastings',\n",
       "  'Niles',\n",
       "  'Hamtramck',\n",
       "  'Allendale',\n",
       "  'Grosse Pointe Park',\n",
       "  'Madison Heights',\n",
       "  'Berkley',\n",
       "  'South Monroe',\n",
       "  'Ferndale',\n",
       "  'Troy',\n",
       "  'Comstock Northwest',\n",
       "  'Coldwater',\n",
       "  'Grosse Pointe',\n",
       "  'Grosse Pointe Woods',\n",
       "  'Oak Park',\n",
       "  'Howell',\n",
       "  'Livonia',\n",
       "  'Ionia',\n",
       "  'Lapeer',\n",
       "  'Burton',\n",
       "  'Portage',\n",
       "  'Shields',\n",
       "  'Milford',\n",
       "  'St. Johns',\n",
       "  'Cutlerville',\n",
       "  'Mount Pleasant',\n",
       "  'Menominee',\n",
       "  'Fair Plain',\n",
       "  'Roseville',\n",
       "  'Eastpointe',\n",
       "  'Southfield',\n",
       "  'Trenton',\n",
       "  'Tecumseh',\n",
       "  'Whitmore Lake',\n",
       "  'Inkster',\n",
       "  'Greenville',\n",
       "  'Forest Hills',\n",
       "  'Royal Oak',\n",
       "  'Swartz Creek',\n",
       "  'Charlotte',\n",
       "  'Beverly Hills',\n",
       "  'Northview',\n",
       "  'Novi',\n",
       "  'Auburn Hills',\n",
       "  'Detroit',\n",
       "  'Lansing',\n",
       "  'St. Joseph',\n",
       "  'Grand Haven',\n",
       "  'Beecher',\n",
       "  'Houghton',\n",
       "  'Alpena',\n",
       "  'Westwood',\n",
       "  'Plymouth',\n",
       "  'Owosso',\n",
       "  'New Baltimore',\n",
       "  'Holly',\n",
       "  'Grand Blanc',\n",
       "  'Big Rapids',\n",
       "  'Center Line',\n",
       "  'Milan',\n",
       "  'Birmingham',\n",
       "  'Garden City',\n",
       "  'Ishpeming',\n",
       "  'River Rouge',\n",
       "  'Ypsilanti',\n",
       "  'Marquette',\n",
       "  'Iron Mountain',\n",
       "  'Flat Rock',\n",
       "  'Battle Creek',\n",
       "  'Norton Shores',\n",
       "  'Port Huron',\n",
       "  'Dowagiac',\n",
       "  'Allen Park',\n",
       "  'Holland',\n",
       "  'Warren',\n",
       "  'Hudsonville',\n",
       "  'Sturgis',\n",
       "  'Rochester',\n",
       "  'Lake Fenton',\n",
       "  'Saginaw',\n",
       "  'Harper Woods',\n",
       "  'Bridgeport',\n",
       "  'Flushing',\n",
       "  'Jackson'],\n",
       " 'Arizona': ['Sahuarita',\n",
       "  'Verde Village',\n",
       "  'Fountain Hills',\n",
       "  'Goodyear',\n",
       "  'Tuba City',\n",
       "  'Cave Creek',\n",
       "  'Surprise',\n",
       "  'Village of Oak Creek (Big Park)',\n",
       "  'Casa Grande',\n",
       "  'Snowflake',\n",
       "  'Glendale',\n",
       "  'Three Points',\n",
       "  'Saddlebrooke',\n",
       "  'Williamson',\n",
       "  'Somerton',\n",
       "  'San Luis',\n",
       "  'Kingman',\n",
       "  'Eloy',\n",
       "  'Winslow',\n",
       "  'Paulden',\n",
       "  'Tempe',\n",
       "  'Holbrook',\n",
       "  'New Kingman-Butler',\n",
       "  'Bisbee',\n",
       "  'Scottsdale',\n",
       "  'Tucson Estates',\n",
       "  'Green Valley',\n",
       "  'Sun City',\n",
       "  'Sierra Vista',\n",
       "  'Avra Valley',\n",
       "  'Page',\n",
       "  'Apache Junction',\n",
       "  'Mesa',\n",
       "  'Bullhead City',\n",
       "  'Prescott',\n",
       "  'Fortuna Foothills',\n",
       "  'San Tan Valley',\n",
       "  'Prescott Valley',\n",
       "  'Catalina Foothills',\n",
       "  'Wickenburg',\n",
       "  'Safford',\n",
       "  'New River',\n",
       "  'Flagstaff',\n",
       "  'Guadalupe',\n",
       "  'Gilbert',\n",
       "  'Kayenta',\n",
       "  'Douglas',\n",
       "  'Yuma',\n",
       "  'Coolidge',\n",
       "  'Sedona',\n",
       "  'Paradise Valley',\n",
       "  'Globe',\n",
       "  'Maricopa',\n",
       "  'Rincon Valley',\n",
       "  'Drexel Heights',\n",
       "  'Golden Valley',\n",
       "  'Queen Creek',\n",
       "  'Sun Lakes',\n",
       "  'Casas Adobes',\n",
       "  'Vail',\n",
       "  'Tanque Verde',\n",
       "  'Anthem',\n",
       "  'Corona de Tucson',\n",
       "  'Oro Valley',\n",
       "  'Doney Park',\n",
       "  'South Tucson',\n",
       "  'Tolleson',\n",
       "  'Picture Rocks',\n",
       "  'Sun City West',\n",
       "  'Flowing Wells',\n",
       "  'Fort Mohave',\n",
       "  'Summit',\n",
       "  'Marana',\n",
       "  'Camp Verde',\n",
       "  'Arizona City',\n",
       "  'Chino Valley',\n",
       "  'Gold Canyon',\n",
       "  'Sierra Vista Southeast',\n",
       "  'Valencia West',\n",
       "  'Benson',\n",
       "  'Show Low',\n",
       "  'Peoria',\n",
       "  'Avondale',\n",
       "  'Tucson',\n",
       "  'Litchfield Park',\n",
       "  'Nogales',\n",
       "  'Cottonwood',\n",
       "  'Chandler',\n",
       "  'Payson',\n",
       "  'Phoenix',\n",
       "  'Catalina',\n",
       "  'Lake Havasu City',\n",
       "  'Florence',\n",
       "  'Youngtown',\n",
       "  'Rio Rico',\n",
       "  'Buckeye',\n",
       "  'El Mirage'],\n",
       " 'Pennsylvania': ['Ellwood City',\n",
       "  'Penn Wynne',\n",
       "  'Blakely',\n",
       "  'Lower Allen',\n",
       "  'Enola',\n",
       "  'Bloomsburg',\n",
       "  'Bethel Park',\n",
       "  'California',\n",
       "  'DuBois',\n",
       "  'Schuylkill Haven',\n",
       "  'Easton',\n",
       "  'Willow Grove',\n",
       "  'Morrisville',\n",
       "  'Palmyra',\n",
       "  'Duquesne',\n",
       "  'Middletown',\n",
       "  'Turtle Creek',\n",
       "  'Taylor',\n",
       "  'Village Green-Green Ridge',\n",
       "  'Pittsburgh',\n",
       "  'Harleysville',\n",
       "  'Ardmore',\n",
       "  'Folsom',\n",
       "  'Kennett Square',\n",
       "  'Hatboro',\n",
       "  'Huntingdon',\n",
       "  'Jefferson Hills',\n",
       "  'Palmerton',\n",
       "  'Erie',\n",
       "  'Swarthmore',\n",
       "  'Croydon',\n",
       "  'Lansdowne',\n",
       "  'Shippensburg',\n",
       "  'Shenandoah',\n",
       "  'Upper St. Clair',\n",
       "  'Milton',\n",
       "  'Bellevue',\n",
       "  'Shanor-Northvue',\n",
       "  'Munhall',\n",
       "  'Bethlehem',\n",
       "  'Tyrone',\n",
       "  'Hanover',\n",
       "  'Phoenixville',\n",
       "  'Beaver Falls',\n",
       "  'Allentown',\n",
       "  'Lewistown',\n",
       "  'Blandon',\n",
       "  'Lansdale',\n",
       "  'Hellertown',\n",
       "  'Kutztown',\n",
       "  'Yeadon',\n",
       "  'State College',\n",
       "  'Clifton Heights',\n",
       "  'Wilkinsburg',\n",
       "  'Arlington Heights',\n",
       "  'Homeacre-Lyndora',\n",
       "  'New Brighton',\n",
       "  'Pottstown',\n",
       "  'Uniontown',\n",
       "  'Dunmore',\n",
       "  'Shamokin',\n",
       "  'Clairton',\n",
       "  'Hazleton',\n",
       "  'Farrell',\n",
       "  'Hollidaysburg',\n",
       "  'Plum',\n",
       "  'Audubon',\n",
       "  'Greensburg',\n",
       "  'Shillington',\n",
       "  'West View',\n",
       "  'Clarion',\n",
       "  'Parkville',\n",
       "  'Oil City',\n",
       "  'Blue Bell',\n",
       "  'Broomall',\n",
       "  'South Williamsport',\n",
       "  'McKees Rocks',\n",
       "  'Butler',\n",
       "  'Oxford',\n",
       "  'Ridley Park',\n",
       "  'Altoona',\n",
       "  'Carnegie',\n",
       "  'Chambersburg',\n",
       "  'Lititz',\n",
       "  'Paxtonia',\n",
       "  'Pleasant Hills',\n",
       "  'Glenshaw',\n",
       "  'Plymouth Meeting',\n",
       "  'Lancaster',\n",
       "  'Swissvale',\n",
       "  'Elizabethtown',\n",
       "  'Pittston',\n",
       "  'Coatesville',\n",
       "  'Monessen',\n",
       "  'East York',\n",
       "  'Levittown',\n",
       "  'Colonial Park',\n",
       "  'White Oak',\n",
       "  'Franklin',\n",
       "  'McKeesport',\n",
       "  'Park Forest Village',\n",
       "  'Oreland',\n",
       "  'Meadville',\n",
       "  'Birdsboro',\n",
       "  'Hermitage',\n",
       "  'Ephrata',\n",
       "  'Moosic',\n",
       "  'Fernway',\n",
       "  'Willow Street',\n",
       "  'Fort Washington',\n",
       "  'Bradford',\n",
       "  'Doylestown',\n",
       "  'Collegeville',\n",
       "  'Grove City',\n",
       "  'King of Prussia',\n",
       "  'Wilkes-Barre',\n",
       "  'Corry',\n",
       "  'Carlisle',\n",
       "  'Horsham',\n",
       "  'Sugarcreek',\n",
       "  'Downingtown',\n",
       "  'Latrobe',\n",
       "  'Lehighton',\n",
       "  'Perkasie',\n",
       "  'York',\n",
       "  'Harrisburg',\n",
       "  'Coraopolis',\n",
       "  'Folcroft',\n",
       "  'Monaca',\n",
       "  'Darby',\n",
       "  'Chester',\n",
       "  'Conshohocken',\n",
       "  'Norwood',\n",
       "  'Quakertown',\n",
       "  'Baldwin',\n",
       "  'Wyomissing',\n",
       "  'Arnold',\n",
       "  'Murrysville',\n",
       "  'Pottsville',\n",
       "  'Sharon',\n",
       "  'Sunbury',\n",
       "  'Swoyersville',\n",
       "  'Whitehall',\n",
       "  'Mount Carmel',\n",
       "  'Reading',\n",
       "  'Trooper',\n",
       "  'Fullerton',\n",
       "  'Brookhaven',\n",
       "  'Lionville',\n",
       "  'Montgomeryville',\n",
       "  'New Cumberland',\n",
       "  'Jeannette',\n",
       "  'Monroeville',\n",
       "  'Hershey',\n",
       "  'Brentwood',\n",
       "  'Titusville',\n",
       "  'Economy',\n",
       "  'Indiana',\n",
       "  'Waynesboro',\n",
       "  'Kulpsville',\n",
       "  'Carbondale',\n",
       "  'Maple Glen',\n",
       "  'Ancient Oaks',\n",
       "  'Glenolden',\n",
       "  'Lock Haven',\n",
       "  'Norristown',\n",
       "  'Souderton',\n",
       "  'Lewisburg',\n",
       "  'Franklin Park',\n",
       "  'New Kensington',\n",
       "  'Allison Park',\n",
       "  'Punxsutawney',\n",
       "  'Emmaus',\n",
       "  'Progress',\n",
       "  'Dormont',\n",
       "  'Old Forge',\n",
       "  'Kingston',\n",
       "  'Greenville',\n",
       "  'Nazareth',\n",
       "  'Northwest Harborcreek',\n",
       "  'Forest Hills',\n",
       "  'Mountain Top',\n",
       "  'Red Lion',\n",
       "  'Lebanon',\n",
       "  'Sanatoga',\n",
       "  'Linglestown',\n",
       "  'Tamaqua',\n",
       "  'Weigelstown',\n",
       "  'Catasauqua',\n",
       "  'Archbald',\n",
       "  'Carnot-Moon',\n",
       "  'Crafton',\n",
       "  'Washington',\n",
       "  'Richboro',\n",
       "  'Somerset',\n",
       "  'Mechanicsburg',\n",
       "  'South Park Township',\n",
       "  'Dickson City',\n",
       "  'East Stroudsburg',\n",
       "  'Johnstown',\n",
       "  'Sayre',\n",
       "  'Wescosville',\n",
       "  'Fox Chapel',\n",
       "  'Shiloh',\n",
       "  'Connellsville',\n",
       "  'Scranton',\n",
       "  'Media',\n",
       "  'Plymouth',\n",
       "  'Clarks Summit',\n",
       "  'Columbia',\n",
       "  'Drexel Hill',\n",
       "  'Steelton',\n",
       "  'Lower Burrell',\n",
       "  'Paoli',\n",
       "  'Woodlyn',\n",
       "  'Oakmont',\n",
       "  'Canonsburg',\n",
       "  'Bellefonte',\n",
       "  'New Castle',\n",
       "  'West Mifflin',\n",
       "  'Edinboro',\n",
       "  'Aliquippa',\n",
       "  'Williamsport',\n",
       "  'New Holland',\n",
       "  'Olyphant',\n",
       "  'Vandergrift',\n",
       "  'Gettysburg',\n",
       "  'Philadelphia',\n",
       "  'Bangor',\n",
       "  'Wyndmoor',\n",
       "  'St. Marys',\n",
       "  'Bristol',\n",
       "  'Millersville',\n",
       "  'West Chester',\n",
       "  'Nanticoke',\n",
       "  'Selinsgrove',\n",
       "  'Warren',\n",
       "  'Bridgeville',\n",
       "  'Northampton',\n",
       "  'Clearfield',\n",
       "  'Mount Joy',\n",
       "  'Wilson',\n",
       "  'Stroudsburg',\n",
       "  'Castle Shannon',\n",
       "  'Fairless Hills',\n",
       "  'Camp Hill',\n",
       "  'Leola',\n",
       "  'Prospect Park',\n",
       "  'Exeter',\n",
       "  'Collingdale',\n",
       "  'Sharon Hill',\n",
       "  'Westmont',\n",
       "  'Ambler',\n",
       "  'Schlusser',\n",
       "  'Glenside',\n",
       "  'Berwick',\n",
       "  'Ambridge']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File Names\n",
    "yt_video_links_filename = \"YouTube Video Links.txt\"\n",
    "transcript_sentences_filename = \"transcript_sentences.csv\"\n",
    "related_transcript_sentences_filename = \"related_transcript_sentences.csv\"\n",
    "arizona_cities_filename = \"arizona-cities.txt\"\n",
    "michigan_cities_filename = \"michigan-cities.txt\"\n",
    "pennsylvania_cities_filename = \"pennsylvania-cities.txt\"\n",
    "# Folder Names\n",
    "video_output_path = \"Video\"\n",
    "audio_output_path = \"Audio\"\n",
    "transcription_output_path = \"Transcription\"\n",
    "cities_path = \"State Cities\"\n",
    "# Configs\n",
    "remove_video = True\n",
    "remove_audio = True\n",
    "# Categories\n",
    "presidential_candidates = {\n",
    "    \"Donald Trump\": [\n",
    "        \"Donald\", \"Trump\"\n",
    "    ],\n",
    "    \"Kamala Harris\": [\n",
    "        \"Kamala\", \"Harris\"\n",
    "    ]\n",
    "}\n",
    "state_cities = {\n",
    "    \"Michigan\": read_items_from_file(os.path.join(cities_path, michigan_cities_filename)),\n",
    "    \"Arizona\": read_items_from_file(os.path.join(cities_path, arizona_cities_filename)),\n",
    "    \"Pennsylvania\": read_items_from_file(os.path.join(cities_path, pennsylvania_cities_filename))\n",
    "}\n",
    "# Others\n",
    "stop_words = set(stopwords.words('english'))\n",
    "generic_abstract_nouns = {\n",
    "    \"thing\", \"stuff\", \"event\",\n",
    "    \"aspect\", \"issue\", \"place\",\n",
    "    \"person\"\n",
    "}\n",
    "# Additional Preprocessing of Configurations\n",
    "presidential_candidates = {presidential_candidate: list(set(names)) for presidential_candidate, names in presidential_candidates.items()}\n",
    "state_cities = {state: list(set(cities)) for state, cities in state_cities.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Data (YouTube Videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_youtube_video(video_filename: str, stream: Stream) -> tuple[str, str]:\n",
    "    # Create Video Directory\n",
    "    os.makedirs(video_output_path, exist_ok=True)\n",
    "    \n",
    "    # Set Path for Video File\n",
    "    video_file = os.path.join(video_output_path, video_filename)\n",
    "    \n",
    "    # Delete Old Existing Video File (note: to clean any corrupted file)\n",
    "    if os.path.exists(video_file):\n",
    "        os.remove(video_file)\n",
    "        \n",
    "    # Download Video File\n",
    "    print(\"\") # Just New Line for Better Output\n",
    "    print(f'Downloading (Video): {video_filename}')\n",
    "    print(\"\") # Just New Line for Better Output\n",
    "    stream.download(output_path=video_output_path, filename=video_filename)\n",
    "    print(\"\") # Just New Line for Better Output\n",
    "    print(\"\") # Just New Line for Better Output\n",
    "    \n",
    "    # Return Video File and Name\n",
    "    return video_file, video_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Extraction (Video to Audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_from_video(video_file: str, video_filename: str) -> tuple[str, str]:\n",
    "    # Create the Audio Directory\n",
    "    os.makedirs(audio_output_path, exist_ok=True)\n",
    "\n",
    "    # Set Audio File Name (\".mp3\")\n",
    "    audio_filename = f'{os.path.splitext(video_filename)[0]}.mp3'\n",
    "\n",
    "    # Set Path for Audio File\n",
    "    audio_file = os.path.join(audio_output_path, audio_filename)\n",
    "    \n",
    "    # Delete Old Existing Audio File (note: to clean any corrupted file)\n",
    "    if os.path.exists(audio_file):\n",
    "        os.remove(audio_file)\n",
    "    \n",
    "    # Extract Audio File\n",
    "    print(f'Extracting (Audio): {audio_filename}')\n",
    "    print(\"\") # Just New Line for Better Output\n",
    "    (\n",
    "        ffmpeg\n",
    "        .input(video_file)\n",
    "        .output(audio_file, format=\"mp3\", acodec=\"libmp3lame\", loglevel=\"info\")\n",
    "        .run(overwrite_output=True)\n",
    "    )\n",
    "    \n",
    "    # Return Audio File and Name\n",
    "    return audio_file, audio_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcription (Audio to Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio_to_text(audio_file: str, audio_filename: str, index: int):\n",
    "    # Create the Transcription Directory\n",
    "    os.makedirs(transcription_output_path, exist_ok=True)\n",
    "    \n",
    "    # Set Transcription File Name (\"[index]....txt\")\n",
    "    transcription_filename = f'{os.path.splitext(audio_filename)[0]}.txt'\n",
    "    \n",
    "    # Set Path for Transcription File\n",
    "    transcription_file = os.path.join(transcription_output_path, transcription_filename)\n",
    "    \n",
    "    # Delete Old Existing Transcription File (note: to clean any corrupted file)\n",
    "    if os.path.exists(transcription_file):\n",
    "        os.remove(transcription_file)\n",
    "        \n",
    "    # Get/Download OpenAI Whisper Model\n",
    "    \"\"\" \n",
    "    Models: \n",
    "        tiny, base, small, medium, large, turbo\n",
    "    English-Only:\n",
    "        tiny.en, base.en, small.en, medium.en\n",
    "    \n",
    "    Required VRAM:              Speed:\n",
    "        1) 1GB - tiny, base         1) 10x - tiny\n",
    "        2) 2GB - small              2) 8x - turbo\n",
    "        3) 5GB - medium             3) 7x - base\n",
    "        4) 6GB - turbo              4) 4x - small\n",
    "        5) 10GB - large             5) 2x - medium\n",
    "                                    6) 1x - large\n",
    "    \n",
    "    Quote from OpenAI: \n",
    "        - The .en models for English-only applications tend to perform better, especially for the tiny.en and base.en models.\n",
    "        We observed that the difference becomes less significant for the small.en and medium.en models.\n",
    "    \n",
    "    Note: 4GB lang VRAM ko kaya small.en ginamit\n",
    "    \"\"\"  \n",
    "    print(f'Transcribing (Text): {transcription_filename}')\n",
    "    print(\"\") # Just New Line for Better Output\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    model = whisper.load_model(\"small.en\", device=device)\n",
    "\n",
    "    # Transcribe Audio File (Saves Whole Text in Memory Before Disk to Avoid Corruption)\n",
    "    result = model.transcribe(audio_file, fp16=False, verbose=False)\n",
    "    with open(transcription_file, \"w\") as f:\n",
    "        f.write(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d009d19a4f64d859b6d3b01be662897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting YouTube URLs:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading (Video): LIVE_ Donald Trump hosts MAGA rally in Michigan.mp4\n",
      "\n",
      " ↳ |██████████████████████████████████████████████████████████████████| 100.0%\n",
      "\n",
      "Extracting (Audio): LIVE_ Donald Trump hosts MAGA rally in Michigan.mp3\n",
      "\n",
      "Transcribing (Text): [0] LIVE_ Donald Trump hosts MAGA rally in Michigan.txt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                   | 0/268701 [00:00<?, ?frames/s]\u001b[A\n",
      "  0%|                                                                         | 200/268701 [00:02<54:39, 81.86frames/s]\u001b[A\n",
      "  1%|▊                                                                      | 3168/268701 [00:04<05:34, 792.75frames/s]\u001b[A\n",
      "  2%|█▋                                                                     | 6152/268701 [00:12<09:06, 480.83frames/s]\u001b[A\n",
      "  3%|██▎                                                                    | 8852/268701 [00:19<10:06, 428.38frames/s]\u001b[A\n",
      "  4%|███                                                                   | 11816/268701 [00:27<10:37, 402.89frames/s]\u001b[A\n",
      "  5%|███▊                                                                  | 14676/268701 [00:34<10:24, 406.63frames/s]\u001b[A\n",
      "  6%|████▌                                                                 | 17336/268701 [00:40<09:51, 424.83frames/s]\u001b[A\n",
      "  8%|█████▎                                                                | 20260/268701 [00:47<09:57, 415.90frames/s]\u001b[A\n",
      "  9%|█████▉                                                                | 22948/268701 [00:55<10:16, 398.82frames/s]\u001b[A\n",
      "  9%|██████▌                                                               | 25376/268701 [01:00<09:48, 413.16frames/s]\u001b[A\n",
      " 10%|███████▎                                                              | 28084/268701 [01:06<09:35, 417.96frames/s]\u001b[A\n",
      " 11%|███████▉                                                              | 30660/268701 [01:14<10:00, 396.46frames/s]\u001b[A\n",
      " 12%|████████▋                                                             | 33514/268701 [01:21<10:05, 388.19frames/s]\u001b[A\n",
      " 14%|█████████▍                                                            | 36382/268701 [01:28<09:45, 396.88frames/s]\u001b[A\n",
      " 15%|██████████▏                                                           | 39098/268701 [01:36<09:55, 385.88frames/s]\u001b[A\n",
      " 16%|██████████▉                                                           | 41854/268701 [01:44<10:20, 365.52frames/s]\u001b[A\n",
      " 17%|███████████▋                                                          | 44710/268701 [01:54<11:04, 337.03frames/s]\u001b[A\n",
      " 18%|████████████▎                                                         | 47466/268701 [02:02<10:50, 340.20frames/s]\u001b[A\n",
      " 19%|█████████████                                                         | 49982/268701 [02:09<10:31, 346.35frames/s]\u001b[A\n",
      " 20%|█████████████▋                                                        | 52634/268701 [02:13<09:08, 393.78frames/s]\u001b[A\n",
      " 21%|██████████████▍                                                       | 55606/268701 [02:21<08:58, 395.50frames/s]\u001b[A\n",
      " 22%|███████████████▏                                                      | 58254/268701 [02:28<09:05, 386.08frames/s]\u001b[A\n",
      " 23%|███████████████▉                                                      | 61248/268701 [02:36<09:04, 380.65frames/s]\u001b[A\n",
      " 24%|████████████████▋                                                     | 63848/268701 [02:42<08:36, 396.75frames/s]\u001b[A\n",
      " 25%|█████████████████▎                                                    | 66480/268701 [02:48<08:05, 416.82frames/s]\u001b[A\n",
      " 26%|██████████████████                                                    | 69168/268701 [02:53<07:40, 433.12frames/s]\u001b[A\n",
      " 27%|██████████████████▋                                                   | 71788/268701 [03:00<07:44, 423.51frames/s]\u001b[A\n",
      " 28%|███████████████████▍                                                  | 74604/268701 [03:09<08:30, 380.21frames/s]\u001b[A\n",
      " 29%|████████████████████▏                                                 | 77572/268701 [03:16<08:13, 387.53frames/s]\u001b[A\n",
      " 30%|████████████████████▉                                                 | 80268/268701 [03:25<08:48, 356.74frames/s]\u001b[A\n",
      " 31%|█████████████████████▋                                                | 83148/268701 [03:33<08:42, 355.10frames/s]\u001b[A\n",
      " 32%|██████████████████████▍                                               | 85988/268701 [03:41<08:34, 355.37frames/s]\u001b[A\n",
      " 33%|███████████████████████▏                                              | 88856/268701 [03:51<08:52, 337.77frames/s]\u001b[A\n",
      " 34%|███████████████████████▉                                              | 91856/268701 [04:00<08:54, 330.78frames/s]\u001b[A\n",
      " 35%|████████████████████████▌                                             | 94316/268701 [04:08<08:54, 326.18frames/s]\u001b[A\n",
      " 36%|█████████████████████████▎                                            | 97316/268701 [04:14<07:51, 363.68frames/s]\u001b[A\n",
      " 37%|██████████████████████████                                            | 99844/268701 [04:21<07:31, 373.82frames/s]\u001b[A\n",
      " 38%|██████████████████████████▎                                          | 102428/268701 [04:28<07:29, 369.60frames/s]\u001b[A\n",
      " 39%|███████████████████████████                                          | 105260/268701 [04:33<06:28, 420.85frames/s]\u001b[A\n",
      " 40%|███████████████████████████▋                                         | 108044/268701 [04:40<06:42, 399.10frames/s]\u001b[A\n",
      " 41%|████████████████████████████▌                                        | 111012/268701 [04:48<06:34, 400.05frames/s]\u001b[A\n",
      " 42%|█████████████████████████████▏                                       | 113764/268701 [04:56<06:48, 378.92frames/s]\u001b[A\n",
      " 43%|█████████████████████████████▊                                       | 116276/268701 [05:04<07:10, 353.90frames/s]\u001b[A\n",
      " 44%|██████████████████████████████▌                                      | 119132/268701 [05:12<07:01, 355.10frames/s]\u001b[A\n",
      " 45%|███████████████████████████████▎                                     | 122036/268701 [05:18<06:23, 382.65frames/s]\u001b[A\n",
      " 46%|████████████████████████████████                                     | 124724/268701 [05:26<06:19, 379.54frames/s]\u001b[A\n",
      " 47%|████████████████████████████████▊                                    | 127548/268701 [05:33<06:13, 377.53frames/s]\u001b[A\n",
      " 49%|█████████████████████████████████▌                                   | 130524/268701 [05:41<06:08, 374.85frames/s]\u001b[A\n",
      " 49%|██████████████████████████████████                                   | 132660/268701 [05:47<06:05, 372.23frames/s]\u001b[A\n",
      " 50%|██████████████████████████████████▋                                  | 135268/268701 [05:51<05:14, 423.99frames/s]\u001b[A\n",
      " 51%|███████████████████████████████████▍                                 | 138020/268701 [05:59<05:23, 403.57frames/s]\u001b[A\n",
      " 52%|████████████████████████████████████▏                                | 140716/268701 [06:07<05:33, 383.42frames/s]\u001b[A\n",
      " 53%|████████████████████████████████████▉                                | 143716/268701 [06:13<05:09, 404.14frames/s]\u001b[A\n",
      " 53%|████████████████████████████████████▉                                | 143716/268701 [06:30<05:09, 404.14frames/s]\u001b[A\n",
      " 55%|█████████████████████████████████████▋                               | 146556/268701 [06:40<09:16, 219.34frames/s]\u001b[A\n",
      " 56%|██████████████████████████████████████▎                              | 149212/268701 [06:46<07:50, 253.93frames/s]\u001b[A\n",
      " 57%|███████████████████████████████████████                              | 152212/268701 [06:51<06:18, 307.52frames/s]\u001b[A\n",
      " 58%|███████████████████████████████████████▊                             | 155156/268701 [06:59<05:46, 327.91frames/s]\u001b[A\n",
      " 59%|████████████████████████████████████████▌                            | 158084/268701 [07:06<05:19, 346.36frames/s]\u001b[A\n",
      " 60%|█████████████████████████████████████████▎                           | 160696/268701 [07:14<05:12, 345.79frames/s]\u001b[A\n",
      " 61%|█████████████████████████████████████████▉                           | 163408/268701 [07:22<05:07, 342.84frames/s]\u001b[A\n",
      " 62%|██████████████████████████████████████████▌                          | 165888/268701 [07:29<04:59, 343.65frames/s]\u001b[A\n",
      " 63%|███████████████████████████████████████████▎                         | 168548/268701 [07:34<04:21, 382.49frames/s]\u001b[A\n",
      " 64%|███████████████████████████████████████████▉                         | 171184/268701 [07:41<04:12, 385.90frames/s]\u001b[A\n",
      " 65%|████████████████████████████████████████████▌                        | 173524/268701 [07:48<04:19, 366.44frames/s]\u001b[A\n",
      " 66%|█████████████████████████████████████████████▎                       | 176352/268701 [07:55<04:01, 382.03frames/s]\u001b[A\n",
      " 67%|██████████████████████████████████████████████                       | 179348/268701 [08:03<03:57, 375.91frames/s]\u001b[A\n",
      " 68%|██████████████████████████████████████████████▊                      | 182292/268701 [08:12<04:02, 356.83frames/s]\u001b[A\n",
      " 69%|███████████████████████████████████████████████▌                     | 185080/268701 [08:20<03:47, 367.68frames/s]\u001b[A\n",
      " 70%|████████████████████████████████████████████████▎                    | 187900/268701 [08:28<03:48, 354.38frames/s]\u001b[A\n",
      " 71%|█████████████████████████████████████████████████                    | 190900/268701 [08:35<03:27, 374.21frames/s]\u001b[A\n",
      " 72%|█████████████████████████████████████████████████▋                   | 193600/268701 [08:42<03:15, 383.74frames/s]\u001b[A\n",
      " 73%|██████████████████████████████████████████████████▍                  | 196300/268701 [08:50<03:18, 365.11frames/s]\u001b[A\n",
      " 74%|███████████████████████████████████████████████████                  | 198900/268701 [08:55<02:57, 392.38frames/s]\u001b[A\n",
      " 75%|███████████████████████████████████████████████████▊                 | 201800/268701 [09:05<03:04, 363.49frames/s]\u001b[A\n",
      " 76%|████████████████████████████████████████████████████▌                | 204700/268701 [09:11<02:47, 383.00frames/s]\u001b[A\n",
      " 77%|█████████████████████████████████████████████████████▎               | 207500/268701 [09:16<02:24, 424.93frames/s]\u001b[A\n",
      " 78%|██████████████████████████████████████████████████████               | 210300/268701 [09:24<02:27, 396.21frames/s]\u001b[A\n",
      " 79%|██████████████████████████████████████████████████████▋              | 212900/268701 [09:29<02:06, 439.65frames/s]\u001b[A\n",
      " 80%|███████████████████████████████████████████████████████▍             | 215700/268701 [09:37<02:09, 410.20frames/s]\u001b[A\n",
      " 81%|████████████████████████████████████████████████████████▏            | 218600/268701 [09:43<01:59, 419.49frames/s]\u001b[A\n",
      " 82%|████████████████████████████████████████████████████████▉            | 221500/268701 [09:50<01:51, 424.98frames/s]\u001b[A\n",
      " 84%|█████████████████████████████████████████████████████████▋           | 224500/268701 [09:59<01:52, 392.96frames/s]\u001b[A\n",
      " 85%|██████████████████████████████████████████████████████████▍          | 227400/268701 [10:08<01:51, 370.97frames/s]\u001b[A\n",
      " 85%|██████████████████████████████████████████████████████████▉          | 229600/268701 [10:14<01:49, 357.21frames/s]\u001b[A\n",
      " 86%|███████████████████████████████████████████████████████████▋         | 232400/268701 [10:19<01:28, 410.28frames/s]\u001b[A\n",
      " 87%|████████████████████████████████████████████████████████████▎        | 235100/268701 [10:26<01:24, 395.53frames/s]\u001b[A\n",
      " 89%|█████████████████████████████████████████████████████████████        | 238000/268701 [10:35<01:21, 375.44frames/s]\u001b[A\n",
      " 90%|█████████████████████████████████████████████████████████████▊       | 240900/268701 [10:44<01:18, 356.14frames/s]\u001b[A\n",
      " 91%|██████████████████████████████████████████████████████████████▌      | 243700/268701 [10:52<01:10, 355.42frames/s]\u001b[A\n",
      " 92%|███████████████████████████████████████████████████████████████▎     | 246600/268701 [11:00<01:01, 359.91frames/s]\u001b[A\n",
      " 93%|████████████████████████████████████████████████████████████████     | 249300/268701 [11:07<00:54, 358.68frames/s]\u001b[A\n",
      " 94%|████████████████████████████████████████████████████████████████▋    | 252000/268701 [11:14<00:45, 370.34frames/s]\u001b[A\n",
      " 95%|█████████████████████████████████████████████████████████████████▎   | 254400/268701 [11:20<00:37, 377.77frames/s]\u001b[A\n",
      " 96%|██████████████████████████████████████████████████████████████████   | 257300/268701 [11:28<00:30, 369.87frames/s]\u001b[A\n",
      " 97%|██████████████████████████████████████████████████████████████████▊  | 260100/268701 [11:36<00:23, 361.30frames/s]\u001b[A\n",
      " 98%|███████████████████████████████████████████████████████████████████▍ | 262800/268701 [11:44<00:16, 357.55frames/s]\u001b[A\n",
      " 99%|████████████████████████████████████████████████████████████████████▏| 265600/268701 [11:53<00:08, 348.02frames/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████▊| 268200/268701 [12:00<00:01, 343.82frames/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████| 268701/268701 [12:03<00:00, 371.19frames/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading (Video): Kamala Harris rally in Arizona_ FULL SPEECH.mp4\n",
      "\n",
      " ↳ |██████████████████████████████████████████████████████████████████| 100.0%\n",
      "\n",
      "Extracting (Audio): Kamala Harris rally in Arizona_ FULL SPEECH.mp3\n",
      "\n",
      "Transcribing (Text): [3] Kamala Harris rally in Arizona_ FULL SPEECH.txt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                    | 0/88474 [00:00<?, ?frames/s]\u001b[A\n",
      "  3%|██▎                                                                     | 2880/88474 [00:05<02:44, 521.35frames/s]\u001b[A\n",
      "  6%|████▍                                                                   | 5402/88474 [00:10<02:50, 486.02frames/s]\u001b[A\n",
      "  9%|██████▌                                                                 | 8026/88474 [00:18<03:08, 426.76frames/s]\u001b[A\n",
      " 12%|████████▌                                                              | 10610/88474 [00:23<02:52, 452.07frames/s]\u001b[A\n",
      " 15%|██████████▊                                                            | 13546/88474 [00:29<02:41, 462.84frames/s]\u001b[A\n",
      " 18%|████████████▉                                                          | 16154/88474 [00:35<02:38, 457.70frames/s]\u001b[A\n",
      " 21%|███████████████▏                                                       | 18850/88474 [00:41<02:37, 442.68frames/s]\u001b[A\n",
      " 24%|█████████████████▏                                                     | 21422/88474 [00:47<02:29, 449.46frames/s]\u001b[A\n",
      " 28%|███████████████████▌                                                   | 24414/88474 [00:53<02:19, 460.66frames/s]\u001b[A\n",
      " 30%|█████████████████████▌                                                 | 26902/88474 [00:58<02:13, 461.53frames/s]\u001b[A\n",
      " 34%|███████████████████████▉                                               | 29778/88474 [01:06<02:15, 432.49frames/s]\u001b[A\n",
      " 37%|██████████████████████████                                             | 32466/88474 [01:14<02:23, 390.43frames/s]\u001b[A\n",
      " 40%|████████████████████████████▍                                          | 35366/88474 [01:21<02:13, 398.52frames/s]\u001b[A\n",
      " 43%|██████████████████████████████▋                                        | 38298/88474 [01:28<02:00, 416.00frames/s]\u001b[A\n",
      " 47%|█████████████████████████████████                                      | 41254/88474 [01:35<01:52, 418.59frames/s]\u001b[A\n",
      " 50%|███████████████████████████████████▍                                   | 44134/88474 [01:42<01:49, 405.99frames/s]\u001b[A\n",
      " 53%|█████████████████████████████████████▌                                 | 46874/88474 [01:48<01:39, 416.12frames/s]\u001b[A\n",
      " 56%|███████████████████████████████████████▉                               | 49738/88474 [01:55<01:30, 426.01frames/s]\u001b[A\n",
      " 60%|██████████████████████████████████████████▎                            | 52702/88474 [02:02<01:25, 417.73frames/s]\u001b[A\n",
      " 62%|████████████████████████████████████████████▏                          | 54990/88474 [02:07<01:19, 420.37frames/s]\u001b[A\n",
      " 65%|██████████████████████████████████████████████                         | 57414/88474 [02:12<01:09, 445.49frames/s]\u001b[A\n",
      " 68%|████████████████████████████████████████████████▍                      | 60282/88474 [02:17<00:59, 470.06frames/s]\u001b[A\n",
      " 71%|██████████████████████████████████████████████████▎                    | 62714/88474 [02:23<00:56, 455.98frames/s]\u001b[A\n",
      " 74%|████████████████████████████████████████████████████▋                  | 65630/88474 [02:29<00:48, 474.25frames/s]\u001b[A\n",
      " 77%|██████████████████████████████████████████████████████▋                | 68102/88474 [02:35<00:44, 458.27frames/s]\u001b[A\n",
      " 80%|████████████████████████████████████████████████████████▉              | 71010/88474 [02:42<00:40, 434.76frames/s]\u001b[A\n",
      " 83%|██████████████████████████████████████████████████████████▌            | 73042/88474 [02:47<00:36, 427.28frames/s]\u001b[A\n",
      " 86%|████████████████████████████████████████████████████████████▊          | 75734/88474 [02:53<00:29, 430.00frames/s]\u001b[A\n",
      " 89%|███████████████████████████████████████████████████████████████        | 78586/88474 [03:01<00:23, 414.37frames/s]\u001b[A\n",
      " 92%|█████████████████████████████████████████████████████████████████▍     | 81560/88474 [03:08<00:16, 409.30frames/s]\u001b[A\n",
      " 96%|███████████████████████████████████████████████████████████████████▊   | 84516/88474 [03:15<00:09, 408.89frames/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████████████████████████████████▏| 87516/88474 [03:23<00:02, 402.02frames/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████| 88474/88474 [03:27<00:00, 426.98frames/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading (Video): Trump leading Harris in Arizona, new CBS News poll finds.mp4\n",
      "\n",
      " ↳ |██████████████████████████████████████████████████████████████████| 100.0%\n",
      "\n",
      "Extracting (Audio): Trump leading Harris in Arizona, new CBS News poll finds.mp3\n",
      "\n",
      "Transcribing (Text): [4] Trump leading Harris in Arizona, new CBS News poll finds.txt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                    | 0/22616 [00:00<?, ?frames/s]\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Transcribe Audio to Text -> Delete/Keep Audio File\u001b[39;00m\n\u001b[0;32m     43\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTranscribing (Text) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 44\u001b[0m \u001b[43mtranscribe_audio_to_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_audio: os\u001b[38;5;241m.\u001b[39mremove(audio_file)\n\u001b[0;32m     47\u001b[0m pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 42\u001b[0m, in \u001b[0;36mtranscribe_audio_to_text\u001b[1;34m(audio_file, audio_filename, index)\u001b[0m\n\u001b[0;32m     39\u001b[0m model \u001b[38;5;241m=\u001b[39m whisper\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmall.en\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Transcribe Audio File (Saves Whole Text in Memory Before Disk to Avoid Corruption)\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(transcription_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     44\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\whisper\\transcribe.py:279\u001b[0m, in \u001b[0;36mtranscribe\u001b[1;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[0m\n\u001b[0;32m    276\u001b[0m mel_segment \u001b[38;5;241m=\u001b[39m pad_or_trim(mel_segment, N_FRAMES)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(dtype)\n\u001b[0;32m    278\u001b[0m decode_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_tokens[prompt_reset_since:]\n\u001b[1;32m--> 279\u001b[0m result: DecodingResult \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel_segment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(result\u001b[38;5;241m.\u001b[39mtokens)\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_speech_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;66;03m# no voice activity check\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\whisper\\transcribe.py:195\u001b[0m, in \u001b[0;36mtranscribe.<locals>.decode_with_fallback\u001b[1;34m(segment)\u001b[0m\n\u001b[0;32m    192\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_of\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    194\u001b[0m options \u001b[38;5;241m=\u001b[39m DecodingOptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, temperature\u001b[38;5;241m=\u001b[39mt)\n\u001b[1;32m--> 195\u001b[0m decode_result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m needs_fallback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    199\u001b[0m     compression_ratio_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m decode_result\u001b[38;5;241m.\u001b[39mcompression_ratio \u001b[38;5;241m>\u001b[39m compression_ratio_threshold\n\u001b[0;32m    201\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\whisper\\decoding.py:824\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(model, mel, options, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m    822\u001b[0m     options \u001b[38;5;241m=\u001b[39m replace(options, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 824\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mDecodingTask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m single \u001b[38;5;28;01melse\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\whisper\\decoding.py:718\u001b[0m, in \u001b[0;36mDecodingTask.run\u001b[1;34m(self, mel)\u001b[0m\n\u001b[0;32m    715\u001b[0m tokenizer: Tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\n\u001b[0;32m    716\u001b[0m n_audio: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m mel\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 718\u001b[0m audio_features: Tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_audio_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# encoder forward pass\u001b[39;00m\n\u001b[0;32m    719\u001b[0m tokens: Tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_tokens])\u001b[38;5;241m.\u001b[39mrepeat(n_audio, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    721\u001b[0m \u001b[38;5;66;03m# detect language if requested, overwriting the language token\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\whisper\\decoding.py:655\u001b[0m, in \u001b[0;36mDecodingTask._get_audio_features\u001b[1;34m(self, mel)\u001b[0m\n\u001b[0;32m    653\u001b[0m     audio_features \u001b[38;5;241m=\u001b[39m mel\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 655\u001b[0m     audio_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m audio_features\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m (\n\u001b[0;32m    658\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfloat16 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m    659\u001b[0m ):\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    661\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio_features has an incorrect dtype: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio_features\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    662\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\whisper\\model.py:201\u001b[0m, in \u001b[0;36mAudioEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    198\u001b[0m x \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m--> 201\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_post(x)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\whisper\\model.py:170\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[1;34m(self, x, xa, mask, kv_cache)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn:\n\u001b[0;32m    169\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn_ln(x), xa, kv_cache\u001b[38;5;241m=\u001b[39mkv_cache)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 170\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_ln(x))\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1918\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1909\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m   1911\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[0;32m   1913\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[1;32m-> 1918\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m   1919\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m   1920\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "yt_urls = []\n",
    "with open(yt_video_links_filename, \"r\") as file:\n",
    "    yt_urls = list(set(url.strip() for url in file.readlines() if url.strip()))\n",
    "\n",
    "with tqdm(total=len(yt_urls), desc=\"Getting YouTube URLs\") as pbar:\n",
    "    for url in yt_urls:\n",
    "        current = f'[{index+1}/{len(yt_urls)}]'\n",
    "                \n",
    "        # Get Video Information\n",
    "        yt = YouTube(url, on_progress_callback=on_progress)\n",
    "        stream = yt.streams.get_audio_only()\n",
    "\n",
    "        # Sanitize Video File Name\n",
    "        video_filename = sanitize_filename(stream.default_filename)\n",
    "        \n",
    "        # Get File Name Without Extension (e.g., \".mp4\")\n",
    "        transcription_filename = f'{os.path.splitext(video_filename)[0]}.txt'\n",
    "        \n",
    "        # Skip If Transcription Already Exists\n",
    "        transcription_exists = False\n",
    "        pbar.set_description(f'Checking Existing Transcription File {current}')\n",
    "        if os.path.exists(transcription_output_path):\n",
    "            for existing_transcription_filename in os.listdir(transcription_output_path):\n",
    "                if existing_transcription_filename == transcription_filename:\n",
    "                    existing_transcription_path = os.path.join(transcription_output_path, existing_transcription_filename)\n",
    "                    if os.path.exists(existing_transcription_path):\n",
    "                        transcription_exists = True\n",
    "        if transcription_exists:\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        # Download YouTube Video\n",
    "        pbar.set_description(f'Downloading (Video) {current}')\n",
    "        video_file, video_filename = download_youtube_video(video_filename, stream)\n",
    "        \n",
    "        # Extract Audio from Video -> Delete/Keep Video File\n",
    "        pbar.set_description(f'Extracting (Audio) {current}')\n",
    "        audio_file, audio_filename = extract_audio_from_video(video_file, video_filename)\n",
    "        if remove_video: os.remove(video_file)\n",
    "        \n",
    "        # Transcribe Audio to Text -> Delete/Keep Audio File\n",
    "        pbar.set_description(f'Transcribing (Text) {current}')\n",
    "        transcribe_audio_to_text(audio_file, audio_filename, index)\n",
    "        if remove_audio: os.remove(audio_file)\n",
    "        \n",
    "        pbar.update(1)\n",
    "    pbar.set_description(\"Finished Data Gathering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_transcripts_into_csv_of_sentences() -> DataFrame:\n",
    "    # Initialize List of Sentences\n",
    "    list_of_sentences = []\n",
    "    \n",
    "    def is_sentence_complete(sentence: str) -> bool:\n",
    "        \"\"\"\n",
    "        Its a Proper Sentence If:\n",
    "            1) It has Atleast 1 Noun or Pronoun\n",
    "            2) It has Atleast 1 Verb\n",
    "        \"\"\"\n",
    "        pos_tags = pos_tag(word_tokenize(sentence)) # POS Tagging\n",
    "        has_subject = any(tag in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"] for _, tag in pos_tags) # Exclude Sentence w/out Noun and Pronoun\n",
    "        has_verb = any(tag.startswith(\"VB\") for _, tag in pos_tags) # Exclude Sentence w/out Verb\n",
    "    \n",
    "        return has_subject and has_verb\n",
    "    \n",
    "    # Collect List of All Sentences from Transcripts\n",
    "    for filename in os.listdir(transcription_output_path):\n",
    "        file_path = os.path.join(transcription_output_path, filename)\n",
    "        \n",
    "        with open(file_path, \"r\") as file:\n",
    "            text = file.read()\n",
    "            \n",
    "            # Split into Sentences\n",
    "            sentences = sent_tokenize(text)\n",
    "\n",
    "            # Filter Proper Sentences (With Noun/Proper-Noun and Verb)\n",
    "            sentences = [sentence for sentence in sentences if is_sentence_complete(sentence)]\n",
    "            \n",
    "            # Add Sentence to the List\n",
    "            list_of_sentences.extend(sentences)\n",
    "\n",
    "    # Save List of All Sentences into CSV file\n",
    "    df = pd.DataFrame(list_of_sentences, columns=[\"Sentence\"])\n",
    "    df.to_csv(transcript_sentences_filename, index=False)\n",
    "    return df\n",
    "\n",
    "process_transcripts_into_csv_of_sentences().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning: Topic Modeling and Sentence Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_related_sentences() -> DataFrame:\n",
    "    # Get All Sentences from Transcript\n",
    "    df = pd.read_csv(transcript_sentences_filename)\n",
    "    sentences = df[\"Sentence\"].tolist()\n",
    "    \n",
    "    # Set Filter for Words as Possible Topics\n",
    "    def filter_possible_topics(text) -> list:\n",
    "        \"\"\"\n",
    "        Filter Words If its a Possible Topic:\n",
    "            1) Only Nouns and Proper Nouns (e.g. Dollars, Currency)\n",
    "            2) No Stop Words (e.g. in, to)\n",
    "            3) No Generic Abstract Nouns (e.g. thing, stuff)\n",
    "            4) Minumum of Three Letter Words (e.g. USA)\n",
    "            5) Exclude Numbers\n",
    "        \"\"\"\n",
    "        \n",
    "        pos_tags = pos_tag(word_tokenize(text)) # POS Tagging\n",
    "        possible_topics = [\n",
    "            token.lower() for token, pos in pos_tags\n",
    "            if pos in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"] # Nouns / Proper Nouns\n",
    "            and token.lower() not in stop_words # Exclude Stop Words\n",
    "            and token.lower() not in generic_abstract_nouns # Exclude Generic Abstract Nouns\n",
    "            and len(token) > 2 # Exclude One/Two Letter Words\n",
    "            and not token.isnumeric() # Exclude Numbers\n",
    "        ]\n",
    "        \n",
    "        return possible_topics\n",
    "    vectorizer_model = CountVectorizer(tokenizer=filter_possible_topics)\n",
    "\n",
    "    # Train BERTopic model\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        vectorizer_model=vectorizer_model\n",
    "    )\n",
    "    topic_model.fit_transform(sentences)\n",
    "    \n",
    "    # Get BERTopic Results\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    \n",
    "    # Initialize Lists for our filtered results\n",
    "    list_of_related_sentences = []\n",
    "    \n",
    "    # Analyze each topic row in topic_info\n",
    "    for _, row in topic_info.iterrows():\n",
    "        if row[\"Topic\"] == -1: continue # Skip Outlier\n",
    "\n",
    "        # Get List of Topics and its Related Sentences\n",
    "        topic_keywords = row[\"Representation\"]\n",
    "        related_sentences = row[\"Representative_Docs\"]\n",
    "        \n",
    "        # Check Candidate Mentions in Topics\n",
    "        presidential_candidate_mentions = set() # Avoid Duplicates\n",
    "        for presidential_candidate, names in presidential_candidates.items():\n",
    "            if (\n",
    "                any(name.lower() == keyword.lower() for name in names for keyword in topic_keywords) \n",
    "                or any(presidential_candidate.lower() == keyword.lower() for keyword in topic_keywords)\n",
    "            ): \n",
    "                presidential_candidate_mentions.add(presidential_candidate)\n",
    "        \n",
    "        # Make Sure Only 1 Candidate is Mentioned\n",
    "        if len(presidential_candidate_mentions) != 1: continue\n",
    "\n",
    "        # Check State Mentions in Topics (Including Cities)\n",
    "        state_mentions = set() # Avoid Duplicates\n",
    "        for state, cities in state_cities.items():\n",
    "            if (\n",
    "                any(city.lower() == keyword.lower() for city in cities for keyword in topic_keywords) \n",
    "                or any(state.lower() == keyword.lower() for keyword in topic_keywords)\n",
    "            ): \n",
    "                state_mentions.add(state)\n",
    "\n",
    "        # Make Sure Only 1 State is Mentioned\n",
    "        if len(presidential_candidate_mentions) != 1: continue\n",
    "        \n",
    "        \"\"\"\n",
    "        Add Related Sentences Only If:\n",
    "            1) Only 1 Candidate is Mentioned\n",
    "            2) Only 1 State is Mentioned\n",
    "        \"\"\"\n",
    "        if len(presidential_candidate_mentions) == 1 and len(state_mentions) == 1:\n",
    "            presidential_candidate = presidential_candidate_mentions[0]\n",
    "            state = state_mentions[0]\n",
    "\n",
    "            # Add All Related Sentences with Corresponding Presidential Candidate, State, and Topic Keywords\n",
    "            for sentence in related_sentences:\n",
    "                list_of_related_sentences.append({\n",
    "                    \"Sentence\": sentence,\n",
    "                    \"Presidential_Candidate\": presidential_candidate,\n",
    "                    \"State\": state,\n",
    "                    \"Topic_Keywords\": topic_keywords\n",
    "                })\n",
    "    \n",
    "    # Save List of All Related Sentences into CSV file\n",
    "    df = pd.DataFrame(list_of_related_sentences)\n",
    "    df.to_csv(related_transcript_sentences_filename, index=False)\n",
    "    return df\n",
    "\n",
    "filter_related_sentences().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_statistics():\n",
    "    df = pd.read_csv(related_transcript_sentences_filename)\n",
    "    \n",
    "    # Group by candidate and state and count\n",
    "    stats = df.groupby([\"Presidential_Candidate\", \"State\"]).size().reset_index(name=\"count\")\n",
    "    \n",
    "    print(\"\\nSentence counts per candidate and state:\")\n",
    "    print(\"----------------------------------------\")\n",
    "    for _, row in stats.iterrows():\n",
    "        print(f'{row[\"Presidential_Candidate\"]} - {row[\"State\"]}: {row[\"count\"]} sentences')\n",
    "\n",
    "print_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sentiment Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training: Sentiment Analysis with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation: Hyperparameter Tuning and Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of Concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
