{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation already completed.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "    ðŸ—¿ READ ME ðŸ—¿\n",
    "    - This Only Needs To Run Once.\n",
    "    - This Needs to Restart the Kernel after Installing the Dependencies.  \n",
    "    - To Avoid Unintended Restart: This adds an init.flag file in root folder after successful installation.\n",
    "    - To Rerun: Delete init.flag file in root folder.\n",
    "    \n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\"\"\"\n",
    "# Import OS Dependency\n",
    "import os, subprocess, sys\n",
    "\n",
    "# Check If This Already Runned Once\n",
    "if os.path.exists(\"init.flag\"):\n",
    "    print(\"Installation already completed.\")\n",
    "else:\n",
    "    print(\"Starting Installation...\")\n",
    "\n",
    "    # Ensure pip is Installed\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"ensurepip\"], shell=True)\n",
    "\n",
    "    # Install Main Dependencies\n",
    "    subprocess.check_call(f'python -m pip install nltk \"ffmpeg-python\" \"openai-whisper\" pandas pytubefix bertopic \"scikit-learn\"', shell=True)\n",
    "    \n",
    "    # Install Other Dependencies\n",
    "    subprocess.check_call(f'python -m pip install torch tqdm', shell=True)\n",
    "    \n",
    "    # Install BERTopic Dependencies\n",
    "    subprocess.check_call(f'python -m pip install \"numpy<2\" \"tf-keras\"', shell=True)\n",
    "\n",
    "    # Add Flag File to Set that this Already Runned Once\n",
    "    open(\"init.flag\", \"w\").close()\n",
    "\n",
    "    # Restart the Kernel to Load Installed Dependences\n",
    "    os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\MSI\n",
      "[nltk_data]     Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\MSI Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\MSI\n",
      "[nltk_data]     Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Main Dependencies\n",
    "import os, re, nltk, ffmpeg, whisper\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pytubefix import YouTube, Stream\n",
    "from pytubefix.cli import on_progress\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import Other Dependencies\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Additional Downloads\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename: str) -> str:\n",
    "    # Escape Double Quotes\n",
    "    filename = filename.replace('\"', '\\\\\"')\n",
    "\n",
    "    # Replace Invalid Characters with \"_\"\n",
    "    invalid_chars = re.compile(r'[<>:\"/\\\\|?*]')\n",
    "    sanitized_filename = invalid_chars.sub(\"_\", filename)\n",
    "\n",
    "    return sanitized_filename\n",
    "    \n",
    "def read_unique_items_from_file(file: str) -> list:\n",
    "    with open(file, \"r\") as f:\n",
    "        return list(set(url.strip() for url in f.readlines() if url.strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Set Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Names\n",
    "yt_video_links_filename = \"YouTube Video Links.txt\"\n",
    "transcript_sentences_filename = \"transcript_sentences.csv\"\n",
    "related_transcript_sentences_filename = \"related_transcript_sentences.csv\"\n",
    "\n",
    "# Folder Names\n",
    "video_output_path = \"Video\"\n",
    "audio_output_path = \"Audio\"\n",
    "transcription_output_path = \"Transcription\"\n",
    "cities_path = \"State Cities\"\n",
    "\n",
    "# Boolean Flags\n",
    "remove_video = True\n",
    "remove_audio = True\n",
    "\n",
    "# Numeric Constants \n",
    "max_consecutive_words_for_topic = 3 # e.g. Unigram: \"Donald\" | Bigram: \"Donald Trump\" | Trigram: \"President Donald Trump\"\n",
    "\n",
    "# Sentence Categories\n",
    "presidential_candidates = {\n",
    "    \"Donald Trump\": [\n",
    "        \"Donald\", \"Trump\"\n",
    "    ],\n",
    "    \"Kamala Harris\": [\n",
    "        \"Kamala\", \"Harris\"\n",
    "    ]\n",
    "}\n",
    "state_cities = {\n",
    "    \"Michigan\": read_unique_items_from_file(os.path.join(cities_path, \"michigan-cities.txt\")),\n",
    "    \"Arizona\": read_unique_items_from_file(os.path.join(cities_path, \"arizona-cities.txt\")),\n",
    "    \"Pennsylvania\": read_unique_items_from_file(os.path.join(cities_path, \"pennsylvania-cities.txt\"))\n",
    "}\n",
    "\n",
    "# Words for Sentence Filtering\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "generic_abstract_nouns = {\n",
    "    \"thing\", \"stuff\", \"event\",\n",
    "    \"aspect\", \"issue\", \"place\",\n",
    "    \"person\"\n",
    "}\n",
    "\n",
    "# Additional Preprocessing of Configurations\n",
    "presidential_candidates = {presidential_candidate: list(set(names)) for presidential_candidate, names in presidential_candidates.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Data (YouTube Videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_youtube_video(video_filename: str, stream: Stream) -> tuple[str, str]:\n",
    "    # Create Video Directory\n",
    "    os.makedirs(video_output_path, exist_ok=True)\n",
    "    \n",
    "    # Set Path for Video File\n",
    "    video_file = os.path.join(video_output_path, video_filename)\n",
    "    \n",
    "    # Delete Old Existing Video File (note: to clean any corrupted file)\n",
    "    if os.path.exists(video_file):\n",
    "        os.remove(video_file)\n",
    "        \n",
    "    # Download Video File\n",
    "    print(\"\") # Just New Line for Better Output\n",
    "    print(f'Downloading (Video): {video_filename}')\n",
    "    print(\"\") # Just New Line for Better Output\n",
    "    stream.download(output_path=video_output_path, filename=video_filename)\n",
    "    print(\"\") # Just New Line for Better Output\n",
    "    print(\"\") # Just New Line for Better Output\n",
    "    \n",
    "    # Return Video File and Name\n",
    "    return video_file, video_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Extraction (Video to Audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_from_video(video_file: str, video_filename: str) -> tuple[str, str]:\n",
    "    # Create the Audio Directory\n",
    "    os.makedirs(audio_output_path, exist_ok=True)\n",
    "\n",
    "    # Set Audio File Name (\"[YouTube Video ID] [title].mp3\")\n",
    "    audio_filename = f'{os.path.splitext(video_filename)[0]}.mp3'\n",
    "\n",
    "    # Set Path for Audio File\n",
    "    audio_file = os.path.join(audio_output_path, audio_filename)\n",
    "    \n",
    "    # Delete Old Existing Audio File (note: to clean any corrupted file)\n",
    "    if os.path.exists(audio_file):\n",
    "        os.remove(audio_file)\n",
    "    \n",
    "    # Extract Audio File\n",
    "    print(f'Extracting (Audio): {audio_filename}')\n",
    "    print(\"\") # Just New Line for Better Output\n",
    "    (\n",
    "        ffmpeg\n",
    "        .input(video_file)\n",
    "        .output(audio_file, format=\"mp3\", acodec=\"libmp3lame\", loglevel=\"info\")\n",
    "        .run(overwrite_output=True)\n",
    "    )\n",
    "    \n",
    "    # Return Audio File and Name\n",
    "    return audio_file, audio_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcription (Audio to Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio_to_text(audio_file: str, audio_filename: str):\n",
    "    # Create the Transcription Directory\n",
    "    os.makedirs(transcription_output_path, exist_ok=True)\n",
    "    \n",
    "    # Set Transcription File Name (\"[YouTube Video ID] [title].txt\")\n",
    "    transcription_filename = f'{os.path.splitext(audio_filename)[0]}.txt'\n",
    "    \n",
    "    # Set Path for Transcription File\n",
    "    transcription_file = os.path.join(transcription_output_path, transcription_filename)\n",
    "            \n",
    "    # Get/Download OpenAI Whisper Model\n",
    "    \"\"\" \n",
    "    Models: \n",
    "        tiny, base, small, medium, large, turbo\n",
    "    English-Only:\n",
    "        tiny.en, base.en, small.en, medium.en\n",
    "    \n",
    "    Required VRAM:              Speed:\n",
    "        1) 1GB - tiny, base         1) 10x - tiny\n",
    "        2) 2GB - small              2) 8x - turbo\n",
    "        3) 5GB - medium             3) 7x - base\n",
    "        4) 6GB - turbo              4) 4x - small\n",
    "        5) 10GB - large             5) 2x - medium\n",
    "                                    6) 1x - large\n",
    "    \n",
    "    Quote from OpenAI: \n",
    "        - The .en models for English-only applications tend to perform better, especially for the tiny.en and base.en models.\n",
    "        We observed that the difference becomes less significant for the small.en and medium.en models.\n",
    "    \n",
    "    Note: 4GB lang VRAM ko kaya small.en ginamit\n",
    "    \"\"\"  \n",
    "    print(f'Transcribing (Text): {transcription_filename}')\n",
    "    print(\"\") # Just New Line for Better Output\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    model = whisper.load_model(\"small.en\", device=device)\n",
    "\n",
    "    # Transcribe Audio File (Saves Whole Text in Memory Before Disk to Avoid Corruption)\n",
    "    result = model.transcribe(audio_file, fp16=False, verbose=False)\n",
    "    try:\n",
    "        with open(transcription_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(result[\"text\"])\n",
    "    except:\n",
    "        if os.path.exists(transcription_file):\n",
    "            os.remove(transcription_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019e762c435c47169a7a366dc5ef628a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting YouTube URLs:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yt_urls = read_unique_items_from_file(yt_video_links_filename)\n",
    "\n",
    "with tqdm(total=len(yt_urls), desc=\"Getting YouTube URLs\") as pbar:\n",
    "    for index, url in enumerate(yt_urls):        \n",
    "        current = f'{index+1}/{len(yt_urls)}'\n",
    "                \n",
    "        # Get Video Information\n",
    "        yt = YouTube(url, on_progress_callback=on_progress)\n",
    "        stream = yt.streams.get_audio_only()\n",
    "        \n",
    "        # Sanitize Video File Name and Add YouTube Video ID\n",
    "        video_filename = f'[{yt.video_id}] {sanitize_filename(stream.default_filename)}'\n",
    "        \n",
    "        # Get File Name Without Extension (e.g., \".mp4\")\n",
    "        filename = os.path.splitext(video_filename)[0]\n",
    "        \n",
    "        # Skip If Transcription Already Exists\n",
    "        transcription_exists = False\n",
    "        pbar.set_description(f'Checking Existing Transcriptions (Video {current})')\n",
    "        if os.path.exists(transcription_output_path):\n",
    "            transcription_filename = f'{filename}.txt'\n",
    "            for existing_transcription_filename in os.listdir(transcription_output_path):\n",
    "                if existing_transcription_filename == transcription_filename:\n",
    "                    existing_transcription_path = os.path.join(transcription_output_path, existing_transcription_filename)\n",
    "                    if os.path.exists(existing_transcription_path):\n",
    "                        transcription_exists = True\n",
    "        if transcription_exists:\n",
    "            # Delete/Keep Video File\n",
    "            if remove_video:\n",
    "                video_file = os.path.join(video_output_path, video_filename)\n",
    "                if os.path.exists(video_file):\n",
    "                    os.remove(video_file)\n",
    "                    \n",
    "            # Delete/Keep Audio File\n",
    "            if remove_audio:\n",
    "                audio_filename = f'{filename}.mp3'\n",
    "                audio_file = os.path.join(audio_output_path, audio_filename)\n",
    "                if os.path.exists(audio_file):\n",
    "                    os.remove(audio_file)\n",
    "                    \n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        # Log YouTube URL being Processed\n",
    "        print(\"\") # Just New Line for Better Output\n",
    "        print(f'Found YouTube Video (URL): {url}')\n",
    "        \n",
    "        # Download YouTube Video\n",
    "        pbar.set_description(f'Downloading (Video {current}) ')\n",
    "        video_file, video_filename = download_youtube_video(video_filename, stream)\n",
    "        \n",
    "        # Extract Audio from Video -> Delete/Keep Video File\n",
    "        pbar.set_description(f'Extracting (Audio {current})')\n",
    "        audio_file, audio_filename = extract_audio_from_video(video_file, video_filename)\n",
    "        if remove_video: os.remove(video_file)\n",
    "        \n",
    "        # Transcribe Audio to Text -> Delete/Keep Audio File\n",
    "        pbar.set_description(f'Transcribing (Text {current})')\n",
    "        transcribe_audio_to_text(audio_file, audio_filename)\n",
    "        if remove_audio: os.remove(audio_file)\n",
    "        \n",
    "        pbar.update(1)\n",
    "    pbar.set_description(\"Finished Data Gathering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3d5a9170514729a51d10ea3c82977c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Sentences from Transcripts:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>And I have to say, I've noticed this, especial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Are you following up?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Somebody who was raised in the middle class, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And I will make sure that your abortion rights...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He's a Marine.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence\n",
       "0  And I have to say, I've noticed this, especial...\n",
       "1                              Are you following up?\n",
       "2  Somebody who was raised in the middle class, b...\n",
       "3  And I will make sure that your abortion rights...\n",
       "4                                     He's a Marine."
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_transcripts_into_csv_of_sentences() -> DataFrame:\n",
    "    # Initialize List of Sentences\n",
    "    list_of_sentences = []\n",
    "    \n",
    "    def is_sentence_complete(sentence: str) -> bool:\n",
    "        \"\"\"\n",
    "        Its a Proper Sentence If:\n",
    "            1) It has Atleast 1 Noun or Pronoun\n",
    "            2) It has Atleast 1 Verb\n",
    "        \"\"\"\n",
    "        pos_tags = pos_tag(word_tokenize(sentence)) # POS Tagging\n",
    "        has_subject = any(tag in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"] for _, tag in pos_tags) # Exclude Sentence w/out Noun and Pronoun\n",
    "        has_verb = any(tag.startswith(\"VB\") for _, tag in pos_tags) # Exclude Sentence w/out Verb\n",
    "    \n",
    "        return has_subject and has_verb\n",
    "    \n",
    "    # Collect List of All Sentences from Transcripts\n",
    "    transcription_files = os.listdir(transcription_output_path)\n",
    "    with tqdm(total=len(transcription_files), desc=\"Collecting Sentences from Transcripts\") as pbar:\n",
    "        for filename in transcription_files:\n",
    "            file_path = os.path.join(transcription_output_path, filename)\n",
    "            \n",
    "            with open(file_path, \"r\") as file:\n",
    "                text = file.read()\n",
    "                \n",
    "                # Split into Sentences\n",
    "                sentences = list(set(sent_tokenize(text)))\n",
    "    \n",
    "                # Filter Proper Sentences (With Noun/Proper-Noun and Verb)\n",
    "                sentences = [sentence for sentence in sentences if is_sentence_complete(sentence)]\n",
    "                \n",
    "                # Add Sentence to the List\n",
    "                list_of_sentences.extend(sentences)\n",
    "                \n",
    "            pbar.update(1)\n",
    "\n",
    "    # Save List of All Sentences into CSV file\n",
    "    df = pd.DataFrame(list_of_sentences, columns=[\"Sentence\"])\n",
    "    df.to_csv(transcript_sentences_filename, index=False)\n",
    "    return df\n",
    "\n",
    "list_of_sentences = process_transcripts_into_csv_of_sentences()\n",
    "list_of_sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning: Topic Modeling and Sentence Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-19 20:37:21,052 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00b5c38c52a4ec989483c411fb6abb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-19 20:37:31,681 - BERTopic - Embedding - Completed âœ“\n",
      "2024-10-19 20:37:31,681 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-10-19 20:37:43,673 - BERTopic - Dimensionality - Completed âœ“\n",
      "2024-10-19 20:37:43,673 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-10-19 20:37:43,801 - BERTopic - Cluster - Completed âœ“\n",
      "2024-10-19 20:37:43,801 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-10-19 20:37:45,416 - BERTopic - Representation - Completed âœ“\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Related Sentences: 3\n",
      "Number of Topic Clusters: 77\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>1159</td>\n",
       "      <td>-1_president_people_lot_country</td>\n",
       "      <td>[president, people, lot, country, things, year...</td>\n",
       "      <td>[He looked at it, he said, these people aren't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>0_election_vote_candidate_ballot</td>\n",
       "      <td>[election, vote, candidate, ballot, republican...</td>\n",
       "      <td>[I know we are going to win this election, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>1_bit_job_mcmahon_michelle</td>\n",
       "      <td>[bit, job, mcmahon, michelle, mark, interview,...</td>\n",
       "      <td>[Somebody who was raised in the middle class, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>89</td>\n",
       "      <td>2_trump_politics_problems_president</td>\n",
       "      <td>[trump, politics, problems, president, donald ...</td>\n",
       "      <td>[The people who are voting for Donald Trump an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>74</td>\n",
       "      <td>3_criminals_gang_immigrants_murder</td>\n",
       "      <td>[criminals, gang, immigrants, murder, suspect,...</td>\n",
       "      <td>[Armed Venezuelan gang members storming an apa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>71</td>\n",
       "      <td>12</td>\n",
       "      <td>71_numbers_numbers numbers_numbers virginia_vi...</td>\n",
       "      <td>[numbers, numbers numbers, numbers virginia, v...</td>\n",
       "      <td>[His numbers were very bad., His numbers were ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>72</td>\n",
       "      <td>12</td>\n",
       "      <td>72_governor_welcome_governors_whitmer</td>\n",
       "      <td>[governor, welcome, governors, whitmer, honor,...</td>\n",
       "      <td>[By the way, this is a really great governor, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>73</td>\n",
       "      <td>12</td>\n",
       "      <td>73_latino_latino voters_voters_sort</td>\n",
       "      <td>[latino, latino voters, voters, sort, message,...</td>\n",
       "      <td>[You know, but I think that I think it's somet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>74</td>\n",
       "      <td>11</td>\n",
       "      <td>74_hand_man eye_hands middle_eye baby</td>\n",
       "      <td>[hand, man eye, hands middle, eye baby, eye ba...</td>\n",
       "      <td>[Man, just not to finger or whip your eye., Pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>75</td>\n",
       "      <td>10</td>\n",
       "      <td>75_tire_bible_grocery store_trump bible</td>\n",
       "      <td>[tire, bible, grocery store, trump bible, tire...</td>\n",
       "      <td>[He wants you to buy the Word of God, Donald T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                               Name  \\\n",
       "0      -1   1159                    -1_president_people_lot_country   \n",
       "1       0    146                   0_election_vote_candidate_ballot   \n",
       "2       1    125                         1_bit_job_mcmahon_michelle   \n",
       "3       2     89                2_trump_politics_problems_president   \n",
       "4       3     74                 3_criminals_gang_immigrants_murder   \n",
       "..    ...    ...                                                ...   \n",
       "72     71     12  71_numbers_numbers numbers_numbers virginia_vi...   \n",
       "73     72     12              72_governor_welcome_governors_whitmer   \n",
       "74     73     12                73_latino_latino voters_voters_sort   \n",
       "75     74     11              74_hand_man eye_hands middle_eye baby   \n",
       "76     75     10            75_tire_bible_grocery store_trump bible   \n",
       "\n",
       "                                       Representation  \\\n",
       "0   [president, people, lot, country, things, year...   \n",
       "1   [election, vote, candidate, ballot, republican...   \n",
       "2   [bit, job, mcmahon, michelle, mark, interview,...   \n",
       "3   [trump, politics, problems, president, donald ...   \n",
       "4   [criminals, gang, immigrants, murder, suspect,...   \n",
       "..                                                ...   \n",
       "72  [numbers, numbers numbers, numbers virginia, v...   \n",
       "73  [governor, welcome, governors, whitmer, honor,...   \n",
       "74  [latino, latino voters, voters, sort, message,...   \n",
       "75  [hand, man eye, hands middle, eye baby, eye ba...   \n",
       "76  [tire, bible, grocery store, trump bible, tire...   \n",
       "\n",
       "                                  Representative_Docs  \n",
       "0   [He looked at it, he said, these people aren't...  \n",
       "1   [I know we are going to win this election, and...  \n",
       "2   [Somebody who was raised in the middle class, ...  \n",
       "3   [The people who are voting for Donald Trump an...  \n",
       "4   [Armed Venezuelan gang members storming an apa...  \n",
       "..                                                ...  \n",
       "72  [His numbers were very bad., His numbers were ...  \n",
       "73  [By the way, this is a really great governor, ...  \n",
       "74  [You know, but I think that I think it's somet...  \n",
       "75  [Man, just not to finger or whip your eye., Pl...  \n",
       "76  [He wants you to buy the Word of God, Donald T...  \n",
       "\n",
       "[77 rows x 5 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_related_sentences() -> tuple[DataFrame, BERTopic]:\n",
    "    # Get All Sentences from Transcript\n",
    "    df = pd.read_csv(transcript_sentences_filename)\n",
    "    sentences = df[\"Sentence\"].tolist()\n",
    "    \n",
    "    # Set Filter for Words as Possible Topics\n",
    "    def filter_possible_topics(text: str) -> list:\n",
    "        \"\"\"\n",
    "        Filter Words If its a Possible Topic:\n",
    "            1) Only Nouns and Proper Nouns (e.g. Dollars, Currency)\n",
    "            2) No Stop Words (e.g. in, to)\n",
    "            3) No Generic Abstract Nouns (e.g. thing, stuff)\n",
    "            4) Minumum of Three Letter Words (e.g. USA)\n",
    "            5) Exclude Numbers\n",
    "        \"\"\"\n",
    "        \n",
    "        pos_tags = pos_tag(word_tokenize(text)) # POS Tagging\n",
    "        possible_topics = [\n",
    "            token.lower() for token, pos in pos_tags\n",
    "            if pos in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"] # Nouns / Proper Nouns\n",
    "            and token.lower() not in stop_words # Exclude Stop Words\n",
    "            and token.lower() not in generic_abstract_nouns # Exclude Generic Abstract Nouns\n",
    "            and len(token) > 2 # Exclude One/Two Letter Words\n",
    "            and not token.isnumeric() # Exclude Numbers\n",
    "        ]\n",
    "        \n",
    "        return possible_topics\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        ngram_range=(1, max_consecutive_words_for_topic),\n",
    "        tokenizer=filter_possible_topics\n",
    "    )\n",
    "\n",
    "    # Train BERTopic model\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        n_gram_range=(1, max_consecutive_words_for_topic),\n",
    "        vectorizer_model=vectorizer_model,        \n",
    "        verbose=True\n",
    "    )\n",
    "    topic_model.fit_transform(sentences)\n",
    "    \n",
    "    # Get BERTopic Results\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    \n",
    "    # Initialize Lists for our filtered results\n",
    "    list_of_related_sentences = []\n",
    "    \n",
    "    # Analyze each topic row in topic_info\n",
    "    for _, row in topic_info.iterrows():\n",
    "        if row[\"Topic\"] == -1: continue # Skip Outlier\n",
    "\n",
    "        # Get List of Topics and its Related Sentences\n",
    "        topic_keywords = row[\"Representation\"]\n",
    "        related_sentences = row[\"Representative_Docs\"]\n",
    "        \n",
    "        # Check Candidate Mentions in Topics\n",
    "        presidential_candidate_mentions = set() # Avoid Duplicates\n",
    "        for presidential_candidate, names in presidential_candidates.items():\n",
    "            if (\n",
    "                any(name.lower() in keyword.lower() for name in names for keyword in topic_keywords) \n",
    "                or any(presidential_candidate.lower() in keyword.lower() for keyword in topic_keywords)\n",
    "            ): \n",
    "                presidential_candidate_mentions.add(presidential_candidate)\n",
    "        \n",
    "        # Make Sure Only 1 Candidate is Mentioned\n",
    "        if len(presidential_candidate_mentions) != 1: continue\n",
    "\n",
    "        # Check State Mentions in Topics (Including Cities)\n",
    "        state_mentions = set() # Avoid Duplicates\n",
    "        for state, cities in state_cities.items():\n",
    "            if (\n",
    "                any(city.lower() in keyword.lower() for city in cities for keyword in topic_keywords) \n",
    "                or any(state.lower() in keyword.lower() for keyword in topic_keywords)\n",
    "            ): \n",
    "                state_mentions.add(state)\n",
    "\n",
    "        # Make Sure Only 1 State is Mentioned\n",
    "        if len(presidential_candidate_mentions) != 1: continue\n",
    "        \n",
    "        \"\"\"\n",
    "        Add Related Sentences Only If:\n",
    "            1) Only 1 Candidate is Mentioned\n",
    "            2) Only 1 State is Mentioned\n",
    "        \"\"\"\n",
    "        if len(presidential_candidate_mentions) == 1 and len(state_mentions) == 1:\n",
    "            presidential_candidate = presidential_candidate_mentions.pop()\n",
    "            state = state_mentions.pop()\n",
    "\n",
    "            # Add All Related Sentences with Corresponding Presidential Candidate, State, and Topic Keywords\n",
    "            for sentence in related_sentences:\n",
    "                list_of_related_sentences.append({\n",
    "                    \"Sentence\": sentence,\n",
    "                    \"Presidential_Candidate\": presidential_candidate,\n",
    "                    \"State\": state,\n",
    "                    \"Topic_Keywords\": topic_keywords\n",
    "                })\n",
    "    \n",
    "    # Save List of All Related Sentences into CSV file\n",
    "    df = pd.DataFrame(list_of_related_sentences)\n",
    "    df.to_csv(related_transcript_sentences_filename, index=False)\n",
    "    return df, topic_model\n",
    "\n",
    "list_of_related_sentences, bertopic_model = filter_related_sentences()\n",
    "print(f'Number of Related Sentences: {len(list_of_related_sentences)}')\n",
    "print(f'Number of Topic Clusters: {len(bertopic_model.get_topic_info())}')\n",
    "bertopic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Presidential_Candidate</th>\n",
       "      <th>State</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>And I know you were just reporting in Detroit.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>[detroit, man detroit, man, lot flesh, reporti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They love him in Detroit, right?</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>[detroit, man detroit, man, lot flesh, reporti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They love him in Detroit.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>[detroit, man detroit, man, lot flesh, reporti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Sentence Presidential_Candidate  \\\n",
       "0  And I know you were just reporting in Detroit.           Donald Trump   \n",
       "1                They love him in Detroit, right?           Donald Trump   \n",
       "2                       They love him in Detroit.           Donald Trump   \n",
       "\n",
       "      State                                     Topic_Keywords  \n",
       "0  Michigan  [detroit, man detroit, man, lot flesh, reporti...  \n",
       "1  Michigan  [detroit, man detroit, man, lot flesh, reporti...  \n",
       "2  Michigan  [detroit, man detroit, man, lot flesh, reporti...  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_related_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_8b422\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_8b422_level0_col0\" class=\"col_heading level0 col0\" >Presidential_Candidate</th>\n",
       "      <th id=\"T_8b422_level0_col1\" class=\"col_heading level0 col1\" >State</th>\n",
       "      <th id=\"T_8b422_level0_col2\" class=\"col_heading level0 col2\" >count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_8b422_row0_col0\" class=\"data row0 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_8b422_row0_col1\" class=\"data row0 col1\" >Michigan</td>\n",
       "      <td id=\"T_8b422_row0_col2\" class=\"data row0 col2\" >3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e085a79f10>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sa tingin ko need natin 5k sentences minimum for Related di lang for gathered.\n",
    "Kasi mamaya 5k Unrelated Sentences nakuha natin tas 100 lang dun Related with candidate & state.\n",
    "\n",
    "Ang naiisip ko since 6 Combination = 3 candidate * 2 state\n",
    "Gawin natin 5000/6 = 834 Related Sentences required set natin as minimum per Combination\n",
    "\n",
    "Trump  - Arizona      = 834 Related Sentences\n",
    "Harris - Arizona      = 834 Related Sentences\n",
    "Trump  - Michigan     = 834 Related Sentences\n",
    "Harris - Michigan     = 834 Related Sentences\n",
    "Trump  - Pennsylvania = 834 Related Sentences\n",
    "Harris - Pennsylvania = 834 Related Sentences\n",
    "                     --------------------------\n",
    "                      ~5000 Related Sentences\n",
    "\"\"\"\n",
    "(\n",
    "    pd\n",
    "    .read_csv(related_transcript_sentences_filename)\n",
    "    .groupby([\"Presidential_Candidate\", \"State\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    "    .style.hide(axis=\"index\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sentiment Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training: Sentiment Analysis with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation: Hyperparameter Tuning and Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of Concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
