{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation already completed.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "    ðŸ—¿ READ ME ðŸ—¿\n",
    "    - This Only Needs To Run Once.\n",
    "    - This Needs to Restart the Kernel after Installing the Dependencies.  \n",
    "    - To Avoid Unintended Restart: This adds an init.flag file in root folder after successful installation.\n",
    "    - To Rerun: Delete init.flag file in root folder.\n",
    "    \n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\"\"\"\n",
    "# Import OS Dependency\n",
    "import os, subprocess, sys\n",
    "\n",
    "# Check If This Already Runned Once\n",
    "if os.path.exists(\"init.flag\"):\n",
    "    print(\"Installation already completed.\")\n",
    "else:\n",
    "    print(\"Starting Installation...\")\n",
    "\n",
    "    # Ensure pip is Installed\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"ensurepip\"], shell=True)\n",
    "\n",
    "    # Install Main Dependencies\n",
    "    subprocess.check_call(f'python -m pip install nltk \"ffmpeg-python\" \"openai-whisper\" pandas pytubefix bertopic \"scikit-learn\"', shell=True)\n",
    "    \n",
    "    # Install Other Dependencies\n",
    "    subprocess.check_call(f'python -m pip install torch tqdm', shell=True)\n",
    "    \n",
    "    # Install BERTopic Dependencies\n",
    "    subprocess.check_call(f'python -m pip install \"numpy<2\" \"tf-keras\"', shell=True)\n",
    "\n",
    "    # Add Flag File to Set that this Already Runned Once\n",
    "    open(\"init.flag\", \"w\").close()\n",
    "\n",
    "    # Restart the Kernel to Load Installed Dependences\n",
    "    os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import Main Dependencies\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mre\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mffmpeg\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mwhisper\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataFrame\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\__init__.py:146\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjsontags\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# PACKAGES\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\chunk\\__init__.py:155\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Natural Language Toolkit: Chunkers\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2001-2024 NLTK Project\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# For license information, see LICENSE.TXT\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mClasses and interfaces for identifying non-overlapping linguistic\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03mgroups (such as base noun phrases) in unrestricted text.  This task is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m     pattern is valid.\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChunkParserI\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnamed_entity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Maxent_NE_Chunker\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregexp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RegexpChunkParser, RegexpParser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\chunk\\api.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChunkScore\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParserI\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mChunkParserI\u001b[39;00m(ParserI):\n\u001b[0;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    A processing interface for identifying non-overlapping groups in\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    unrestricted text.  Typically, chunk parsers are used to find base\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m    will always generate a parse.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\parse\\__init__.py:100\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecursivedescent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     96\u001b[0m     RecursiveDescentParser,\n\u001b[0;32m     97\u001b[0m     SteppingRecursiveDescentParser,\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshiftreduce\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ShiftReduceParser, SteppingShiftReduceParser\n\u001b[1;32m--> 100\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransitionparser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransitionParser\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TestGrammar, extract_test_sentences, load_parser\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mviterbi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ViterbiParser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\parse\\transitionparser.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m svm\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_svmlight_file\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\__init__.py:12\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# See http://scikit-learn.sourceforge.net/modules/svm.html for complete\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# documentation.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#         of their respective owners.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# License: BSD 3 clause (C) INRIA 2010\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bounds\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m l1_min_c\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_classes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC, SVR, LinearSVC, LinearSVR, NuSVC, NuSVR, OneClassSVM\n\u001b[0;32m     14\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinearSVC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinearSVR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml1_min_c\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     23\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseEstimator, OutlierMixin, RegressorMixin, _fit_context\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearClassifierMixin, LinearModel, SparseCoefMixin\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, StrOptions\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_classification_targets\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\__init__.py:32\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_huber\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuberRegressor\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_least_angle\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     Lars,\n\u001b[0;32m     25\u001b[0m     LarsCV,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     lars_path_gram,\n\u001b[0;32m     31\u001b[0m )\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_logistic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression, LogisticRegressionCV\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_omp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     34\u001b[0m     OrthogonalMatchingPursuit,\n\u001b[0;32m     35\u001b[0m     OrthogonalMatchingPursuitCV,\n\u001b[0;32m     36\u001b[0m     orthogonal_mp,\n\u001b[0;32m     37\u001b[0m     orthogonal_mp_gram,\n\u001b[0;32m     38\u001b[0m )\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_passive_aggressive\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PassiveAggressiveClassifier, PassiveAggressiveRegressor\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_cv\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelBinarizer, LabelEncoder\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _fit_liblinear\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     30\u001b[0m     Bunch,\n\u001b[0;32m     31\u001b[0m     check_array,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     compute_class_weight,\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Hidden, Interval, StrOptions\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1149\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:936\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1032\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1130\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import Main Dependencies\n",
    "import os, re, nltk, ffmpeg, whisper\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pytubefix import YouTube, Stream\n",
    "from pytubefix.cli import on_progress\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import Other Dependencies\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Additional Downloads\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename: str) -> str:\n",
    "    # Escape Double Quotes\n",
    "    filename = filename.replace('\"', '\\\\\"')\n",
    "\n",
    "    # Replace Invalid Characters with \"_\"\n",
    "    invalid_chars = re.compile(r'[<>:\"/\\\\|?*]')\n",
    "    sanitized_filename = invalid_chars.sub(\"_\", filename)\n",
    "\n",
    "    return sanitized_filename\n",
    "def read_unique_items_from_file(file: str) -> list:\n",
    "    with open(file, \"r\") as f:\n",
    "        return list(set(url.strip() for url in f.readlines() if url.strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Set Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Michigan': ['Port Huron',\n",
       "  'Highland Park',\n",
       "  'Flat Rock',\n",
       "  'Mount Pleasant',\n",
       "  'New Baltimore',\n",
       "  'Ludington',\n",
       "  'Livonia',\n",
       "  'Byron Center',\n",
       "  'Norton Shores',\n",
       "  'Richmond',\n",
       "  'Ironwood',\n",
       "  'Oak Park',\n",
       "  'River Rouge',\n",
       "  'Rochester',\n",
       "  'Whitmore Lake',\n",
       "  'Zeeland',\n",
       "  'Benton Harbor',\n",
       "  'Davison',\n",
       "  'Pontiac',\n",
       "  'Taylor',\n",
       "  'Wixom',\n",
       "  'Haslett',\n",
       "  'Farmington',\n",
       "  'St. Clair Shores',\n",
       "  'Northview',\n",
       "  'Portage',\n",
       "  'Bridgeport',\n",
       "  'Allendale',\n",
       "  'Flushing',\n",
       "  'Garden City',\n",
       "  'Grand Rapids',\n",
       "  'Jackson',\n",
       "  'East Grand Rapids',\n",
       "  'Harper Woods',\n",
       "  'Hastings',\n",
       "  'Beverly Hills',\n",
       "  'Hazel Park',\n",
       "  'Westwood',\n",
       "  'Three Rivers',\n",
       "  'Grosse Pointe Park',\n",
       "  'South Monroe',\n",
       "  'Grand Haven',\n",
       "  'St. Clair',\n",
       "  'Ypsilanti',\n",
       "  'Lansing',\n",
       "  'Mason',\n",
       "  'Albion',\n",
       "  'Fraser',\n",
       "  'South Lyon',\n",
       "  'Clawson',\n",
       "  'Midland',\n",
       "  'Melvindale',\n",
       "  'Jenison',\n",
       "  'Trenton',\n",
       "  'Ecorse',\n",
       "  'Monroe',\n",
       "  'Royal Oak',\n",
       "  'Battle Creek',\n",
       "  'Birmingham',\n",
       "  'Hudsonville',\n",
       "  'Southfield',\n",
       "  'Fenton',\n",
       "  'Troy',\n",
       "  'Cadillac',\n",
       "  'Holly',\n",
       "  'East Lansing',\n",
       "  'Hamtramck',\n",
       "  'Greenville',\n",
       "  'Ferndale',\n",
       "  'Grand Blanc',\n",
       "  'Novi',\n",
       "  'Auburn Hills',\n",
       "  'Flint',\n",
       "  'Milford',\n",
       "  'Saline',\n",
       "  'Muskegon Heights',\n",
       "  'Eastpointe',\n",
       "  'Swartz Creek',\n",
       "  'Woodhaven',\n",
       "  'Houghton',\n",
       "  'Big Rapids',\n",
       "  'Wayne',\n",
       "  'Eastwood',\n",
       "  'Alma',\n",
       "  'Lincoln Park',\n",
       "  'Kentwood',\n",
       "  'Ionia',\n",
       "  'Shields',\n",
       "  'Wyoming',\n",
       "  'Temperance',\n",
       "  'Niles',\n",
       "  'Eaton Rapids',\n",
       "  'Buena Vista',\n",
       "  'Burton',\n",
       "  'Sturgis',\n",
       "  'Howell',\n",
       "  'Adrian',\n",
       "  'Beecher',\n",
       "  'Sterling Heights',\n",
       "  'Freeland',\n",
       "  'Bay City',\n",
       "  'Charlotte',\n",
       "  'Huntington Woods',\n",
       "  'Ann Arbor',\n",
       "  'St. Johns',\n",
       "  'Dearborn',\n",
       "  'Grand Ledge',\n",
       "  'Traverse City',\n",
       "  'Tecumseh',\n",
       "  'Cutlerville',\n",
       "  'Owosso',\n",
       "  'Alpena',\n",
       "  'Wyandotte',\n",
       "  'Plymouth',\n",
       "  'Comstock Park',\n",
       "  'Farmington Hills',\n",
       "  'Lambertville',\n",
       "  'Menominee',\n",
       "  'Springfield',\n",
       "  'Lapeer',\n",
       "  'Dearborn Heights',\n",
       "  'Forest Hills',\n",
       "  'Grosse Pointe Woods',\n",
       "  'Marshall',\n",
       "  'Petoskey',\n",
       "  'Kalamazoo',\n",
       "  'Iron Mountain',\n",
       "  'Warren',\n",
       "  'Marquette',\n",
       "  'Grosse Pointe',\n",
       "  'Roseville',\n",
       "  'Rochester Hills',\n",
       "  'Okemos',\n",
       "  'Ishpeming',\n",
       "  'Saginaw',\n",
       "  'Sault Ste. Marie',\n",
       "  'Center Line',\n",
       "  'Waverly',\n",
       "  'Muskegon',\n",
       "  'Walker',\n",
       "  'Northville',\n",
       "  'Mount Clemens',\n",
       "  'Manistee',\n",
       "  'Rockford',\n",
       "  'St. Joseph',\n",
       "  'Marysville',\n",
       "  'Kingsford',\n",
       "  'Grandville',\n",
       "  'Milan',\n",
       "  'Inkster',\n",
       "  'Grosse Pointe Farms',\n",
       "  'Southgate',\n",
       "  'Brighton',\n",
       "  'Madison Heights',\n",
       "  'Westland',\n",
       "  'Fair Plain',\n",
       "  'Holt',\n",
       "  'St. Louis',\n",
       "  'Comstock Northwest',\n",
       "  'Detroit',\n",
       "  'Hillsdale',\n",
       "  'Lake Fenton',\n",
       "  'Coldwater',\n",
       "  'Escanaba',\n",
       "  'Walled Lake',\n",
       "  'Berkley',\n",
       "  'Belding',\n",
       "  'Dowagiac',\n",
       "  'Riverview',\n",
       "  'Holland',\n",
       "  'Romulus',\n",
       "  'Allen Park'],\n",
       " 'Arizona': ['Three Points',\n",
       "  'Florence',\n",
       "  'El Mirage',\n",
       "  'Phoenix',\n",
       "  'Gold Canyon',\n",
       "  'Fountain Hills',\n",
       "  'San Luis',\n",
       "  'Doney Park',\n",
       "  'Holbrook',\n",
       "  'Marana',\n",
       "  'Prescott Valley',\n",
       "  'Douglas',\n",
       "  'Glendale',\n",
       "  'Picture Rocks',\n",
       "  'Fortuna Foothills',\n",
       "  'Summit',\n",
       "  'Rio Rico',\n",
       "  'Paulden',\n",
       "  'Yuma',\n",
       "  'San Tan Valley',\n",
       "  'Flagstaff',\n",
       "  'Fort Mohave',\n",
       "  'Vail',\n",
       "  'Show Low',\n",
       "  'Buckeye',\n",
       "  'Tempe',\n",
       "  'Eloy',\n",
       "  'Payson',\n",
       "  'Casa Grande',\n",
       "  'Green Valley',\n",
       "  'Sahuarita',\n",
       "  'Tolleson',\n",
       "  'Flowing Wells',\n",
       "  'Maricopa',\n",
       "  'Benson',\n",
       "  'Goodyear',\n",
       "  'South Tucson',\n",
       "  'Arizona City',\n",
       "  'Golden Valley',\n",
       "  'Litchfield Park',\n",
       "  'New River',\n",
       "  'Paradise Valley',\n",
       "  'Chandler',\n",
       "  'Peoria',\n",
       "  'Sierra Vista',\n",
       "  'Prescott',\n",
       "  'Cave Creek',\n",
       "  'Chino Valley',\n",
       "  'Anthem',\n",
       "  'Somerton',\n",
       "  'Cottonwood',\n",
       "  'Youngtown',\n",
       "  'Village of Oak Creek (Big Park)',\n",
       "  'Corona de Tucson',\n",
       "  'Sierra Vista Southeast',\n",
       "  'Catalina',\n",
       "  'Wickenburg',\n",
       "  'Sun City',\n",
       "  'Saddlebrooke',\n",
       "  'Tuba City',\n",
       "  'Drexel Heights',\n",
       "  'Bisbee',\n",
       "  'Scottsdale',\n",
       "  'Williamson',\n",
       "  'Valencia West',\n",
       "  'Winslow',\n",
       "  'Rincon Valley',\n",
       "  'Camp Verde',\n",
       "  'Sun Lakes',\n",
       "  'Avondale',\n",
       "  'Tucson',\n",
       "  'Nogales',\n",
       "  'Apache Junction',\n",
       "  'Verde Village',\n",
       "  'Globe',\n",
       "  'Mesa',\n",
       "  'Page',\n",
       "  'Lake Havasu City',\n",
       "  'Gilbert',\n",
       "  'Avra Valley',\n",
       "  'New Kingman-Butler',\n",
       "  'Safford',\n",
       "  'Oro Valley',\n",
       "  'Snowflake',\n",
       "  'Tucson Estates',\n",
       "  'Casas Adobes',\n",
       "  'Bullhead City',\n",
       "  'Sedona',\n",
       "  'Tanque Verde',\n",
       "  'Coolidge',\n",
       "  'Guadalupe',\n",
       "  'Kingman',\n",
       "  'Surprise',\n",
       "  'Kayenta',\n",
       "  'Queen Creek',\n",
       "  'Catalina Foothills',\n",
       "  'Sun City West'],\n",
       " 'Pennsylvania': ['Vandergrift',\n",
       "  'Hollidaysburg',\n",
       "  'Wilson',\n",
       "  'Castle Shannon',\n",
       "  'Waynesboro',\n",
       "  'Berwick',\n",
       "  'Shenandoah',\n",
       "  'Quakertown',\n",
       "  'Schuylkill Haven',\n",
       "  'Wilkes-Barre',\n",
       "  'Trooper',\n",
       "  'Sharon',\n",
       "  'Jeannette',\n",
       "  'Blue Bell',\n",
       "  'Wyomissing',\n",
       "  'Enola',\n",
       "  'Clairton',\n",
       "  'Lancaster',\n",
       "  'Canonsburg',\n",
       "  'Fernway',\n",
       "  'Bradford',\n",
       "  'Richboro',\n",
       "  'Hanover',\n",
       "  'Kingston',\n",
       "  'Scranton',\n",
       "  'Aliquippa',\n",
       "  'Arnold',\n",
       "  'Taylor',\n",
       "  'Lower Allen',\n",
       "  'Munhall',\n",
       "  'Horsham',\n",
       "  'Pottsville',\n",
       "  'Olyphant',\n",
       "  'Tamaqua',\n",
       "  'Jefferson Hills',\n",
       "  'Middletown',\n",
       "  'Wescosville',\n",
       "  'Indiana',\n",
       "  'Media',\n",
       "  'Farrell',\n",
       "  'East York',\n",
       "  'South Park Township',\n",
       "  'Perkasie',\n",
       "  'New Kensington',\n",
       "  'Shillington',\n",
       "  'Blakely',\n",
       "  'Ambler',\n",
       "  'New Brighton',\n",
       "  'Allentown',\n",
       "  'Uniontown',\n",
       "  'Parkville',\n",
       "  'California',\n",
       "  'Conshohocken',\n",
       "  'Shamokin',\n",
       "  'Willow Street',\n",
       "  'McKeesport',\n",
       "  'McKees Rocks',\n",
       "  'New Castle',\n",
       "  'Harleysville',\n",
       "  'Brentwood',\n",
       "  'Hellertown',\n",
       "  'New Holland',\n",
       "  'Washington',\n",
       "  'Pittsburgh',\n",
       "  'Village Green-Green Ridge',\n",
       "  'Archbald',\n",
       "  'Weigelstown',\n",
       "  'Harrisburg',\n",
       "  'Phoenixville',\n",
       "  'Red Lion',\n",
       "  'Sunbury',\n",
       "  'Clifton Heights',\n",
       "  'DuBois',\n",
       "  'Lewisburg',\n",
       "  'Catasauqua',\n",
       "  'Glenolden',\n",
       "  'Lionville',\n",
       "  'Audubon',\n",
       "  'King of Prussia',\n",
       "  'Morrisville',\n",
       "  'Sanatoga',\n",
       "  'Shiloh',\n",
       "  'Penn Wynne',\n",
       "  'Yeadon',\n",
       "  'Crafton',\n",
       "  'Upper St. Clair',\n",
       "  'Birdsboro',\n",
       "  'Chester',\n",
       "  'Connellsville',\n",
       "  'Oakmont',\n",
       "  'Croydon',\n",
       "  'Nazareth',\n",
       "  'Reading',\n",
       "  'Monessen',\n",
       "  'South Williamsport',\n",
       "  'Stroudsburg',\n",
       "  'East Stroudsburg',\n",
       "  'Huntingdon',\n",
       "  'Greenville',\n",
       "  'Homeacre-Lyndora',\n",
       "  'Folcroft',\n",
       "  'Souderton',\n",
       "  'Collingdale',\n",
       "  'Allison Park',\n",
       "  'Grove City',\n",
       "  'Hatboro',\n",
       "  'Blandon',\n",
       "  'Mechanicsburg',\n",
       "  'Ephrata',\n",
       "  'Titusville',\n",
       "  'Kulpsville',\n",
       "  'Chambersburg',\n",
       "  'Hazleton',\n",
       "  'Bloomsburg',\n",
       "  'New Cumberland',\n",
       "  'Shanor-Northvue',\n",
       "  'Bellefonte',\n",
       "  'Levittown',\n",
       "  'Baldwin',\n",
       "  'Norristown',\n",
       "  'Palmerton',\n",
       "  'Pottstown',\n",
       "  'Greensburg',\n",
       "  'Westmont',\n",
       "  'Coatesville',\n",
       "  'West Mifflin',\n",
       "  'Meadville',\n",
       "  'Murrysville',\n",
       "  'Ardmore',\n",
       "  'Swoyersville',\n",
       "  'Bellevue',\n",
       "  'Bridgeville',\n",
       "  'Punxsutawney',\n",
       "  'Drexel Hill',\n",
       "  'West Chester',\n",
       "  'Fullerton',\n",
       "  'Brookhaven',\n",
       "  'Erie',\n",
       "  'Lansdale',\n",
       "  'Darby',\n",
       "  'Lower Burrell',\n",
       "  'Clarks Summit',\n",
       "  'Swarthmore',\n",
       "  'Carlisle',\n",
       "  'St. Marys',\n",
       "  'Bristol',\n",
       "  'Fort Washington',\n",
       "  'Norwood',\n",
       "  'Mount Carmel',\n",
       "  'State College',\n",
       "  'Collegeville',\n",
       "  'Progress',\n",
       "  'Shippensburg',\n",
       "  'Wyndmoor',\n",
       "  'Glenside',\n",
       "  'Altoona',\n",
       "  'Hershey',\n",
       "  'Sayre',\n",
       "  'Camp Hill',\n",
       "  'Moosic',\n",
       "  'Corry',\n",
       "  'Willow Grove',\n",
       "  'Woodlyn',\n",
       "  'Lewistown',\n",
       "  'Economy',\n",
       "  'Schlusser',\n",
       "  'Nanticoke',\n",
       "  'Somerset',\n",
       "  'Plymouth',\n",
       "  'Plymouth Meeting',\n",
       "  'Prospect Park',\n",
       "  'Beaver Falls',\n",
       "  'Downingtown',\n",
       "  'Northwest Harborcreek',\n",
       "  'Ellwood City',\n",
       "  'Northampton',\n",
       "  'Lansdowne',\n",
       "  'Clearfield',\n",
       "  'Lebanon',\n",
       "  'Coraopolis',\n",
       "  'Leola',\n",
       "  'Palmyra',\n",
       "  'Tyrone',\n",
       "  'Mountain Top',\n",
       "  'Oil City',\n",
       "  'Forest Hills',\n",
       "  'Millersville',\n",
       "  'Montgomeryville',\n",
       "  'Old Forge',\n",
       "  'Mount Joy',\n",
       "  'Monroeville',\n",
       "  'Warren',\n",
       "  'Selinsgrove',\n",
       "  'Bangor',\n",
       "  'Franklin',\n",
       "  'Lititz',\n",
       "  'Kutztown',\n",
       "  'Oxford',\n",
       "  'Emmaus',\n",
       "  'Turtle Creek',\n",
       "  'Lehighton',\n",
       "  'Carnegie',\n",
       "  'Fox Chapel',\n",
       "  'Ancient Oaks',\n",
       "  'Steelton',\n",
       "  'West View',\n",
       "  'Plum',\n",
       "  'Latrobe',\n",
       "  'Franklin Park',\n",
       "  'Swissvale',\n",
       "  'Philadelphia',\n",
       "  'Elizabethtown',\n",
       "  'Dormont',\n",
       "  'Ridley Park',\n",
       "  'Paxtonia',\n",
       "  'Williamsport',\n",
       "  'Wilkinsburg',\n",
       "  'Bethel Park',\n",
       "  'Monaca',\n",
       "  'Lock Haven',\n",
       "  'Park Forest Village',\n",
       "  'Dickson City',\n",
       "  'White Oak',\n",
       "  'Dunmore',\n",
       "  'Sugarcreek',\n",
       "  'Butler',\n",
       "  'Pleasant Hills',\n",
       "  'Arlington Heights',\n",
       "  'Carbondale',\n",
       "  'Linglestown',\n",
       "  'Glenshaw',\n",
       "  'Carnot-Moon',\n",
       "  'Clarion',\n",
       "  'Paoli',\n",
       "  'Pittston',\n",
       "  'Sharon Hill',\n",
       "  'Bethlehem',\n",
       "  'Exeter',\n",
       "  'York',\n",
       "  'Columbia',\n",
       "  'Whitehall',\n",
       "  'Edinboro',\n",
       "  'Gettysburg',\n",
       "  'Milton',\n",
       "  'Ambridge',\n",
       "  'Hermitage',\n",
       "  'Broomall',\n",
       "  'Colonial Park',\n",
       "  'Johnstown',\n",
       "  'Doylestown',\n",
       "  'Easton',\n",
       "  'Oreland',\n",
       "  'Folsom',\n",
       "  'Duquesne',\n",
       "  'Kennett Square',\n",
       "  'Maple Glen',\n",
       "  'Fairless Hills']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File Names\n",
    "yt_video_links_filename = \"YouTube Video Links.txt\"\n",
    "transcript_sentences_filename = \"transcript_sentences.csv\"\n",
    "related_transcript_sentences_filename = \"related_transcript_sentences.csv\"\n",
    "# Folder Names\n",
    "video_output_path = \"Video\"\n",
    "audio_output_path = \"Audio\"\n",
    "transcription_output_path = \"Transcription\"\n",
    "cities_path = \"State Cities\"\n",
    "# Configs\n",
    "remove_video = True\n",
    "remove_audio = True\n",
    "# Categories\n",
    "presidential_candidates = {\n",
    "    \"Donald Trump\": [\n",
    "        \"Donald\", \"Trump\"\n",
    "    ],\n",
    "    \"Kamala Harris\": [\n",
    "        \"Kamala\", \"Harris\"\n",
    "    ]\n",
    "}\n",
    "state_cities = {\n",
    "    \"Michigan\": read_unique_items_from_file(os.path.join(cities_path, \"michigan-cities.txt\")),\n",
    "    \"Arizona\": read_unique_items_from_file(os.path.join(cities_path, \"arizona-cities.txt\")),\n",
    "    \"Pennsylvania\": read_unique_items_from_file(os.path.join(cities_path, \"pennsylvania-cities.txt\"))\n",
    "}\n",
    "# Others\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "generic_abstract_nouns = {\n",
    "    \"thing\", \"stuff\", \"event\",\n",
    "    \"aspect\", \"issue\", \"place\",\n",
    "    \"person\"\n",
    "}\n",
    "# Additional Preprocessing of Configurations\n",
    "presidential_candidates = {presidential_candidate: list(set(names)) for presidential_candidate, names in presidential_candidates.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Data (YouTube Videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_youtube_video(video_filename: str, stream: Stream) -> tuple[str, str]:\n",
    "    # Create Video Directory\n",
    "    os.makedirs(video_output_path, exist_ok=True)\n",
    "    \n",
    "    # Set Path for Video File\n",
    "    video_file = os.path.join(video_output_path, video_filename)\n",
    "    \n",
    "    # Delete Old Existing Video File (note: to clean any corrupted file)\n",
    "    if os.path.exists(video_file):\n",
    "        os.remove(video_file)\n",
    "        \n",
    "    # Download Video File\n",
    "    print(\"\") # Just New Line for Better Output\n",
    "    print(f'Downloading (Video): {video_filename}')\n",
    "    print(\"\") # Just New Line for Better Output\n",
    "    stream.download(output_path=video_output_path, filename=video_filename)\n",
    "    print(\"\") # Just New Line for Better Output\n",
    "    print(\"\") # Just New Line for Better Output\n",
    "    \n",
    "    # Return Video File and Name\n",
    "    return video_file, video_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Extraction (Video to Audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_from_video(video_file: str, video_filename: str) -> tuple[str, str]:\n",
    "    # Create the Audio Directory\n",
    "    os.makedirs(audio_output_path, exist_ok=True)\n",
    "\n",
    "    # Set Audio File Name (\".mp3\")\n",
    "    audio_filename = f'{os.path.splitext(video_filename)[0]}.mp3'\n",
    "\n",
    "    # Set Path for Audio File\n",
    "    audio_file = os.path.join(audio_output_path, audio_filename)\n",
    "    \n",
    "    # Delete Old Existing Audio File (note: to clean any corrupted file)\n",
    "    if os.path.exists(audio_file):\n",
    "        os.remove(audio_file)\n",
    "    \n",
    "    # Extract Audio File\n",
    "    print(f'Extracting (Audio): {audio_filename}')\n",
    "    print(\"\") # Just New Line for Better Output\n",
    "    (\n",
    "        ffmpeg\n",
    "        .input(video_file)\n",
    "        .output(audio_file, format=\"mp3\", acodec=\"libmp3lame\", loglevel=\"info\")\n",
    "        .run(overwrite_output=True)\n",
    "    )\n",
    "    \n",
    "    # Return Audio File and Name\n",
    "    return audio_file, audio_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcription (Audio to Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio_to_text(audio_file: str, audio_filename: str, index: int):\n",
    "    # Create the Transcription Directory\n",
    "    os.makedirs(transcription_output_path, exist_ok=True)\n",
    "    \n",
    "    # Set Transcription File Name (\".txt\")\n",
    "    transcription_filename = f'{os.path.splitext(audio_filename)[0]}.txt'\n",
    "    \n",
    "    # Set Path for Transcription File\n",
    "    transcription_file = os.path.join(transcription_output_path, transcription_filename)\n",
    "            \n",
    "    # Get/Download OpenAI Whisper Model\n",
    "    \"\"\" \n",
    "    Models: \n",
    "        tiny, base, small, medium, large, turbo\n",
    "    English-Only:\n",
    "        tiny.en, base.en, small.en, medium.en\n",
    "    \n",
    "    Required VRAM:              Speed:\n",
    "        1) 1GB - tiny, base         1) 10x - tiny\n",
    "        2) 2GB - small              2) 8x - turbo\n",
    "        3) 5GB - medium             3) 7x - base\n",
    "        4) 6GB - turbo              4) 4x - small\n",
    "        5) 10GB - large             5) 2x - medium\n",
    "                                    6) 1x - large\n",
    "    \n",
    "    Quote from OpenAI: \n",
    "        - The .en models for English-only applications tend to perform better, especially for the tiny.en and base.en models.\n",
    "        We observed that the difference becomes less significant for the small.en and medium.en models.\n",
    "    \n",
    "    Note: 4GB lang VRAM ko kaya small.en ginamit\n",
    "    \"\"\"  \n",
    "    print(f'Transcribing (Text): {transcription_filename}')\n",
    "    print(\"\") # Just New Line for Better Output\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    model = whisper.load_model(\"tiny.en\", device=device)\n",
    "\n",
    "    # Transcribe Audio File (Saves Whole Text in Memory Before Disk to Avoid Corruption)\n",
    "    result = model.transcribe(audio_file, fp16=False, verbose=False)\n",
    "    with open(transcription_file, \"w\") as f:\n",
    "        f.write(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.youtube.com/watch?v=dxnMTg42bLA',\n",
       " 'https://www.youtube.com/watch?v=V84s1Kb37WM',\n",
       " 'https://www.youtube.com/watch?v=WwN0kHydiUc']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_urls = read_unique_items_from_file(yt_video_links_filename)\n",
    "\n",
    "with tqdm(total=len(yt_urls), desc=\"Getting YouTube URLs\") as pbar:\n",
    "    for index, url in enumerate(yt_urls):\n",
    "        current = f'[{index+1}/{len(yt_urls)}]'\n",
    "                \n",
    "        # Get Video Information\n",
    "        yt = YouTube(url, on_progress_callback=on_progress)\n",
    "        stream = yt.streams.get_audio_only()\n",
    "\n",
    "        # Sanitize Video File Name\n",
    "        video_filename = sanitize_filename(stream.default_filename)\n",
    "        \n",
    "        # Get File Name Without Extension (e.g., \".mp4\")\n",
    "        transcription_filename = f'{os.path.splitext(video_filename)[0]}.txt'\n",
    "        \n",
    "        # Skip If Transcription Already Exists\n",
    "        transcription_exists = False\n",
    "        pbar.set_description(f'Checking Existing Transcription File {current}')\n",
    "        if os.path.exists(transcription_output_path):\n",
    "            for existing_transcription_filename in os.listdir(transcription_output_path):\n",
    "                if existing_transcription_filename == transcription_filename:\n",
    "                    existing_transcription_path = os.path.join(transcription_output_path, existing_transcription_filename)\n",
    "                    if os.path.exists(existing_transcription_path):\n",
    "                        transcription_exists = True\n",
    "        if transcription_exists:\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        # Download YouTube Video\n",
    "        pbar.set_description(f'Downloading (Video) {current}')\n",
    "        video_file, video_filename = download_youtube_video(video_filename, stream)\n",
    "        \n",
    "        # Extract Audio from Video -> Delete/Keep Video File\n",
    "        pbar.set_description(f'Extracting (Audio) {current}')\n",
    "        audio_file, audio_filename = extract_audio_from_video(video_file, video_filename)\n",
    "        if remove_video: os.remove(video_file)\n",
    "        \n",
    "        # Transcribe Audio to Text -> Delete/Keep Audio File\n",
    "        pbar.set_description(f'Transcribing (Text) {current}')\n",
    "        transcribe_audio_to_text(audio_file, audio_filename, index)\n",
    "        if remove_audio: os.remove(audio_file)\n",
    "        \n",
    "        pbar.update(1)\n",
    "    pbar.set_description(\"Finished Data Gathering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_transcripts_into_csv_of_sentences() -> DataFrame:\n",
    "    # Initialize List of Sentences\n",
    "    list_of_sentences = []\n",
    "    \n",
    "    def is_sentence_complete(sentence: str) -> bool:\n",
    "        \"\"\"\n",
    "        Its a Proper Sentence If:\n",
    "            1) It has Atleast 1 Noun or Pronoun\n",
    "            2) It has Atleast 1 Verb\n",
    "        \"\"\"\n",
    "        pos_tags = pos_tag(word_tokenize(sentence)) # POS Tagging\n",
    "        has_subject = any(tag in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"] for _, tag in pos_tags) # Exclude Sentence w/out Noun and Pronoun\n",
    "        has_verb = any(tag.startswith(\"VB\") for _, tag in pos_tags) # Exclude Sentence w/out Verb\n",
    "    \n",
    "        return has_subject and has_verb\n",
    "    \n",
    "    # Collect List of All Sentences from Transcripts\n",
    "    for filename in os.listdir(transcription_output_path):\n",
    "        file_path = os.path.join(transcription_output_path, filename)\n",
    "        \n",
    "        with open(file_path, \"r\") as file:\n",
    "            text = file.read()\n",
    "            \n",
    "            # Split into Sentences\n",
    "            sentences = sent_tokenize(text)\n",
    "\n",
    "            # Filter Proper Sentences (With Noun/Proper-Noun and Verb)\n",
    "            sentences = [sentence for sentence in sentences if is_sentence_complete(sentence)]\n",
    "            \n",
    "            # Add Sentence to the List\n",
    "            list_of_sentences.extend(sentences)\n",
    "\n",
    "    # Save List of All Sentences into CSV file\n",
    "    df = pd.DataFrame(list_of_sentences, columns=[\"Sentence\"])\n",
    "    df.to_csv(transcript_sentences_filename, index=False)\n",
    "    return df\n",
    "\n",
    "process_transcripts_into_csv_of_sentences().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning: Topic Modeling and Sentence Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_related_sentences() -> DataFrame:\n",
    "    # Get All Sentences from Transcript\n",
    "    df = pd.read_csv(transcript_sentences_filename)\n",
    "    sentences = df[\"Sentence\"].tolist()\n",
    "    \n",
    "    # Set Filter for Words as Possible Topics\n",
    "    def filter_possible_topics(text) -> list:\n",
    "        \"\"\"\n",
    "        Filter Words If its a Possible Topic:\n",
    "            1) Only Nouns and Proper Nouns (e.g. Dollars, Currency)\n",
    "            2) No Stop Words (e.g. in, to)\n",
    "            3) No Generic Abstract Nouns (e.g. thing, stuff)\n",
    "            4) Minumum of Three Letter Words (e.g. USA)\n",
    "            5) Exclude Numbers\n",
    "        \"\"\"\n",
    "        \n",
    "        pos_tags = pos_tag(word_tokenize(text)) # POS Tagging\n",
    "        possible_topics = [\n",
    "            token.lower() for token, pos in pos_tags\n",
    "            if pos in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"] # Nouns / Proper Nouns\n",
    "            and token.lower() not in stop_words # Exclude Stop Words\n",
    "            and token.lower() not in generic_abstract_nouns # Exclude Generic Abstract Nouns\n",
    "            and len(token) > 2 # Exclude One/Two Letter Words\n",
    "            and not token.isnumeric() # Exclude Numbers\n",
    "        ]\n",
    "        \n",
    "        return possible_topics\n",
    "    vectorizer_model = CountVectorizer(tokenizer=filter_possible_topics)\n",
    "\n",
    "    # Train BERTopic model\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        vectorizer_model=vectorizer_model\n",
    "    )\n",
    "    topic_model.fit_transform(sentences)\n",
    "    \n",
    "    # Get BERTopic Results\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    \n",
    "    # Initialize Lists for our filtered results\n",
    "    list_of_related_sentences = []\n",
    "    \n",
    "    # Analyze each topic row in topic_info\n",
    "    for _, row in topic_info.iterrows():\n",
    "        if row[\"Topic\"] == -1: continue # Skip Outlier\n",
    "\n",
    "        # Get List of Topics and its Related Sentences\n",
    "        topic_keywords = row[\"Representation\"]\n",
    "        related_sentences = row[\"Representative_Docs\"]\n",
    "        \n",
    "        # Check Candidate Mentions in Topics\n",
    "        presidential_candidate_mentions = set() # Avoid Duplicates\n",
    "        for presidential_candidate, names in presidential_candidates.items():\n",
    "            if (\n",
    "                any(name.lower() == keyword.lower() for name in names for keyword in topic_keywords) \n",
    "                or any(presidential_candidate.lower() == keyword.lower() for keyword in topic_keywords)\n",
    "            ): \n",
    "                presidential_candidate_mentions.add(presidential_candidate)\n",
    "        \n",
    "        # Make Sure Only 1 Candidate is Mentioned\n",
    "        if len(presidential_candidate_mentions) != 1: continue\n",
    "\n",
    "        # Check State Mentions in Topics (Including Cities)\n",
    "        state_mentions = set() # Avoid Duplicates\n",
    "        for state, cities in state_cities.items():\n",
    "            if (\n",
    "                any(city.lower() == keyword.lower() for city in cities for keyword in topic_keywords) \n",
    "                or any(state.lower() == keyword.lower() for keyword in topic_keywords)\n",
    "            ): \n",
    "                state_mentions.add(state)\n",
    "\n",
    "        # Make Sure Only 1 State is Mentioned\n",
    "        if len(presidential_candidate_mentions) != 1: continue\n",
    "        \n",
    "        \"\"\"\n",
    "        Add Related Sentences Only If:\n",
    "            1) Only 1 Candidate is Mentioned\n",
    "            2) Only 1 State is Mentioned\n",
    "        \"\"\"\n",
    "        if len(presidential_candidate_mentions) == 1 and len(state_mentions) == 1:\n",
    "            presidential_candidate = presidential_candidate_mentions[0]\n",
    "            state = state_mentions[0]\n",
    "\n",
    "            # Add All Related Sentences with Corresponding Presidential Candidate, State, and Topic Keywords\n",
    "            for sentence in related_sentences:\n",
    "                list_of_related_sentences.append({\n",
    "                    \"Sentence\": sentence,\n",
    "                    \"Presidential_Candidate\": presidential_candidate,\n",
    "                    \"State\": state,\n",
    "                    \"Topic_Keywords\": topic_keywords\n",
    "                })\n",
    "    \n",
    "    # Save List of All Related Sentences into CSV file\n",
    "    df = pd.DataFrame(list_of_related_sentences)\n",
    "    df.to_csv(related_transcript_sentences_filename, index=False)\n",
    "    return df\n",
    "\n",
    "filter_related_sentences().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_statistics():\n",
    "    df = pd.read_csv(related_transcript_sentences_filename)\n",
    "    \n",
    "    # Group by candidate and state and count\n",
    "    stats = df.groupby([\"Presidential_Candidate\", \"State\"]).size().reset_index(name=\"count\")\n",
    "    \n",
    "    print(\"\\nSentence counts per candidate and state:\")\n",
    "    print(\"----------------------------------------\")\n",
    "    for _, row in stats.iterrows():\n",
    "        print(f'{row[\"Presidential_Candidate\"]} - {row[\"State\"]}: {row[\"count\"]} sentences')\n",
    "\n",
    "print_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sentiment Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training: Sentiment Analysis with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation: Hyperparameter Tuning and Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of Concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
