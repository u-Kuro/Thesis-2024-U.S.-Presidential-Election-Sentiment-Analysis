{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Main Dependencies\n",
    "import os, re, nltk\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import Other Dependencies\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Additional Downloads\n",
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename: str) -> str:\n",
    "    # Escape Double Quotes\n",
    "    filename = filename.replace('\"', '\\\\\"')\n",
    "\n",
    "    # Replace Invalid Characters with \"_\"\n",
    "    invalid_chars = re.compile(r'[<>:\"/\\\\|?*]')\n",
    "    sanitized_filename = invalid_chars.sub(\"_\", filename)\n",
    "\n",
    "    return sanitized_filename\n",
    "    \n",
    "def read_unique_items_from_file(file: str) -> list:\n",
    "    with open(file, \"r\") as f:\n",
    "        return list(set(url.strip() for url in f.readlines() if url.strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# File Names\n",
    "transcript_sentences_filename = \"transcript_paragraphs.csv\"\n",
    "related_transcript_sentences_filename = \"related_transcript_sentences.csv\"\n",
    "\n",
    "# Folder Names\n",
    "transcription_output_path = \"Transcription\"\n",
    "cities_path = \"State Cities\"\n",
    "\n",
    "# Boolean Flags\n",
    "remove_video = True\n",
    "remove_audio = True\n",
    "\n",
    "# Numeric Constants \n",
    "max_consecutive_words_for_topic = 2 # e.g. Unigram: \"Donald\" | Bigram: \"Donald Trump\" | Trigram: \"President Donald Trump\"\n",
    "minimum_number_of_word_in_related_sentence = 5\n",
    "\n",
    "# Sentence Categories\n",
    "presidential_candidates = {\n",
    "    \"Donald Trump\": [\n",
    "        \"Donald\", \"Trump\"\n",
    "    ],\n",
    "    \"Kamala Harris\": [\n",
    "        \"Kamala\", \"Harris\"\n",
    "    ]\n",
    "}\n",
    "state_cities = {\n",
    "    \"Michigan\": read_unique_items_from_file(os.path.join(cities_path, \"michigan-cities.txt\")),\n",
    "    \"Arizona\": read_unique_items_from_file(os.path.join(cities_path, \"arizona-cities.txt\")),\n",
    "    \"Pennsylvania\": read_unique_items_from_file(os.path.join(cities_path, \"pennsylvania-cities.txt\"))\n",
    "}\n",
    "\n",
    "# Words for Sentence Filtering\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "generic_abstract_nouns = {\n",
    "    \"thing\", \"stuff\", \"event\",\n",
    "    \"aspect\", \"issue\", \"place\",\n",
    "    \"person\"\n",
    "}\n",
    "\n",
    "# Additional Preprocessing of Configurations\n",
    "presidential_candidates = {presidential_candidate: list(set(names)) for presidential_candidate, names in presidential_candidates.items()}\n",
    "presidential_candidates_and_states_combinations = [\n",
    "    f\"{pattern}_{loc}\".lower() for name, parts in presidential_candidates.items() \n",
    "    for loc in [state for state in state_cities] + [city for cities in state_cities.values() for city in cities]\n",
    "    for pattern in [name, '_'.join(parts)] + parts\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Extraction (Transcripts to CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9290c1a3668e4ebaad7fb90cfb0e2926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Segmenting Transcripts:   0%|          | 0/269 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29cf9ef0e1e43dc9b17530d44f4d090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [2/269 Transcript]:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac32821c48443c6bc29296fdd0ce395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [3/269 Transcript]:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be77a23741e4d698800f2aa7b9648aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [4/269 Transcript]:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331b4f7ef2e34edfabb578db7298b997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [5/269 Transcript]:   0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba44a3c65d544a0baeca7e89fc2b674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [6/269 Transcript]:   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0014ffe80b3349c28190dad786a7db48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [7/269 Transcript]:   0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f1b31dc9524904b4aeba19e0d25c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [8/269 Transcript]:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13edf2789cc947e1bf7a3a2d4656f736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [9/269 Transcript]:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc97a10e3cac4e1799ccda25f03e7e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [10/269 Transcript]:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2165ede708d648b9ba31f9aae4e036a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [11/269 Transcript]:   0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c3d410fe194bc69267d164af2afd85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [12/269 Transcript]:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224008a441e3469bbb7298c4f1d657e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [13/269 Transcript]:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ca9cfc13e94f42a0cd42d174efdee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [14/269 Transcript]:   0%|          | 0/237 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2639f04a2e40ea8c56ecbe8735ee3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [15/269 Transcript]:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b6116b7c954b77b76276560a876c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [16/269 Transcript]:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e91067b371d4070912d396bce80f625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [17/269 Transcript]:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856143cc5a3e4f5e9737e0b1270ec5e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [18/269 Transcript]:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c856731b43f44a899f20db97c328b758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [19/269 Transcript]:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36545432135b4553bf04f4e6a930054f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [20/269 Transcript]:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57bc11fd7ad4d4cb0b615f9278ff6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [21/269 Transcript]:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0d9b714e844c329411b2e00646d8e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [22/269 Transcript]:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe3894b05df4216a8057550436a8f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [23/269 Transcript]:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8bffa1f227a41c89fa08355c8920060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [24/269 Transcript]:   0%|          | 0/145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8a034aa7364e089068e47b83325bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [25/269 Transcript]:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0131ebcd318b45dd80c6e57fe4ea2ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [26/269 Transcript]:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae694d3f42244538c1a6ef303e7ae29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [27/269 Transcript]:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe66d6d5f934e60abfb3331fef88518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [28/269 Transcript]:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4561a11fc8f940ca86724d16edfc2017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [29/269 Transcript]:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63640b71d5e4ee0bdec20b8c179092d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [30/269 Transcript]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0360945a6d4326971a2bed6c166eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [31/269 Transcript]:   0%|          | 0/189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08057d90a86f4d10b03eff4130610c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [32/269 Transcript]:   0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1dcdfca8d9746a392cddaf539552f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Paragraphs [33/269 Transcript]:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_transcripts_into_csv_of_paragraphs() -> DataFrame:\n",
    "    # Initialize BERT models\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize List of Paragraphs\n",
    "    list_of_paragraphs = []\n",
    "    \n",
    "    def para_tokenize(sentences: list, current: str, threshold: float = 0.5) -> list:        \n",
    "        # Handle empty input\n",
    "        if not sentences: return []\n",
    "        \n",
    "        # Initialize Paragraphs w/ First Sentence\n",
    "        paragraphs = [[sentences[0]]]\n",
    "        \n",
    "        # Add Next Sentence if its Related to Current Sentence\n",
    "        last_sentences_idx = len(sentences) - 1\n",
    "        with tqdm(total=last_sentences_idx - 1, desc=f'Identifying Paragraphs [{current} Transcript]') as pbar:\n",
    "            for i in range(1, last_sentences_idx):\n",
    "                current_sentence = sentences[i]\n",
    "                last_sentence = paragraphs[-1][-1]\n",
    "                \n",
    "                # Encode Next and Current Sentence\n",
    "                encoding = tokenizer(\n",
    "                    last_sentence,\n",
    "                    current_sentence, \n",
    "                    return_tensors=\"pt\", \n",
    "                    padding=True, \n",
    "                    truncation=True\n",
    "                )\n",
    "                \n",
    "                # Check Relation Between Next and Current Sentences\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**encoding)\n",
    "                    probs = torch.softmax(outputs.logits, dim=1)\n",
    "                    is_related = bool(probs[0][0] > threshold)\n",
    "                    \n",
    "                # Add Next Sentence w/ Current Paragraph List or Create New Paragraph List\n",
    "                if is_related:\n",
    "                    paragraphs[-1].append(current_sentence)\n",
    "                else:\n",
    "                    paragraphs.append([current_sentence])\n",
    "                pbar.update(1)\n",
    "\n",
    "            # Return List of Each Paragraph List as Paragraph Text\n",
    "            return [\" \".join(para) for para in paragraphs]\n",
    "\n",
    "    # Collect List of Paragraphs from Transcription Files\n",
    "    transcription_files = os.listdir(transcription_output_path)\n",
    "    total_transcription_file = len(transcription_files)\n",
    "    with tqdm(total=total_transcription_file, desc=\"Segmenting Transcripts\") as pbar:\n",
    "        for index, filename in enumerate(transcription_files):\n",
    "            current = f'{index+1}/{total_transcription_file}'\n",
    "            if filename == \".ipynb_checkpoints\":\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            pbar.set_description(f'Splitting Transcripts [{current} Transcript] ')\n",
    "\n",
    "            # Open Transcription File\n",
    "            file_path = os.path.join(transcription_output_path, filename)\n",
    "            with open(file_path, \"r\") as file:\n",
    "                text = file.read()\n",
    "                \n",
    "                # Split into Sentences\n",
    "                sentences = list(set(sent_tokenize(text)))\n",
    "    \n",
    "                # Segment into Paragraphs\n",
    "                list_of_paragraphs.extend(para_tokenize(sentences, current))\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "    # Save List of All Paragraphs into CSV file\n",
    "    df = pd.DataFrame(list(set(list_of_paragraphs)), columns=[\"Paragraph\"])\n",
    "    df.to_csv(transcript_sentences_filename, index=False)\n",
    "    return df\n",
    "\n",
    "list_of_paragraphs = process_transcripts_into_csv_of_paragraphs()\n",
    "print(f'Number of Paragraph: {len(list_of_paragraphs)}')\n",
    "list_of_paragraphs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic: Relevant Sentence Filtering (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 06:55:25,049 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969e207754fe435f98b53f12292a78f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/211 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 06:56:40,390 - BERTopic - Embedding - Completed ✓\n",
      "2024-10-21 06:56:40,390 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-10-21 06:56:52,225 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-10-21 06:56:52,233 - BERTopic - Zeroshot Step 1 - Finding documents that could be assigned to either one of the zero-shot topics\n",
      "2024-10-21 06:57:00,142 - BERTopic - Zeroshot Step 1 - Completed ✓\n",
      "2024-10-21 06:57:21,582 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-10-21 06:57:21,954 - BERTopic - Cluster - Completed ✓\n",
      "2024-10-21 06:57:21,954 - BERTopic - Zeroshot Step 2 - Combining topics from zero-shot topic modeling with topics from clustering...\n",
      "2024-10-21 06:57:21,979 - BERTopic - Zeroshot Step 2 - Completed ✓\n",
      "2024-10-21 06:57:21,979 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-10-21 06:57:36,277 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Related Sentences: 1304\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Presidential_Candidate</th>\n",
       "      <th>State</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump is winning Pennsylvania.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[trump pennsylvania, donald trump, donald, pen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump is taking Pennsylvania.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[trump pennsylvania, pennsylvania, trump, , , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But they would say we're going to stop Magga.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[magga, maga, magga maga, donald trump, butler...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As you can see, I'm not just MAGA, I'm Dark MAGA.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[magga, maga, magga maga, donald trump, butler...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So months after that assassination bid on form...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[magga, maga, magga maga, donald trump, butler...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Presidential_Candidate  \\\n",
       "0              Donald Trump is winning Pennsylvania.           Donald Trump   \n",
       "1                      Trump is taking Pennsylvania.           Donald Trump   \n",
       "2      But they would say we're going to stop Magga.           Donald Trump   \n",
       "3  As you can see, I'm not just MAGA, I'm Dark MAGA.           Donald Trump   \n",
       "4  So months after that assassination bid on form...           Donald Trump   \n",
       "\n",
       "          State                                     Topic_Keywords  \n",
       "0  Pennsylvania  [trump pennsylvania, donald trump, donald, pen...  \n",
       "1  Pennsylvania  [trump pennsylvania, pennsylvania, trump, , , ...  \n",
       "2  Pennsylvania  [magga, maga, magga maga, donald trump, butler...  \n",
       "3  Pennsylvania  [magga, maga, magga maga, donald trump, butler...  \n",
       "4  Pennsylvania  [magga, maga, magga maga, donald trump, butler...  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_related_sentences_from_related_paragraphs() -> tuple[DataFrame, BERTopic]:\n",
    "    # Get All Collected Paragraphs from Transcript\n",
    "    df = pd.read_csv(transcript_sentences_filename)\n",
    "    paragraphs = df[\"Paragraph\"].tolist()\n",
    "    \n",
    "    # Set Filter for Words as Possible Topics\n",
    "    def filter_possible_topics(text: str) -> list:\n",
    "        \"\"\"\n",
    "        Filter Words If its a Possible Topic:\n",
    "            1) Only Nouns and Proper Nouns (e.g. Dollars, Currency)\n",
    "            2) No Stop Words (e.g. in, to)\n",
    "            3) No Generic Abstract Nouns (e.g. thing, stuff)\n",
    "            4) Minumum of Three Letter Words (e.g. USA)\n",
    "            5) Exclude Numbers\n",
    "        \"\"\"\n",
    "        \n",
    "        pos_tags = pos_tag(word_tokenize(text)) # POS Tagging\n",
    "        possible_topics = [\n",
    "            token.lower() for token, pos in pos_tags\n",
    "            if pos in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"] # Nouns / Proper Nouns\n",
    "            and token.lower() not in stop_words # Exclude Stop Words\n",
    "            and token.lower() not in generic_abstract_nouns # Exclude Generic Abstract Nouns\n",
    "            and len(token) > 2 # Exclude One/Two Letter Words\n",
    "            and not token.isnumeric() # Exclude Numbers\n",
    "        ]\n",
    "        \n",
    "        return possible_topics\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        ngram_range=(1, max_consecutive_words_for_topic),\n",
    "        tokenizer=filter_possible_topics\n",
    "    )\n",
    "\n",
    "    # Train BERTopic model\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        n_gram_range=(1, max_consecutive_words_for_topic),\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        zeroshot_topic_list=presidential_candidates_and_states_combinations,\n",
    "        verbose=True\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(paragraphs)\n",
    "    \n",
    "    # Get BERTopic Results\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    topics_and_documents = pd.DataFrame({\"Topic\": topics, \"Representative_Docs\": paragraphs})\n",
    "    \n",
    "    # Initialize Lists for Related Sentences\n",
    "    list_of_related_sentences = []\n",
    "\n",
    "    def is_sentence_complete(sentence: str) -> bool:\n",
    "        \"\"\"\n",
    "        Its a Complete Sentence If:\n",
    "            1) It has Atleast 1 Noun or Pronoun\n",
    "            2) It has Atleast 1 Verb\n",
    "            3) Minimum of 5 or N Words\n",
    "        \"\"\"\n",
    "        word_tokens = word_tokenize(sentence) # Tokenize Sentence into Words\n",
    "\n",
    "        if len(word_tokens) < minimum_number_of_word_in_related_sentence: return False # Exclude Sentence with Less than 5 or N Words\n",
    "\n",
    "        pos_tags = pos_tag(word_tokens) # POS Tagging\n",
    "        has_subject = any(tag in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"] for _, tag in pos_tags) # Exclude Sentence w/out Noun and Pronoun\n",
    "        has_verb = any(tag.startswith(\"VB\") for _, tag in pos_tags) # Exclude Sentence w/out Verb*\n",
    "    \n",
    "        return has_subject and has_verb\n",
    "        \n",
    "    # Get Related Sentences from Related Paragraphs\n",
    "    for _, row in topic_info.iterrows():\n",
    "        topic = row[\"Topic\"]\n",
    "        if topic == -1: continue # Skip Outlier\n",
    "\n",
    "        # Get List of Topics and its Corresponding Paragraphs\n",
    "        topic_keywords = row[\"Representation\"]\n",
    "        related_sentences = topics_and_documents[topics_and_documents[\"Topic\"] == topic][\"Representative_Docs\"].tolist()\n",
    "        \n",
    "        # Check Candidate Mentions in Topics\n",
    "        presidential_candidate_mentions = set() # Avoid Duplicates\n",
    "        for presidential_candidate, names in presidential_candidates.items():\n",
    "            if (\n",
    "                any(name.lower() in keyword.lower() for name in names for keyword in topic_keywords) \n",
    "                or any(presidential_candidate.lower() in keyword.lower() for keyword in topic_keywords)\n",
    "            ): \n",
    "                presidential_candidate_mentions.add(presidential_candidate)\n",
    "        \n",
    "        # Make Sure Only 1 Candidate is Mentioned\n",
    "        if len(presidential_candidate_mentions) != 1: continue\n",
    "\n",
    "        # Check State Mentions in Topics (Including Cities)\n",
    "        state_mentions = set() # Avoid Duplicates\n",
    "        for state, cities in state_cities.items():\n",
    "            if (\n",
    "                any(city.lower() in keyword.lower() for city in cities for keyword in topic_keywords) \n",
    "                or any(state.lower() in keyword.lower() for keyword in topic_keywords)\n",
    "            ): \n",
    "                state_mentions.add(state)\n",
    "\n",
    "        # Make Sure Only 1 State is Mentioned\n",
    "        if len(state_mentions) != 1: continue\n",
    "        \"\"\"\n",
    "        Add Related Sentences Only If:\n",
    "            1) Only 1 Candidate is Mentioned\n",
    "            2) Only 1 State is Mentioned\n",
    "        \"\"\"\n",
    "        if (\n",
    "            len(presidential_candidate_mentions) == 1\n",
    "            and len(state_mentions) == 1\n",
    "        ):\n",
    "            presidential_candidate = presidential_candidate_mentions.pop()\n",
    "            state = state_mentions.pop()\n",
    "\n",
    "            # Get and Filter Sentences in Related Paragraphs\n",
    "            related_sentences = [\n",
    "                sentence for paragraph in related_sentences\n",
    "                for sentence in sent_tokenize(paragraph)\n",
    "                if is_sentence_complete(sentence)\n",
    "            ]\n",
    "            \n",
    "            # Add All Related Sentences with their Corresponding Presidential Candidate, State, and Topic Keywords\n",
    "            for sentence in related_sentences:\n",
    "                list_of_related_sentences.append({\n",
    "                    \"Sentence\": sentence,\n",
    "                    \"Presidential_Candidate\": presidential_candidate,\n",
    "                    \"State\": state,\n",
    "                    \"Topic_Keywords\": topic_keywords\n",
    "                })\n",
    "    \n",
    "    # Save List of All Related Sentences into CSV file\n",
    "    df = pd.DataFrame(list_of_related_sentences)\n",
    "    df.to_csv(related_transcript_sentences_filename, index=False)\n",
    "    return df, topic_model\n",
    "\n",
    "list_of_related_sentences, bertopic_model = get_related_sentences_from_related_paragraphs()\n",
    "print(f'Number of Related Sentences: {len(list_of_related_sentences)}')\n",
    "list_of_related_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>3634</td>\n",
       "      <td>-1_trump_pennsylvania_people_state</td>\n",
       "      <td>[trump, pennsylvania, people, state, president...</td>\n",
       "      <td>[Let's look at hurricane relief, which has bee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>kamala harris_nogales</td>\n",
       "      <td>[kamala harris, kamala, harris, , , , , , , ]</td>\n",
       "      <td>[Kamala Harris will take you all. First one.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>kamala_nogales</td>\n",
       "      <td>[kamala, , , , , , , , , ]</td>\n",
       "      <td>[Kamala.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>kamala harris_york</td>\n",
       "      <td>[answer harris, answer, harris, , , , , , , ]</td>\n",
       "      <td>[So yes, is the answer. Kamala Harris.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>kamala harris_ypsilanti</td>\n",
       "      <td>[people harris, people, harris, , , , , , , ]</td>\n",
       "      <td>[Do people really believe that Kamala Harris of.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>88</td>\n",
       "      <td>99</td>\n",
       "      <td>88_poll_points_trump poll_vote</td>\n",
       "      <td>[poll, points, trump poll, vote, polls, vote t...</td>\n",
       "      <td>[Let's find out together. Pennsylvania has a v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>89</td>\n",
       "      <td>144</td>\n",
       "      <td>89_county_pennsylvania_counties_state</td>\n",
       "      <td>[county, pennsylvania, counties, state, philad...</td>\n",
       "      <td>[We're going to actually look at voter registr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>90_pennsylvania winner_favorite trump_firewall...</td>\n",
       "      <td>[pennsylvania winner, favorite trump, firewall...</td>\n",
       "      <td>[Cumberland, Donald Trump won that by 18 and 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>91</td>\n",
       "      <td>55</td>\n",
       "      <td>91_biden_haley_county_primary</td>\n",
       "      <td>[biden, haley, county, primary, pennsylvania, ...</td>\n",
       "      <td>[So that's not, though, where the whole story ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>92</td>\n",
       "      <td>51</td>\n",
       "      <td>92_biden_president biden_biden trump_joe</td>\n",
       "      <td>[biden, president biden, biden trump, joe, joe...</td>\n",
       "      <td>[And I think what people will end up rememberi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                               Name  \\\n",
       "0      -1   3634                 -1_trump_pennsylvania_people_state   \n",
       "1       0      1                              kamala harris_nogales   \n",
       "2       1      1                                     kamala_nogales   \n",
       "3       2      1                                 kamala harris_york   \n",
       "4       3      1                            kamala harris_ypsilanti   \n",
       "..    ...    ...                                                ...   \n",
       "89     88     99                     88_poll_points_trump poll_vote   \n",
       "90     89    144              89_county_pennsylvania_counties_state   \n",
       "91     90     11  90_pennsylvania winner_favorite trump_firewall...   \n",
       "92     91     55                      91_biden_haley_county_primary   \n",
       "93     92     51           92_biden_president biden_biden trump_joe   \n",
       "\n",
       "                                       Representation  \\\n",
       "0   [trump, pennsylvania, people, state, president...   \n",
       "1       [kamala harris, kamala, harris, , , , , , , ]   \n",
       "2                          [kamala, , , , , , , , , ]   \n",
       "3       [answer harris, answer, harris, , , , , , , ]   \n",
       "4       [people harris, people, harris, , , , , , , ]   \n",
       "..                                                ...   \n",
       "89  [poll, points, trump poll, vote, polls, vote t...   \n",
       "90  [county, pennsylvania, counties, state, philad...   \n",
       "91  [pennsylvania winner, favorite trump, firewall...   \n",
       "92  [biden, haley, county, primary, pennsylvania, ...   \n",
       "93  [biden, president biden, biden trump, joe, joe...   \n",
       "\n",
       "                                  Representative_Docs  \n",
       "0   [Let's look at hurricane relief, which has bee...  \n",
       "1       [Kamala Harris will take you all. First one.]  \n",
       "2                                           [Kamala.]  \n",
       "3             [So yes, is the answer. Kamala Harris.]  \n",
       "4   [Do people really believe that Kamala Harris of.]  \n",
       "..                                                ...  \n",
       "89  [Let's find out together. Pennsylvania has a v...  \n",
       "90  [We're going to actually look at voter registr...  \n",
       "91  [Cumberland, Donald Trump won that by 18 and 1...  \n",
       "92  [So that's not, though, where the whole story ...  \n",
       "93  [And I think what people will end up rememberi...  \n",
       "\n",
       "[94 rows x 5 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertopic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_5188c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_5188c_level0_col0\" class=\"col_heading level0 col0\" >Presidential_Candidate</th>\n",
       "      <th id=\"T_5188c_level0_col1\" class=\"col_heading level0 col1\" >State</th>\n",
       "      <th id=\"T_5188c_level0_col2\" class=\"col_heading level0 col2\" >count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_5188c_row0_col0\" class=\"data row0 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_5188c_row0_col1\" class=\"data row0 col1\" >Michigan</td>\n",
       "      <td id=\"T_5188c_row0_col2\" class=\"data row0 col2\" >71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5188c_row1_col0\" class=\"data row1 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_5188c_row1_col1\" class=\"data row1 col1\" >Pennsylvania</td>\n",
       "      <td id=\"T_5188c_row1_col2\" class=\"data row1 col2\" >409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5188c_row2_col0\" class=\"data row2 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_5188c_row2_col1\" class=\"data row2 col1\" >Pennsylvania</td>\n",
       "      <td id=\"T_5188c_row2_col2\" class=\"data row2 col2\" >824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2739e68c2d0>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sa tingin ko need natin 5k sentences minimum for Related di lang for gathered.\n",
    "Kasi mamaya 5k Unrelated Sentences nakuha natin tas 100 lang dun Related with candidate & state.\n",
    "\n",
    "Ang naiisip ko since 6 Combination = 3 candidate * 2 state\n",
    "Gawin natin 5000/6 = 834 Related Sentences required set natin as minimum per Combination\n",
    "\n",
    "Trump  - Arizona      = 834 Related Sentences\n",
    "Harris - Arizona      = 834 Related Sentences\n",
    "Trump  - Michigan     = 834 Related Sentences\n",
    "Harris - Michigan     = 834 Related Sentences\n",
    "Trump  - Pennsylvania = 834 Related Sentences\n",
    "Harris - Pennsylvania = 834 Related Sentences\n",
    "                     --------------------------\n",
    "                      ~5000 Related Sentences\n",
    "\"\"\"\n",
    "def print_statistics():\n",
    "    try:\n",
    "        return (\n",
    "            pd\n",
    "            .read_csv(related_transcript_sentences_filename)\n",
    "            .groupby([\"Presidential_Candidate\", \"State\"])\n",
    "            .size()\n",
    "            .reset_index(name=\"count\")\n",
    "            .style.hide(axis=\"index\")\n",
    "        )\n",
    "    except: return \"No Related Sentences\"\n",
    "        \n",
    "print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
