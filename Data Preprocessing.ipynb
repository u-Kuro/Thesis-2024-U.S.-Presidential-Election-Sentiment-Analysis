{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-11-08T23:14:10.037342Z",
     "start_time": "2024-11-08T23:14:10.020332Z"
    }
   },
   "source": [
    "# Import Dependencies\n",
    "import os, re, nltk\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Additional Downloads\n",
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Define Utilities"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-11-08T23:14:10.058101Z",
     "start_time": "2024-11-08T23:14:10.050876Z"
    }
   },
   "source": [
    "def sanitize_filename(filename: str) -> str:\n",
    "    # Escape Double Quotes\n",
    "    filename = filename.replace('\"', '\\\\\"')\n",
    "\n",
    "    # Replace Invalid Characters with \"_\"\n",
    "    invalid_chars = re.compile(r'[<>:\"/\\\\|?*]')\n",
    "    sanitized_filename = invalid_chars.sub(\"_\", filename)\n",
    "\n",
    "    return sanitized_filename\n",
    "    \n",
    "def read_unique_items_from_file(file: str) -> list:\n",
    "    if os.path.exists(file):\n",
    "        with open(file, \"r\", errors=\"ignore\") as f:\n",
    "            return list(set(e.strip() for e in f.readlines() if e.strip()))\n",
    "    return []"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Set Configurations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-11-08T23:14:10.099038Z",
     "start_time": "2024-11-08T23:14:10.068863Z"
    }
   },
   "source": [
    "# File Names\n",
    "transcript_sentences_filename = \"transcript_sentences.csv\"\n",
    "relevant_transcript_sentences_filename = \"relevant_transcript_sentences.csv\"\n",
    "\n",
    "# Folder Names\n",
    "transcription_path = \"Transcription\"\n",
    "cities_transcription_paths = {\n",
    "    \"Michigan\": os.path.join(transcription_path, \"Michigan\"),\n",
    "    \"Arizona\": os.path.join(transcription_path, \"Arizona\"),\n",
    "    \"Pennsylvania\": os.path.join(transcription_path, \"Pennsylvania\"),\n",
    "}\n",
    "cities_path = \"State Cities\"\n",
    "\n",
    "# Numeric Constants \n",
    "max_pair_of_words_for_topic = 2 # e.g. Unigram(1): \"Donald\" | Bigram(2): \"Donald Trump\"\n",
    "min_number_of_word_in_relevant_sentence = 5 # Only Accepts 5-Word Sentence as Relevant | e.g. \"This is a nice place\"\n",
    "min_similarity_of_topic_modeling = 0.7 # 70% | Range:[0.1, 1] | Minimum Similarity for Topic Assignment | Higher Value Means Stricter Match\n",
    "max_topic_count = None # None or auto or number | Numeric Value may Include Unrelated Sentences | Lower Count Can Remove Topics for Trump and Harris But Higher Chance to Have More Relevant Sentences\n",
    "\n",
    "# Sentence Categories\n",
    "presidential_candidates = {\n",
    "    \"Donald Trump\": [\n",
    "        \"Donald\", \"Trump\"\n",
    "    ],\n",
    "    \"Kamala Harris\": [\n",
    "        \"Kamala\", \"Harris\"\n",
    "    ]\n",
    "}\n",
    "original_state_cities = [\"Arizona\", \"Michigan\", \"Pennsylvania\"]\n",
    "state_cities = {\n",
    "    \"Arizona\": read_unique_items_from_file(os.path.join(cities_path, \"arizona-cities.txt\")),\n",
    "    \"Michigan\": read_unique_items_from_file(os.path.join(cities_path, \"michigan-cities.txt\")),\n",
    "    \"Pennsylvania\": read_unique_items_from_file(os.path.join(cities_path, \"pennsylvania-cities.txt\")),\n",
    "    \"Alabama\": [\"AL\", \"A.L\"],\n",
    "    \"Alaska\": [\"AK\", \"A.K\"],\n",
    "    \"Arkansas\": [\"AR\", \"A.R\"],\n",
    "    \"California\": [\"CA\", \"C.A\"],\n",
    "    \"Colorado\": [\"CO\", \"C.O\"],\n",
    "    \"Connecticut\": [\"CT\", \"C.T\"],\n",
    "    \"Delaware\": [\"DE\", \"D.E\"],\n",
    "    \"Florida\": [\"FL\", \"F.L\"],\n",
    "    \"Georgia\": [\"GA\", \"G.A\"],\n",
    "    \"Hawaii\": [\"HI\", \"H.I\"],\n",
    "    \"Idaho\": [\"ID\", \"I.D\"],\n",
    "    \"Illinois\": [\"IL\", \"I.L\"],\n",
    "    \"Indiana\": [\"IN\", \"I.N\"],\n",
    "    \"Iowa\": [\"IA\", \"I.A\"],\n",
    "    \"Kansas\": [\"KS\", \"K.S\"],\n",
    "    \"Kentucky\": [\"KY\", \"K.Y\"],\n",
    "    \"Louisiana\": [\"LA\", \"L.A\"],\n",
    "    \"Maine\": [\"ME\", \"M.E\"],\n",
    "    \"Maryland\": [\"MD\", \"M.D\"],\n",
    "    \"Massachusetts\": [\"MA\", \"M.A\"],\n",
    "    \"Minnesota\": [\"MN\", \"M.N\"],\n",
    "    \"Mississippi\": [\"MS\", \"M.S\"],\n",
    "    \"Missouri\": [\"MO\", \"M.O\"],\n",
    "    \"Montana\": [\"MT\", \"M.T\"],\n",
    "    \"Nebraska\": [\"NE\", \"N.E\"],\n",
    "    \"Nevada\": [\"NV\", \"N.V\"],\n",
    "    \"New Hampshire\": [\"NH\", \"N.H\"],\n",
    "    \"New Jersey\": [\"NJ\", \"N.J\"],\n",
    "    \"New Mexico\": [\"NM\", \"N.M\"],\n",
    "    \"New York\": [\"NY\", \"N.Y\"],\n",
    "    \"North Carolina\": [\"NC\", \"N.C\"],\n",
    "    \"North Dakota\": [\"ND\", \"N.D\"],\n",
    "    \"Ohio\": [\"OH\", \"O.H\"],\n",
    "    \"Oklahoma\": [\"OK\", \"O.K\"],\n",
    "    \"Oregon\": [\"OR\", \"O.R\"],\n",
    "    \"Rhode Island\": [\"RI\", \"R.I\"],\n",
    "    \"South Carolina\": [\"SC\", \"S.C\"],\n",
    "    \"South Dakota\": [\"SD\", \"S.D\"],\n",
    "    \"Tennessee\": [\"TN\", \"T.N\"],\n",
    "    \"Texas\": [\"TX\", \"T.X\"],\n",
    "    \"Utah\": [\"UT\", \"U.T\"],\n",
    "    \"Vermont\": [\"VT\", \"V.T\"],\n",
    "    \"Virginia\": [\"VA\", \"V.A\"],\n",
    "    \"Washington\": [\"WA\", \"W.A\"],\n",
    "    \"West Virginia\": [\"WV\", \"W.V\"],\n",
    "    \"Wisconsin\": [\"WI\", \"W.I\"],\n",
    "    \"Wyoming\": [\"WY\", \"W.Y\"],\n",
    "}\n",
    "\n",
    "# Words for Sentence Filtering\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Additional Preprocessing of Configurations\n",
    "presidential_candidates = {presidential_candidate: list(set(names)) for presidential_candidate, names in presidential_candidates.items()}\n",
    "presidential_candidates_combinations = [\n",
    "    name.lower()\n",
    "    for full_name, names in presidential_candidates.items()\n",
    "    for name in ([full_name] if max_pair_of_words_for_topic >= 2 else []) + names\n",
    "]\n",
    "presidential_candidates_combinations_in_2d = [\n",
    "    ([full_name.lower()] if max_pair_of_words_for_topic >= 2 else []) + [name.lower() for name in names]\n",
    "    for full_name, names in presidential_candidates.items()\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Sentence Extraction (Transcripts to CSV)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-11-08T23:14:12.464860Z",
     "start_time": "2024-11-08T23:14:10.117009Z"
    }
   },
   "source": [
    "def process_transcripts_into_csv_of_sentences() -> pd.DataFrame:\n",
    "    # Initialize list of sentences and possible states\n",
    "    list_of_sentences = []\n",
    "\n",
    "    # Collect sentences from each state's transcription files\n",
    "    for state, path in cities_transcription_paths.items():\n",
    "        transcription_files = os.listdir(path)\n",
    "        total_transcription_files = len(transcription_files)\n",
    "\n",
    "        with tqdm(total=total_transcription_files, desc=f'Collecting Sentences for {state} [0/{total_transcription_files} Transcript]') as pbar:\n",
    "            for index, filename in enumerate(transcription_files):\n",
    "                current = f'{index + 1}/{total_transcription_files}'\n",
    "                if filename == \".ipynb_checkpoints\":\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                pbar.set_description(f'Collecting Sentences for {state} [{current} Transcript]')\n",
    "\n",
    "                # Open transcription file\n",
    "                file_path = os.path.join(path, filename)\n",
    "                with open(file_path, \"r\", errors=\"ignore\") as file:\n",
    "                    transcription = file.read()\n",
    "\n",
    "                    # Split transcript into sentences\n",
    "                    sentences = sent_tokenize(transcription)\n",
    "\n",
    "                    # Remove consecutive duplicates\n",
    "                    sentences = [sentence for i, sentence in enumerate(sentences) if i == 0 or sentence != sentences[i - 1]]\n",
    "\n",
    "                    # Append each sentence with the state name\n",
    "                    list_of_sentences.extend([(sentence, state) for sentence in sentences])\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Convert the list of sentences and states into a DataFrame\n",
    "    df = pd.DataFrame(list(set(list_of_sentences)), columns=[\"Sentence\", \"Possible_State\"])\n",
    "    df.to_csv(transcript_sentences_filename, index=False, errors=\"ignore\")\n",
    "    return df\n",
    "\n",
    "# Run the function and print summary\n",
    "list_of_sentences = process_transcripts_into_csv_of_sentences()\n",
    "print(f'Number of Sentences: {len(list_of_sentences)}')\n",
    "list_of_sentences"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting Sentences for Michigan [0/260 Transcript]:   0%|          | 0/260 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d4e4a4d0f34c457e8351a9aeaec0dc16"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Collecting Sentences for Arizona [0/168 Transcript]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e8be0b8882b42c0aee3d61e74bd75c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Collecting Sentences for Pennsylvania [0/268 Transcript]:   0%|          | 0/268 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc0c5a27179f4278a951675fc29b4708"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences: 56797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                Sentence Possible_State\n",
       "0      Ladies and gentlemen, this is a remarkable tim...       Michigan\n",
       "1             And I think there's pushback against that.       Michigan\n",
       "2      Yeah, so I agree with Schmidt that it's danger...        Arizona\n",
       "3      You had libertarians on the ballot and yeah, t...        Arizona\n",
       "4      Clean energy focus keeps Washington reliably b...        Arizona\n",
       "...                                                  ...            ...\n",
       "56792  There are thousands and thousands of illegal i...       Michigan\n",
       "56793                                         Off by 12.   Pennsylvania\n",
       "56794  Karl yesterday Michael Watley who has the RNC ...        Arizona\n",
       "56795  I think it's going to be the most important el...       Michigan\n",
       "56796         I am not saying that she will win Arizona.        Arizona\n",
       "\n",
       "[56797 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Possible_State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ladies and gentlemen, this is a remarkable tim...</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>And I think there's pushback against that.</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yeah, so I agree with Schmidt that it's danger...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You had libertarians on the ballot and yeah, t...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clean energy focus keeps Washington reliably b...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56792</th>\n",
       "      <td>There are thousands and thousands of illegal i...</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56793</th>\n",
       "      <td>Off by 12.</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56794</th>\n",
       "      <td>Karl yesterday Michael Watley who has the RNC ...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56795</th>\n",
       "      <td>I think it's going to be the most important el...</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56796</th>\n",
       "      <td>I am not saying that she will win Arizona.</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56797 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# BERTopic: Relevant Sentence Filtering (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-11-08T23:19:13.076360Z",
     "start_time": "2024-11-08T23:14:12.506537Z"
    }
   },
   "source": [
    "def filter_relevant_sentences() -> tuple[DataFrame, BERTopic]:\n",
    "    # Get All Collected Sentences from Transcript and a Map with their Respective Possible State\n",
    "    df = pd.read_csv(transcript_sentences_filename, encoding_errors=\"ignore\")\n",
    "    sentences_possible_state = pd.Series(df['Possible_State'].values, index=df['Sentence']).to_dict()\n",
    "    sentences = df[\"Sentence\"].tolist()\n",
    "    \n",
    "    # Define Filter for Words as Possible Topics\n",
    "    def filter_possible_topics(text: str) -> list:\n",
    "        \"\"\"\n",
    "            Filter Words If it's a Possible Topic:\n",
    "                1) Only Nouns and Proper Nouns (e.g. Dollars, Currency)\n",
    "                2) No Stop Words (e.g. in, to)\n",
    "                3) Minimum of Two-Letter Words (e.g. Ox)\n",
    "                4) Exclude Numbers\n",
    "        \"\"\"\n",
    "        pos_tags = pos_tag(word_tokenize(text)) # POS Tagging\n",
    "        # Return Possible Topics\n",
    "        return [\n",
    "            token.lower() for token, pos in pos_tags\n",
    "            if pos in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"] # Nouns / Proper Nouns\n",
    "            and token.lower() not in stop_words # Exclude Stop Words\n",
    "            and len(token) > 1 # Exclude One-Letter Words (e.g. Included: Ox)\n",
    "            and not token.isnumeric() # Exclude Numbers\n",
    "        ]\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        ngram_range=(1, max_pair_of_words_for_topic),\n",
    "        tokenizer=filter_possible_topics\n",
    "    )\n",
    "\n",
    "    # Train BERTopic model\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        n_gram_range=(1, max_pair_of_words_for_topic),\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        seed_topic_list=presidential_candidates_combinations_in_2d,\n",
    "        zeroshot_topic_list=presidential_candidates_combinations,\n",
    "        zeroshot_min_similarity=min_similarity_of_topic_modeling,\n",
    "        nr_topics=None if max_topic_count is None else \"auto\" if max_topic_count == \"auto\" else max(len(presidential_candidates_combinations), max_topic_count),\n",
    "        verbose=True\n",
    "    )\n",
    "    topic_ids, _ = topic_model.fit_transform(sentences)\n",
    "    \n",
    "    # Get BERTopic Results\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    topics_and_documents = pd.DataFrame({\"Topic\": topic_ids, \"Representative_Docs\": sentences})\n",
    "\n",
    "    # Initialize Lists for Relevant Sentences\n",
    "    list_of_relevant_sentences = []\n",
    "    \n",
    "    # Define Filters for Relevant Sentences\n",
    "    \"\"\"\n",
    "        Add Relevant Sentences Only If:\n",
    "            1) Only 1 Candidate is Mentioned in the Topic\n",
    "            2) No Other State is Mentioned in the Topic Different from Possible State\n",
    "            3) Sentence has Word Count Greater than N or 5\n",
    "    \"\"\"\n",
    "    def get_only_if_1_candidate_mentioned_in_the_topic(topic_ngramed_keywords: list[str]) ->  str | None:\n",
    "        # Collect Candidate Mentions in Topics\n",
    "        presidential_candidate_mentions = set() # Avoid Duplicates\n",
    "        for presidential_candidate, names in presidential_candidates.items():\n",
    "            if (\n",
    "                # Any Candidate is Mentioned in Topic\n",
    "                any(\n",
    "                    (\n",
    "                        presidential_candidate and ngramed_keyword\n",
    "                        and f' {presidential_candidate.strip().lower()} ' in f' {ngramed_keyword.strip().lower()} '\n",
    "                    ) or (\n",
    "                        presidential_candidate and word\n",
    "                        and presidential_candidate.strip().lower() == word.strip().lower()\n",
    "                    )\n",
    "                    for ngramed_keyword in topic_ngramed_keywords\n",
    "                    for word in ngramed_keyword.split(\" \")\n",
    "                )\n",
    "                # Any Other Candidate Names is Mentioned in Topic\n",
    "                or any(\n",
    "                    (\n",
    "                        name and ngramed_keyword\n",
    "                        and f' {name.strip().lower()} ' in f' {ngramed_keyword.strip().lower()} '\n",
    "                    ) or (\n",
    "                        name and word\n",
    "                        and name.strip().lower() == word.strip().lower()\n",
    "                    )\n",
    "                    for name in names\n",
    "                    for ngramed_keyword in topic_ngramed_keywords\n",
    "                    for word in ngramed_keyword.split(\" \")\n",
    "                )\n",
    "            ):\n",
    "                # Add The Candidate Mentioned\n",
    "                presidential_candidate_mentions.add(presidential_candidate)\n",
    "        # Return the Candidate If It's the Only 1 Mentioned\n",
    "        if len(presidential_candidate_mentions) == 1:\n",
    "            return presidential_candidate_mentions.pop()\n",
    "        else:\n",
    "            return None\n",
    "    def get_if_no_other_state_mentioned_in_topic_different_from_possible_state(topic_ngramed_keywords: list[str], sentence: str) ->  str | None:\n",
    "        # Get Possible State for the Sentence\n",
    "        possible_state = sentences_possible_state[sentence]\n",
    "        if possible_state not in state_cities: raise ValueError(f'This Sentence has Invalid Possible State ({possible_state}): \"{sentence}\"')\n",
    "        # Filter Sentence with Topic of [Other State] Not in [Arizona, Michigan, Pennsylvania]\n",
    "        if possible_state not in original_state_cities: return None\n",
    "        # Filter Sentence with Topic of [Other State] Different from its [Possible State]\n",
    "        other_states = [state for state in state_cities if state is not possible_state]\n",
    "        if any(\n",
    "            f' {other_state.strip().lower()} ' in f' {ngramed_keyword.strip().lower()} '\n",
    "            or (\n",
    "                word\n",
    "                and other_state.strip().lower() == word.strip().lower()\n",
    "            )\n",
    "            for other_state in other_states\n",
    "            for ngramed_keyword in topic_ngramed_keywords\n",
    "            for word in ngramed_keyword.split(\" \")\n",
    "        ): return None\n",
    "        # Filter Sentence with Topics of [Other States' Cities] Different from its [Possible State Cities]\n",
    "        other_state_cities = [\n",
    "            other_city\n",
    "            for other_cities in {\n",
    "                state: state_cities[state]\n",
    "                for state in state_cities\n",
    "                if state is not possible_state\n",
    "            }.values()\n",
    "            for other_city in other_cities\n",
    "            if other_city\n",
    "        ]\n",
    "        if any(\n",
    "            f' {other_city.strip().lower()} ' in f' {ngramed_keyword.strip().lower()} '\n",
    "            or (\n",
    "                word\n",
    "                and other_city.strip().lower() == word.strip().lower()\n",
    "            )\n",
    "            for other_city in other_state_cities\n",
    "            for ngramed_keyword in topic_ngramed_keywords\n",
    "            for word in ngramed_keyword.split(\" \")\n",
    "        ): return None\n",
    "        # Return the Possible State\n",
    "        return possible_state\n",
    "    def sentence_has_word_count_greater_than_n(sentence: str, min_number_of_word_in_relevant_sentence: int = min_number_of_word_in_relevant_sentence) -> bool:\n",
    "        # Only include word tags\n",
    "        word_tags = {\n",
    "            \"CC\",  # conjunctions (and, or, but)\n",
    "            \"CD\",  # cardinal numbers\n",
    "            \"DT\",  # determiners (the, a, this)\n",
    "            \"EX\",  # existential there\n",
    "            \"FW\",  # foreign words\n",
    "            \"IN\",  # prepositions\n",
    "            \"JJ\", \"JJR\", \"JJS\",  # adjectives\n",
    "            \"LS\",  # List markers (First, Second, One, Two, A, B, etc.)\n",
    "            \"MD\",  # modals (can, should)\n",
    "            \"NN\", \"NNP\", \"NNPS\", \"NNS\",  # nouns\n",
    "            \"PDT\",  # pre-determiners\n",
    "            \"PRP\", \"PRP$\",  # pronouns\n",
    "            \"RB\", \"RBR\", \"RBS\",  # adverbs\n",
    "            \"RP\",  # particles\n",
    "            \"TO\",  # to\n",
    "            \"UH\",  # interjections\n",
    "            \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\",  # verbs\n",
    "            \"WDT\", \"WP\", \"WP$\", \"WRB\"  # wh-words\n",
    "        }\n",
    "        pos_tags = pos_tag(word_tokenize(sentence)) # POS Tagging\n",
    "        word_count = sum(1 for word, pos in pos_tags if pos in word_tags)\n",
    "        return word_count >= min_number_of_word_in_relevant_sentence\n",
    "\n",
    "    # Get Relevant Sentences\n",
    "    for _, row in topic_info.iterrows():\n",
    "        topic_id = row[\"Topic\"]\n",
    "        if topic_id == -1: continue # Skip Outlier\n",
    "    \n",
    "        # Get List of Topics and their Sentences\n",
    "        topic_ngramed_keywords = [\n",
    "            ngramed_keyword \n",
    "            for ngramed_keyword in row[\"Representation\"]\n",
    "            if ngramed_keyword\n",
    "        ]\n",
    "        topic_sentences = topics_and_documents[topics_and_documents[\"Topic\"] == topic_id][\"Representative_Docs\"].tolist()\n",
    "        \n",
    "        for sentence in topic_sentences:\n",
    "            # Check and Get 1 Candidate from Topics\n",
    "            presidential_candidate = get_only_if_1_candidate_mentioned_in_the_topic(topic_ngramed_keywords)\n",
    "            if presidential_candidate is None: continue\n",
    "            \n",
    "            # Check and Get 1 State from Topics and [Possible State assigned in Sentence] \n",
    "            state = get_if_no_other_state_mentioned_in_topic_different_from_possible_state(topic_ngramed_keywords, sentence)\n",
    "            if state is None: continue\n",
    "            \n",
    "            # Check if sentence has word count greater than N (default: 5)\n",
    "            if not sentence_has_word_count_greater_than_n(sentence): continue\n",
    "            \n",
    "            # Add Relevant Sentence with their Respective Candidate and State\n",
    "            list_of_relevant_sentences.append({\n",
    "                \"Sentence\": sentence,\n",
    "                \"Presidential_Candidate\": presidential_candidate,\n",
    "                \"State\": state,\n",
    "                \"Topic_Keywords\": topic_ngramed_keywords\n",
    "            })\n",
    "    \n",
    "    # Save List of All Relevant Sentences into CSV file\n",
    "    df = pd.DataFrame(list_of_relevant_sentences)\n",
    "    df.to_csv(relevant_transcript_sentences_filename, index=False, errors=\"ignore\")\n",
    "    return df, topic_model\n",
    "\n",
    "list_of_relevant_sentences, bertopic_model = filter_relevant_sentences()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 07:14:12,756 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1775 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "597178959cfc41a5a4685d3d3b44c588"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 07:16:31,652 - BERTopic - Embedding - Completed ✓\n",
      "2024-11-09 07:16:31,652 - BERTopic - Guided - Find embeddings highly related to seeded topics.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec7bde507a484a34afaa4423844343eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 07:16:31,961 - BERTopic - Guided - Completed ✓\n",
      "2024-11-09 07:16:31,961 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-11-09 07:17:16,878 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-11-09 07:17:16,882 - BERTopic - Zeroshot Step 1 - Finding documents that could be assigned to either one of the zero-shot topics\n",
      "2024-11-09 07:17:17,274 - BERTopic - Zeroshot Step 1 - Completed ✓\n",
      "2024-11-09 07:17:48,367 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-11-09 07:17:53,144 - BERTopic - Cluster - Completed ✓\n",
      "2024-11-09 07:17:53,144 - BERTopic - Zeroshot Step 2 - Combining topics from zero-shot topic modeling with topics from clustering...\n",
      "2024-11-09 07:17:53,269 - BERTopic - Zeroshot Step 2 - Completed ✓\n",
      "2024-11-09 07:17:53,269 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-11-09 07:18:19,081 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-11-08T23:19:13.151289Z",
     "start_time": "2024-11-08T23:19:13.135557Z"
    }
   },
   "source": "bertopic_model.get_topic_info().sort_values(by=\"Count\", ascending=False)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     Topic  Count                                               Name  \\\n",
       "0       -1  25582                    -1_trump_harris_election_people   \n",
       "253    252   1139            252_biden_joe biden_joe_president biden   \n",
       "3        2    960                                      kamala harris   \n",
       "447    446    688                   446_blah_man_blah blah_sleazebag   \n",
       "261    260    437           260_momentum_answer_settings_month point   \n",
       "..     ...    ...                                                ...   \n",
       "22      21     10               21_gaza_regard_israel_support israel   \n",
       "323    322     10  322_attack trump_pence trump_extent remarks_ad...   \n",
       "327    326     10  326_bunch democrats_chester county_chester_mercer   \n",
       "405    404     10  404_suburbs_suburbs suburbs_outer suburbs_nash...   \n",
       "2        1      4                                              trump   \n",
       "\n",
       "                                        Representation  \\\n",
       "0    [trump, harris, election, people, state, arizo...   \n",
       "253  [biden, joe biden, joe, president biden, biden...   \n",
       "3    [kamala harris, kamala, harris, president kama...   \n",
       "447  [blah, man, blah blah, sleazebag, guy, job, ki...   \n",
       "261  [momentum, answer, settings, month point, answ...   \n",
       "..                                                 ...   \n",
       "22   [gaza, regard, israel, support israel, year ga...   \n",
       "323  [attack trump, pence trump, extent remarks, ad...   \n",
       "327  [bunch democrats, chester county, chester, mer...   \n",
       "405  [suburbs, suburbs suburbs, outer suburbs, nash...   \n",
       "2    [trump president, trump trump, president trump...   \n",
       "\n",
       "                                   Representative_Docs  \n",
       "0    [He almost sounded like Trump talking., Some a...  \n",
       "253  [Why are you voting for Joe Biden?, And in 202...  \n",
       "3    [What do you think about Kamala Harris?, But K...  \n",
       "447  [He's like having Jeeves change his tire., He'...  \n",
       "261  [I think she's got some momentum., Does she ha...  \n",
       "..                                                 ...  \n",
       "22   [But if these voters are principally concerned...  \n",
       "323  [All she does one attack, vitriolically, Donal...  \n",
       "327  [Matt Mercer, a state GOP spokesman, argued Re...  \n",
       "405  [And finally, we talk about the suburbs., They...  \n",
       "2                   [Trump?, President Trump., Trump.]  \n",
       "\n",
       "[621 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>25582</td>\n",
       "      <td>-1_trump_harris_election_people</td>\n",
       "      <td>[trump, harris, election, people, state, arizo...</td>\n",
       "      <td>[He almost sounded like Trump talking., Some a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>252</td>\n",
       "      <td>1139</td>\n",
       "      <td>252_biden_joe biden_joe_president biden</td>\n",
       "      <td>[biden, joe biden, joe, president biden, biden...</td>\n",
       "      <td>[Why are you voting for Joe Biden?, And in 202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>960</td>\n",
       "      <td>kamala harris</td>\n",
       "      <td>[kamala harris, kamala, harris, president kama...</td>\n",
       "      <td>[What do you think about Kamala Harris?, But K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>446</td>\n",
       "      <td>688</td>\n",
       "      <td>446_blah_man_blah blah_sleazebag</td>\n",
       "      <td>[blah, man, blah blah, sleazebag, guy, job, ki...</td>\n",
       "      <td>[He's like having Jeeves change his tire., He'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>260</td>\n",
       "      <td>437</td>\n",
       "      <td>260_momentum_answer_settings_month point</td>\n",
       "      <td>[momentum, answer, settings, month point, answ...</td>\n",
       "      <td>[I think she's got some momentum., Does she ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>21_gaza_regard_israel_support israel</td>\n",
       "      <td>[gaza, regard, israel, support israel, year ga...</td>\n",
       "      <td>[But if these voters are principally concerned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>322</td>\n",
       "      <td>10</td>\n",
       "      <td>322_attack trump_pence trump_extent remarks_ad...</td>\n",
       "      <td>[attack trump, pence trump, extent remarks, ad...</td>\n",
       "      <td>[All she does one attack, vitriolically, Donal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>326</td>\n",
       "      <td>10</td>\n",
       "      <td>326_bunch democrats_chester county_chester_mercer</td>\n",
       "      <td>[bunch democrats, chester county, chester, mer...</td>\n",
       "      <td>[Matt Mercer, a state GOP spokesman, argued Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>404</td>\n",
       "      <td>10</td>\n",
       "      <td>404_suburbs_suburbs suburbs_outer suburbs_nash...</td>\n",
       "      <td>[suburbs, suburbs suburbs, outer suburbs, nash...</td>\n",
       "      <td>[And finally, we talk about the suburbs., They...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>trump</td>\n",
       "      <td>[trump president, trump trump, president trump...</td>\n",
       "      <td>[Trump?, President Trump., Trump.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>621 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-11-08T23:19:13.198676Z",
     "start_time": "2024-11-08T23:19:13.186939Z"
    }
   },
   "source": "list_of_relevant_sentences",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               Sentence  \\\n",
       "0              Oh, wait, do you mean that Donald Trump?   \n",
       "1                            Donald Trump has the edge.   \n",
       "2     After all these years, we know who Donald Trum...   \n",
       "3                  I want to see the real Donald Trump.   \n",
       "4                      We all know who Donald Trump is.   \n",
       "...                                                 ...   \n",
       "6408                               We'll be right back.   \n",
       "6409                              We're not going back.   \n",
       "6410                       We're going to get them out.   \n",
       "6411                    We just want to get back to it.   \n",
       "6412                               We'll be right back.   \n",
       "\n",
       "     Presidential_Candidate         State  \\\n",
       "0              Donald Trump       Arizona   \n",
       "1              Donald Trump      Michigan   \n",
       "2              Donald Trump  Pennsylvania   \n",
       "3              Donald Trump       Arizona   \n",
       "4              Donald Trump      Michigan   \n",
       "...                     ...           ...   \n",
       "6408           Donald Trump  Pennsylvania   \n",
       "6409           Donald Trump  Pennsylvania   \n",
       "6410           Donald Trump  Pennsylvania   \n",
       "6411           Donald Trump       Arizona   \n",
       "6412           Donald Trump  Pennsylvania   \n",
       "\n",
       "                                         Topic_Keywords  \n",
       "0     [trump trump, trump donald, trump president, t...  \n",
       "1     [trump trump, trump donald, trump president, t...  \n",
       "2     [trump trump, trump donald, trump president, t...  \n",
       "3     [trump trump, trump donald, trump president, t...  \n",
       "4     [trump trump, trump donald, trump president, t...  \n",
       "...                                                 ...  \n",
       "6408  [forth want, window forth, want jim, rundown w...  \n",
       "6409  [forth want, window forth, want jim, rundown w...  \n",
       "6410  [forth want, window forth, want jim, rundown w...  \n",
       "6411  [forth want, window forth, want jim, rundown w...  \n",
       "6412  [forth want, window forth, want jim, rundown w...  \n",
       "\n",
       "[6413 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Presidential_Candidate</th>\n",
       "      <th>State</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oh, wait, do you mean that Donald Trump?</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>[trump trump, trump donald, trump president, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has the edge.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>[trump trump, trump donald, trump president, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After all these years, we know who Donald Trum...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[trump trump, trump donald, trump president, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I want to see the real Donald Trump.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>[trump trump, trump donald, trump president, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We all know who Donald Trump is.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>[trump trump, trump donald, trump president, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6408</th>\n",
       "      <td>We'll be right back.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[forth want, window forth, want jim, rundown w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6409</th>\n",
       "      <td>We're not going back.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[forth want, window forth, want jim, rundown w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6410</th>\n",
       "      <td>We're going to get them out.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[forth want, window forth, want jim, rundown w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6411</th>\n",
       "      <td>We just want to get back to it.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>[forth want, window forth, want jim, rundown w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6412</th>\n",
       "      <td>We'll be right back.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[forth want, window forth, want jim, rundown w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6413 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-11-08T23:19:13.290045Z",
     "start_time": "2024-11-08T23:19:13.274271Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Sa tingin ko need natin 5k sentences minimum for Relevant Sentences di lang for gathered.\n",
    "Kasi mamaya 5k Random Sentences nakuha natin tas 100 lang dun Relevant with candidate & state.\n",
    "\n",
    "Ang naiisip ko since meron 6 Combinations = 3 candidate * 2 state\n",
    "Gawin natin 5000/6 = 834 Relevant Sentences required set natin as minimum per Combination\n",
    "\n",
    "Trump  - Arizona      = 834 Relevant Sentences\n",
    "Harris - Arizona      = 834 Relevant Sentences\n",
    "Trump  - Michigan     = 834 Relevant Sentences\n",
    "Harris - Michigan     = 834 Relevant Sentences\n",
    "Trump  - Pennsylvania = 834 Relevant Sentences\n",
    "Harris - Pennsylvania = 834 Relevant Sentences\n",
    "               -------------------------------\n",
    "               Total: ~5000 Relevant Sentences\n",
    "\"\"\"\n",
    "def print_statistics():\n",
    "    try:\n",
    "        grouped_df = (\n",
    "            list_of_relevant_sentences\n",
    "            .groupby([\"Presidential_Candidate\", \"State\"])\n",
    "            .size()\n",
    "            .reset_index(name=\"count\")\n",
    "        )\n",
    "        total_count = grouped_df[\"count\"].sum()\n",
    "        total_row = pd.DataFrame({\"Presidential_Candidate\": [\"\"], \"State\": [\"Total\"], \"count\": [total_count]})\n",
    "        grouped_df = pd.concat([grouped_df, total_row], ignore_index=True)\n",
    "        return grouped_df.style.hide(axis=\"index\")\n",
    "    except: return \"No Relevant Sentences\"\n",
    "print_statistics()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1f48f15b490>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_2e72d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_2e72d_level0_col0\" class=\"col_heading level0 col0\" >Presidential_Candidate</th>\n",
       "      <th id=\"T_2e72d_level0_col1\" class=\"col_heading level0 col1\" >State</th>\n",
       "      <th id=\"T_2e72d_level0_col2\" class=\"col_heading level0 col2\" >count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_2e72d_row0_col0\" class=\"data row0 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_2e72d_row0_col1\" class=\"data row0 col1\" >Arizona</td>\n",
       "      <td id=\"T_2e72d_row0_col2\" class=\"data row0 col2\" >1041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2e72d_row1_col0\" class=\"data row1 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_2e72d_row1_col1\" class=\"data row1 col1\" >Michigan</td>\n",
       "      <td id=\"T_2e72d_row1_col2\" class=\"data row1 col2\" >1553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2e72d_row2_col0\" class=\"data row2 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_2e72d_row2_col1\" class=\"data row2 col1\" >Pennsylvania</td>\n",
       "      <td id=\"T_2e72d_row2_col2\" class=\"data row2 col2\" >1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2e72d_row3_col0\" class=\"data row3 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_2e72d_row3_col1\" class=\"data row3 col1\" >Arizona</td>\n",
       "      <td id=\"T_2e72d_row3_col2\" class=\"data row3 col2\" >335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2e72d_row4_col0\" class=\"data row4 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_2e72d_row4_col1\" class=\"data row4 col1\" >Michigan</td>\n",
       "      <td id=\"T_2e72d_row4_col2\" class=\"data row4 col2\" >583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2e72d_row5_col0\" class=\"data row5 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_2e72d_row5_col1\" class=\"data row5 col1\" >Pennsylvania</td>\n",
       "      <td id=\"T_2e72d_row5_col2\" class=\"data row5 col2\" >941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2e72d_row6_col0\" class=\"data row6 col0\" ></td>\n",
       "      <td id=\"T_2e72d_row6_col1\" class=\"data row6 col1\" >Total</td>\n",
       "      <td id=\"T_2e72d_row6_col2\" class=\"data row6 col2\" >6413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T23:19:13.345215Z",
     "start_time": "2024-11-08T23:19:13.342901Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
