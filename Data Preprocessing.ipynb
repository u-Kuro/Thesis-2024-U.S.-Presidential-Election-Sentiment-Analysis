{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MSI Laptop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Main Dependencies\n",
    "import os, re, nltk\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import Other Dependencies\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Additional Downloads\n",
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename: str) -> str:\n",
    "    # Escape Double Quotes\n",
    "    filename = filename.replace('\"', '\\\\\"')\n",
    "\n",
    "    # Replace Invalid Characters with \"_\"\n",
    "    invalid_chars = re.compile(r'[<>:\"/\\\\|?*]')\n",
    "    sanitized_filename = invalid_chars.sub(\"_\", filename)\n",
    "\n",
    "    return sanitized_filename\n",
    "    \n",
    "def read_unique_items_from_file(file: str) -> list:\n",
    "    with open(file, \"r\") as f:\n",
    "        return list(set(url.strip() for url in f.readlines() if url.strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# File Names\n",
    "transcript_sentences_filename = \"transcript_sentences.csv\"\n",
    "related_transcript_sentences_filename = \"related_transcript_sentences.csv\"\n",
    "\n",
    "# Folder Names\n",
    "transcription_output_path = \"Transcription\"\n",
    "cities_path = \"State Cities\"\n",
    "\n",
    "# Boolean Flags\n",
    "remove_video = True\n",
    "remove_audio = True\n",
    "\n",
    "# Numeric Constants \n",
    "max_consecutive_words_for_topic = 2 # e.g. Unigram: \"Donald\" | Bigram: \"Donald Trump\" | Trigram: \"President Donald Trump\"\n",
    "\n",
    "# Sentence Categories\n",
    "presidential_candidates = {\n",
    "    \"Donald Trump\": [\n",
    "        \"Donald\", \"Trump\"\n",
    "    ],\n",
    "    \"Kamala Harris\": [\n",
    "        \"Kamala\", \"Harris\"\n",
    "    ]\n",
    "}\n",
    "state_cities = {\n",
    "    \"Michigan\": read_unique_items_from_file(os.path.join(cities_path, \"michigan-cities.txt\")),\n",
    "    \"Arizona\": read_unique_items_from_file(os.path.join(cities_path, \"arizona-cities.txt\")),\n",
    "    \"Pennsylvania\": read_unique_items_from_file(os.path.join(cities_path, \"pennsylvania-cities.txt\"))\n",
    "}\n",
    "\n",
    "# Words for Sentence Filtering\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "generic_abstract_nouns = {\n",
    "    \"thing\", \"stuff\", \"event\",\n",
    "    \"aspect\", \"issue\", \"place\",\n",
    "    \"person\"\n",
    "}\n",
    "\n",
    "# Additional Preprocessing of Configurations\n",
    "presidential_candidates = {presidential_candidate: list(set(names)) for presidential_candidate, names in presidential_candidates.items()}\n",
    "presidential_candidates_and_states_combinations = [\n",
    "    f\"{pattern}_{loc}\".lower() for name, parts in presidential_candidates.items() \n",
    "    for loc in [state for state in state_cities] + [city for cities in state_cities.values() for city in cities]\n",
    "    for pattern in [name, '_'.join(parts), parts[0], parts[1]]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Extraction (Transcripts to CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551e77004764429daf5ea8c3582f2bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Sentences from Transcripts:   0%|          | 0/269 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences: 20478\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>And the people of Pennsylvania were better off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She doesn't even live in the district, but tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Well, the polls are going back and forth like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Okay, I'm not going to go forever on this, but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Don't try to sanitize it and color it up for t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence\n",
       "0  And the people of Pennsylvania were better off...\n",
       "1  She doesn't even live in the district, but tha...\n",
       "2   Well, the polls are going back and forth like...\n",
       "3  Okay, I'm not going to go forever on this, but...\n",
       "4  Don't try to sanitize it and color it up for t..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_transcripts_into_csv_of_sentences() -> DataFrame:\n",
    "    # Initialize List of Sentences\n",
    "    list_of_sentences = []\n",
    "    \n",
    "    def is_sentence_complete(sentence: str) -> bool:\n",
    "        \"\"\"\n",
    "        Its a Proper Sentence If:\n",
    "            1) It has Atleast 1 Noun or Pronoun\n",
    "            2) It has Atleast 1 Verb\n",
    "        \"\"\"\n",
    "        pos_tags = pos_tag(word_tokenize(sentence)) # POS Tagging\n",
    "        has_subject = any(tag in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"] for _, tag in pos_tags) # Exclude Sentence w/out Noun and Pronoun\n",
    "        has_verb = any(tag.startswith(\"VB\") for _, tag in pos_tags) # Exclude Sentence w/out Verb\n",
    "    \n",
    "        return has_subject and has_verb\n",
    "    \n",
    "    # Collect List of All Sentences from Transcripts\n",
    "    transcription_files = os.listdir(transcription_output_path)\n",
    "    with tqdm(total=len(transcription_files), desc=\"Collecting Sentences from Transcripts\") as pbar:\n",
    "        for filename in transcription_files:\n",
    "            if filename == \".ipynb_checkpoints\":\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "                \n",
    "            file_path = os.path.join(transcription_output_path, filename)\n",
    "            \n",
    "            with open(file_path, \"r\") as file:\n",
    "                text = file.read()\n",
    "                \n",
    "                # Split into Sentences\n",
    "                sentences = list(set(sent_tokenize(text)))\n",
    "    \n",
    "                # Filter Proper Sentences (With Noun/Proper-Noun and Verb)\n",
    "                sentences = [sentence for sentence in sentences if is_sentence_complete(sentence)]\n",
    "                \n",
    "                list_of_sentences.extend(sentences)\n",
    "                \n",
    "            pbar.update(1)\n",
    "\n",
    "    # Save List of All Sentences into CSV file\n",
    "    df = pd.DataFrame(list(set(list_of_sentences)), columns=[\"Sentence\"])\n",
    "    df.to_csv(transcript_sentences_filename, index=False)\n",
    "    return df\n",
    "\n",
    "list_of_sentences = process_transcripts_into_csv_of_sentences()\n",
    "print(f'Number of Sentences: {len(list_of_sentences)}')\n",
    "list_of_sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic: Relevant Sentence Filtering (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 02:38:38,045 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6edc341f5e04f2fa2305df10a574d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 02:39:42,103 - BERTopic - Embedding - Completed ✓\n",
      "2024-10-21 02:39:42,103 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-10-21 02:40:14,139 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-10-21 02:40:14,169 - BERTopic - Zeroshot Step 1 - Finding documents that could be assigned to either one of the zero-shot topics\n",
      "2024-10-21 02:40:19,813 - BERTopic - Zeroshot Step 1 - Completed ✓\n",
      "2024-10-21 02:40:37,479 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-10-21 02:40:41,905 - BERTopic - Cluster - Completed ✓\n",
      "2024-10-21 02:40:41,905 - BERTopic - Zeroshot Step 2 - Combining topics from zero-shot topic modeling with topics from clustering...\n",
      "2024-10-21 02:40:41,975 - BERTopic - Zeroshot Step 2 - Completed ✓\n",
      "2024-10-21 02:40:41,979 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-10-21 02:40:53,101 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Related Sentences: 591\n",
      "Number of Topic Clusters: 269\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>9486</td>\n",
       "      <td>-1_trump_state_harris_states</td>\n",
       "      <td>[trump, state, harris, states, election, penns...</td>\n",
       "      <td>[You just need one more state and you've won, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>trump_michigan</td>\n",
       "      <td>[trump way, president trump, way, president, t...</td>\n",
       "      <td>[President Trump is on his way to Michigan sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>kamala harris_washington</td>\n",
       "      <td>[life children, trump politicians, children li...</td>\n",
       "      <td>[Now you have Kamala Harris versus Trump., Pol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>kamala harris_ypsilanti</td>\n",
       "      <td>[people harris, people, harris, , , , , , , ]</td>\n",
       "      <td>[Do people really believe that Kamala Harris of.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>donald trump_pennsylvania</td>\n",
       "      <td>[lead pennsylvania, trump lead, lead, donald t...</td>\n",
       "      <td>[Donald Trump is in the lead in Pennsylvania.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>263</td>\n",
       "      <td>41</td>\n",
       "      <td>263_question question_question_question answer...</td>\n",
       "      <td>[question question, question, question answer,...</td>\n",
       "      <td>[And just repeat my question., I have answered...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>264</td>\n",
       "      <td>33</td>\n",
       "      <td>264_reason_reasons_hell opposite_ingraham cases</td>\n",
       "      <td>[reason, reasons, hell opposite, ingraham case...</td>\n",
       "      <td>[There's a reason for that., Give me one reaso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>265</td>\n",
       "      <td>27</td>\n",
       "      <td>265_problem problem_problem_problem problems_p...</td>\n",
       "      <td>[problem problem, problem, problem problems, p...</td>\n",
       "      <td>[That's the problem., So it is a problem., You...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>266</td>\n",
       "      <td>17</td>\n",
       "      <td>266_case_case case_point case_use course</td>\n",
       "      <td>[case, case case, point case, use course, long...</td>\n",
       "      <td>[That's always the case., That's no longer the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>267</td>\n",
       "      <td>16</td>\n",
       "      <td>267_sense_sense sense_lot sense_sense lot</td>\n",
       "      <td>[sense, sense sense, lot sense, sense lot, sen...</td>\n",
       "      <td>[This makes no sense., It doesn't make sense.,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>269 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Count                                               Name  \\\n",
       "0       -1   9486                       -1_trump_state_harris_states   \n",
       "1        0      1                                     trump_michigan   \n",
       "2        1      4                           kamala harris_washington   \n",
       "3        2      1                            kamala harris_ypsilanti   \n",
       "4        3      1                          donald trump_pennsylvania   \n",
       "..     ...    ...                                                ...   \n",
       "264    263     41  263_question question_question_question answer...   \n",
       "265    264     33    264_reason_reasons_hell opposite_ingraham cases   \n",
       "266    265     27  265_problem problem_problem_problem problems_p...   \n",
       "267    266     17           266_case_case case_point case_use course   \n",
       "268    267     16          267_sense_sense sense_lot sense_sense lot   \n",
       "\n",
       "                                        Representation  \\\n",
       "0    [trump, state, harris, states, election, penns...   \n",
       "1    [trump way, president trump, way, president, t...   \n",
       "2    [life children, trump politicians, children li...   \n",
       "3        [people harris, people, harris, , , , , , , ]   \n",
       "4    [lead pennsylvania, trump lead, lead, donald t...   \n",
       "..                                                 ...   \n",
       "264  [question question, question, question answer,...   \n",
       "265  [reason, reasons, hell opposite, ingraham case...   \n",
       "266  [problem problem, problem, problem problems, p...   \n",
       "267  [case, case case, point case, use course, long...   \n",
       "268  [sense, sense sense, lot sense, sense lot, sen...   \n",
       "\n",
       "                                   Representative_Docs  \n",
       "0    [You just need one more state and you've won, ...  \n",
       "1    [President Trump is on his way to Michigan sho...  \n",
       "2    [Now you have Kamala Harris versus Trump., Pol...  \n",
       "3    [Do people really believe that Kamala Harris of.]  \n",
       "4       [Donald Trump is in the lead in Pennsylvania.]  \n",
       "..                                                 ...  \n",
       "264  [And just repeat my question., I have answered...  \n",
       "265  [There's a reason for that., Give me one reaso...  \n",
       "266  [That's the problem., So it is a problem., You...  \n",
       "267  [That's always the case., That's no longer the...  \n",
       "268  [This makes no sense., It doesn't make sense.,...  \n",
       "\n",
       "[269 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_related_sentences() -> tuple[DataFrame, BERTopic]:\n",
    "    # Get All Sentences from Transcript\n",
    "    df = pd.read_csv(transcript_sentences_filename)\n",
    "    sentences = df[\"Sentence\"].tolist()\n",
    "    \n",
    "    # Set Filter for Words as Possible Topics\n",
    "    def filter_possible_topics(text: str) -> list:\n",
    "        \"\"\"\n",
    "        Filter Words If its a Possible Topic:\n",
    "            1) Only Nouns and Proper Nouns (e.g. Dollars, Currency)\n",
    "            2) No Stop Words (e.g. in, to)\n",
    "            3) No Generic Abstract Nouns (e.g. thing, stuff)\n",
    "            4) Minumum of Three Letter Words (e.g. USA)\n",
    "            5) Exclude Numbers\n",
    "        \"\"\"\n",
    "        \n",
    "        pos_tags = pos_tag(word_tokenize(text)) # POS Tagging\n",
    "        possible_topics = [\n",
    "            token.lower() for token, pos in pos_tags\n",
    "            if pos in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"] # Nouns / Proper Nouns\n",
    "            and token.lower() not in stop_words # Exclude Stop Words\n",
    "            and token.lower() not in generic_abstract_nouns # Exclude Generic Abstract Nouns\n",
    "            and len(token) > 2 # Exclude One/Two Letter Words\n",
    "            and not token.isnumeric() # Exclude Numbers\n",
    "        ]\n",
    "        \n",
    "        return possible_topics\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        ngram_range=(1, max_consecutive_words_for_topic),\n",
    "        tokenizer=filter_possible_topics\n",
    "    )\n",
    "\n",
    "    # Train BERTopic model\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        n_gram_range=(1, max_consecutive_words_for_topic),\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        zeroshot_topic_list=presidential_candidates_and_states_combinations,\n",
    "        verbose=True\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(sentences)\n",
    "    \n",
    "    # Get BERTopic Results\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    topics_and_documents = pd.DataFrame({\"Topic\": topics, \"Representative_Docs\": sentences})\n",
    "    \n",
    "    # Initialize Lists for our filtered results\n",
    "    list_of_related_sentences = []\n",
    "\n",
    "    # Analyze each topic row in topic_info\n",
    "    for _, row in topic_info.iterrows():\n",
    "        topic = row[\"Topic\"]\n",
    "        if topic == -1: continue # Skip Outlier\n",
    "\n",
    "        # Get List of Topics and its Related Sentences\n",
    "        topic_keywords = row[\"Representation\"]\n",
    "        related_sentences = topics_and_documents[topics_and_documents[\"Topic\"] == topic][\"Representative_Docs\"].tolist()\n",
    "        \n",
    "        # Check Candidate Mentions in Topics\n",
    "        presidential_candidate_mentions = set() # Avoid Duplicates\n",
    "        for presidential_candidate, names in presidential_candidates.items():\n",
    "            if (\n",
    "                any(name.lower() in keyword.lower() for name in names for keyword in topic_keywords) \n",
    "                or any(presidential_candidate.lower() in keyword.lower() for keyword in topic_keywords)\n",
    "            ): \n",
    "                presidential_candidate_mentions.add(presidential_candidate)\n",
    "        \n",
    "        # Make Sure Only 1 Candidate is Mentioned\n",
    "        if len(presidential_candidate_mentions) != 1: continue\n",
    "\n",
    "        # Check State Mentions in Topics (Including Cities)\n",
    "        state_mentions = set() # Avoid Duplicates\n",
    "        for state, cities in state_cities.items():\n",
    "            if (\n",
    "                any(city.lower() in keyword.lower() for city in cities for keyword in topic_keywords) \n",
    "                or any(state.lower() in keyword.lower() for keyword in topic_keywords)\n",
    "            ): \n",
    "                state_mentions.add(state)\n",
    "\n",
    "        # # Make Sure Only 1 State is Mentioned\n",
    "        if len(state_mentions) != 1: continue\n",
    "        \"\"\"\n",
    "        Add Related Sentences Only If:\n",
    "            1) Only 1 Candidate is Mentioned\n",
    "            2) Only 1 State is Mentioned\n",
    "        \"\"\"\n",
    "        if (\n",
    "            len(presidential_candidate_mentions) == 1\n",
    "            and len(state_mentions) == 1\n",
    "        ):\n",
    "            presidential_candidate = presidential_candidate_mentions.pop()\n",
    "            state = state_mentions.pop()\n",
    "\n",
    "            # Add All Related Sentences with Corresponding Presidential Candidate, State, and Topic Keywords\n",
    "            for sentence in related_sentences:\n",
    "                list_of_related_sentences.append({\n",
    "                    \"Sentence\": sentence,\n",
    "                    \"Presidential_Candidate\": presidential_candidate,\n",
    "                    \"State\": state,\n",
    "                    \"Topic_Keywords\": topic_keywords\n",
    "                })\n",
    "    \n",
    "    # Save List of All Related Sentences into CSV file\n",
    "    df = pd.DataFrame(list_of_related_sentences)\n",
    "    df.to_csv(related_transcript_sentences_filename, index=False)\n",
    "    return df, topic_model\n",
    "\n",
    "list_of_related_sentences, bertopic_model = filter_related_sentences()\n",
    "print(f'Number of Related Sentences: {len(list_of_related_sentences)}')\n",
    "print(f'Number of Topic Clusters: {len(bertopic_model.get_topic_info())}')\n",
    "bertopic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Presidential_Candidate</th>\n",
       "      <th>State</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump is in the lead in Pennsylvania.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[lead pennsylvania, trump lead, lead, donald t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump is winning Pennsylvania.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[trump work, leads pennsylvania, trump leads, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But now Donald Trump leads in Pennsylvania.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[trump work, leads pennsylvania, trump leads, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Donald Trump putting in the work also in Penns...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[trump work, leads pennsylvania, trump leads, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump is taking Pennsylvania.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[pennsylvania significance, part pennsylvania,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Presidential_Candidate  \\\n",
       "0       Donald Trump is in the lead in Pennsylvania.           Donald Trump   \n",
       "1              Donald Trump is winning Pennsylvania.           Donald Trump   \n",
       "2        But now Donald Trump leads in Pennsylvania.           Donald Trump   \n",
       "3  Donald Trump putting in the work also in Penns...           Donald Trump   \n",
       "4                      Trump is taking Pennsylvania.           Donald Trump   \n",
       "\n",
       "          State                                     Topic_Keywords  \n",
       "0  Pennsylvania  [lead pennsylvania, trump lead, lead, donald t...  \n",
       "1  Pennsylvania  [trump work, leads pennsylvania, trump leads, ...  \n",
       "2  Pennsylvania  [trump work, leads pennsylvania, trump leads, ...  \n",
       "3  Pennsylvania  [trump work, leads pennsylvania, trump leads, ...  \n",
       "4  Pennsylvania  [pennsylvania significance, part pennsylvania,...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_related_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_6d1e9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_6d1e9_level0_col0\" class=\"col_heading level0 col0\" >Presidential_Candidate</th>\n",
       "      <th id=\"T_6d1e9_level0_col1\" class=\"col_heading level0 col1\" >State</th>\n",
       "      <th id=\"T_6d1e9_level0_col2\" class=\"col_heading level0 col2\" >count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_6d1e9_row0_col0\" class=\"data row0 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_6d1e9_row0_col1\" class=\"data row0 col1\" >Arizona</td>\n",
       "      <td id=\"T_6d1e9_row0_col2\" class=\"data row0 col2\" >42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6d1e9_row1_col0\" class=\"data row1 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_6d1e9_row1_col1\" class=\"data row1 col1\" >Pennsylvania</td>\n",
       "      <td id=\"T_6d1e9_row1_col2\" class=\"data row1 col2\" >457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6d1e9_row2_col0\" class=\"data row2 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_6d1e9_row2_col1\" class=\"data row2 col1\" >Pennsylvania</td>\n",
       "      <td id=\"T_6d1e9_row2_col2\" class=\"data row2 col2\" >92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2737a3a9b10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sa tingin ko need natin 5k sentences minimum for Related di lang for gathered.\n",
    "Kasi mamaya 5k Unrelated Sentences nakuha natin tas 100 lang dun Related with candidate & state.\n",
    "\n",
    "Ang naiisip ko since 6 Combination = 3 candidate * 2 state\n",
    "Gawin natin 5000/6 = 834 Related Sentences required set natin as minimum per Combination\n",
    "\n",
    "Trump  - Arizona      = 834 Related Sentences\n",
    "Harris - Arizona      = 834 Related Sentences\n",
    "Trump  - Michigan     = 834 Related Sentences\n",
    "Harris - Michigan     = 834 Related Sentences\n",
    "Trump  - Pennsylvania = 834 Related Sentences\n",
    "Harris - Pennsylvania = 834 Related Sentences\n",
    "                     --------------------------\n",
    "                      ~5000 Related Sentences\n",
    "\"\"\"\n",
    "\n",
    "(\n",
    "    pd\n",
    "    .read_csv(related_transcript_sentences_filename)\n",
    "    .groupby([\"Presidential_Candidate\", \"State\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    "    .style.hide(axis=\"index\")\n",
    ") if os.path.exists(related_transcript_sentences_filename) else \"No Related Sentences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
