{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-11-08T13:53:10.593877Z",
     "start_time": "2024-11-08T13:52:40.754018Z"
    }
   },
   "source": [
    "# Import Dependencies\n",
    "import os, re, nltk\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Additional Downloads\n",
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MSI Laptop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Define Utilities"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-11-08T13:53:10.620840Z",
     "start_time": "2024-11-08T13:53:10.605895Z"
    }
   },
   "source": [
    "def sanitize_filename(filename: str) -> str:\n",
    "    # Escape Double Quotes\n",
    "    filename = filename.replace('\"', '\\\\\"')\n",
    "\n",
    "    # Replace Invalid Characters with \"_\"\n",
    "    invalid_chars = re.compile(r'[<>:\"/\\\\|?*]')\n",
    "    sanitized_filename = invalid_chars.sub(\"_\", filename)\n",
    "\n",
    "    return sanitized_filename\n",
    "    \n",
    "def read_unique_items_from_file(file: str) -> list:\n",
    "    if os.path.exists(file):\n",
    "        with open(file, \"r\", errors=\"ignore\") as f:\n",
    "            return list(set(e.strip() for e in f.readlines() if e.strip()))\n",
    "    return []"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Set Configurations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-11-08T13:53:10.640326Z",
     "start_time": "2024-11-08T13:53:10.624610Z"
    }
   },
   "source": [
    "# File Names\n",
    "transcript_sentences_filename = \"transcript_sentences.csv\"\n",
    "relevant_transcript_sentences_filename = \"relevant_transcript_sentences.csv\"\n",
    "\n",
    "# Folder Names\n",
    "transcription_path = \"Transcription\"\n",
    "cities_transcription_paths = {\n",
    "    \"Michigan\": os.path.join(transcription_path, \"Michigan\"),\n",
    "    \"Arizona\": os.path.join(transcription_path, \"Arizona\"),\n",
    "    \"Pennsylvania\": os.path.join(transcription_path, \"Pennsylvania\"),\n",
    "}\n",
    "cities_path = \"State Cities\"\n",
    "\n",
    "# Numeric Constants \n",
    "max_pair_of_words_for_topic = 2 # e.g. Unigram(1): \"Donald\" | Bigram(2): \"Donald Trump\"\n",
    "min_number_of_word_in_relevant_sentence = 5 # Only Accepts 5-Word Sentence as Relevant | e.g. \"This is a nice place\"\n",
    "min_similarity_of_topic_modeling = 0.51 # 51% | Range:[0.1, 1] | Minimum Similarity for Topic Assignment | Higher Value Means Stricter Match\n",
    "max_topic_count = 120 # auto or number | Numeric Value may Include Unrelated Sentences | Lower Count Can Remove Topics for Trump and Harris But Higher Chance to Have More Relevant Sentences\n",
    "\n",
    "# Sentence Categories\n",
    "presidential_candidates = {\n",
    "    \"Donald Trump\": [\n",
    "        \"Donald\", \"Trump\"\n",
    "    ],\n",
    "    \"Kamala Harris\": [\n",
    "        \"Kamala\", \"Harris\"\n",
    "    ]\n",
    "}\n",
    "original_state_cities = [\"Arizona\", \"Michigan\", \"Pennsylvania\"]\n",
    "state_cities = {\n",
    "    \"Arizona\": read_unique_items_from_file(os.path.join(cities_path, \"arizona-cities.txt\")),\n",
    "    \"Michigan\": read_unique_items_from_file(os.path.join(cities_path, \"michigan-cities.txt\")),\n",
    "    \"Pennsylvania\": read_unique_items_from_file(os.path.join(cities_path, \"pennsylvania-cities.txt\")),\n",
    "    \"Alabama\": [\"AL\", \"A.L\"],\n",
    "    \"Alaska\": [\"AK\", \"A.K\"],\n",
    "    \"Arkansas\": [\"AR\", \"A.R\"],\n",
    "    \"California\": [\"CA\", \"C.A\"],\n",
    "    \"Colorado\": [\"CO\", \"C.O\"],\n",
    "    \"Connecticut\": [\"CT\", \"C.T\"],\n",
    "    \"Delaware\": [\"DE\", \"D.E\"],\n",
    "    \"Florida\": [\"FL\", \"F.L\"],\n",
    "    \"Georgia\": [\"GA\", \"G.A\"],\n",
    "    \"Hawaii\": [\"HI\", \"H.I\"],\n",
    "    \"Idaho\": [\"ID\", \"I.D\"],\n",
    "    \"Illinois\": [\"IL\", \"I.L\"],\n",
    "    \"Indiana\": [\"IN\", \"I.N\"],\n",
    "    \"Iowa\": [\"IA\", \"I.A\"],\n",
    "    \"Kansas\": [\"KS\", \"K.S\"],\n",
    "    \"Kentucky\": [\"KY\", \"K.Y\"],\n",
    "    \"Louisiana\": [\"LA\", \"L.A\"],\n",
    "    \"Maine\": [\"ME\", \"M.E\"],\n",
    "    \"Maryland\": [\"MD\", \"M.D\"],\n",
    "    \"Massachusetts\": [\"MA\", \"M.A\"],\n",
    "    \"Minnesota\": [\"MN\", \"M.N\"],\n",
    "    \"Mississippi\": [\"MS\", \"M.S\"],\n",
    "    \"Missouri\": [\"MO\", \"M.O\"],\n",
    "    \"Montana\": [\"MT\", \"M.T\"],\n",
    "    \"Nebraska\": [\"NE\", \"N.E\"],\n",
    "    \"Nevada\": [\"NV\", \"N.V\"],\n",
    "    \"New Hampshire\": [\"NH\", \"N.H\"],\n",
    "    \"New Jersey\": [\"NJ\", \"N.J\"],\n",
    "    \"New Mexico\": [\"NM\", \"N.M\"],\n",
    "    \"New York\": [\"NY\", \"N.Y\"],\n",
    "    \"North Carolina\": [\"NC\", \"N.C\"],\n",
    "    \"North Dakota\": [\"ND\", \"N.D\"],\n",
    "    \"Ohio\": [\"OH\", \"O.H\"],\n",
    "    \"Oklahoma\": [\"OK\", \"O.K\"],\n",
    "    \"Oregon\": [\"OR\", \"O.R\"],\n",
    "    \"Rhode Island\": [\"RI\", \"R.I\"],\n",
    "    \"South Carolina\": [\"SC\", \"S.C\"],\n",
    "    \"South Dakota\": [\"SD\", \"S.D\"],\n",
    "    \"Tennessee\": [\"TN\", \"T.N\"],\n",
    "    \"Texas\": [\"TX\", \"T.X\"],\n",
    "    \"Utah\": [\"UT\", \"U.T\"],\n",
    "    \"Vermont\": [\"VT\", \"V.T\"],\n",
    "    \"Virginia\": [\"VA\", \"V.A\"],\n",
    "    \"Washington\": [\"WA\", \"W.A\"],\n",
    "    \"West Virginia\": [\"WV\", \"W.V\"],\n",
    "    \"Wisconsin\": [\"WI\", \"W.I\"],\n",
    "    \"Wyoming\": [\"WY\", \"W.Y\"],\n",
    "}\n",
    "\n",
    "# Words for Sentence Filtering\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Additional Preprocessing of Configurations\n",
    "presidential_candidates = {presidential_candidate: list(set(names)) for presidential_candidate, names in presidential_candidates.items()}\n",
    "presidential_candidates_combinations = [\n",
    "    name.lower()\n",
    "    for full_name, names in presidential_candidates.items()\n",
    "    for name in ([full_name] if max_pair_of_words_for_topic >= 2 else []) + names\n",
    "]\n",
    "presidential_candidates_combinations_in_2d = [\n",
    "    ([full_name.lower()] if max_pair_of_words_for_topic >= 2 else []) + [name.lower() for name in names]\n",
    "    for full_name, names in presidential_candidates.items()\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Sentence Extraction (Transcripts to CSV)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-11-08T13:53:13.109353Z",
     "start_time": "2024-11-08T13:53:11.111429Z"
    }
   },
   "source": [
    "def process_transcripts_into_csv_of_sentences() -> pd.DataFrame:\n",
    "    # Initialize list of sentences and possible states\n",
    "    list_of_sentences = []\n",
    "\n",
    "    # Collect sentences from each state's transcription files\n",
    "    for state, path in cities_transcription_paths.items():\n",
    "        transcription_files = os.listdir(path)\n",
    "        total_transcription_files = len(transcription_files)\n",
    "\n",
    "        with tqdm(total=total_transcription_files, desc=f'Collecting Sentences for {state} [0/{total_transcription_files} Transcript]') as pbar:\n",
    "            for index, filename in enumerate(transcription_files):\n",
    "                current = f'{index + 1}/{total_transcription_files}'\n",
    "                if filename == \".ipynb_checkpoints\":\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                pbar.set_description(f'Collecting Sentences for {state} [{current} Transcript]')\n",
    "\n",
    "                # Open transcription file\n",
    "                file_path = os.path.join(path, filename)\n",
    "                with open(file_path, \"r\", errors=\"ignore\") as file:\n",
    "                    transcription = file.read()\n",
    "\n",
    "                    # Split transcript into sentences\n",
    "                    sentences = sent_tokenize(transcription)\n",
    "\n",
    "                    # Remove consecutive duplicates\n",
    "                    sentences = [sentence for i, sentence in enumerate(sentences) if i == 0 or sentence != sentences[i - 1]]\n",
    "\n",
    "                    # Append each sentence with the state name\n",
    "                    list_of_sentences.extend([(sentence, state) for sentence in sentences])\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Convert the list of sentences and states into a DataFrame\n",
    "    df = pd.DataFrame(list(set(list_of_sentences)), columns=[\"Sentence\", \"Possible_State\"])\n",
    "    df.to_csv(transcript_sentences_filename, index=False, errors=\"ignore\")\n",
    "    return df\n",
    "\n",
    "# Run the function and print summary\n",
    "list_of_sentences = process_transcripts_into_csv_of_sentences()\n",
    "print(f'Number of Sentences: {len(list_of_sentences)}')\n",
    "list_of_sentences"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting Sentences for Michigan [0/260 Transcript]:   0%|          | 0/260 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c096cb2d143b44eb85b665f13d364038"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Collecting Sentences for Arizona [0/168 Transcript]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65797fc4d3274ec298acbb62c15aa4c0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Collecting Sentences for Pennsylvania [0/268 Transcript]:   0%|          | 0/268 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a7ac67b8df947098a2fc7e24c2aac9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences: 56797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                Sentence Possible_State\n",
       "0      I've been talking about the whole concept for ...       Michigan\n",
       "1      The poll results fit a recurring theme with vo...       Michigan\n",
       "2                                    I don't understand.        Arizona\n",
       "3      You've got a lot of people who got an A in spe...       Michigan\n",
       "4                                  Thank you Pittsburgh.   Pennsylvania\n",
       "...                                                  ...            ...\n",
       "56792  You can check out more content from my channel...   Pennsylvania\n",
       "56793  So that's heading in the right direction in th...        Arizona\n",
       "56794  I think the F-22 is the most beautiful fighter...        Arizona\n",
       "56795                        Good morning to you, Marky.       Michigan\n",
       "56796               I've never been a big fan of Harris.        Arizona\n",
       "\n",
       "[56797 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Possible_State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I've been talking about the whole concept for ...</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The poll results fit a recurring theme with vo...</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I don't understand.</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You've got a lot of people who got an A in spe...</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you Pittsburgh.</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56792</th>\n",
       "      <td>You can check out more content from my channel...</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56793</th>\n",
       "      <td>So that's heading in the right direction in th...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56794</th>\n",
       "      <td>I think the F-22 is the most beautiful fighter...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56795</th>\n",
       "      <td>Good morning to you, Marky.</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56796</th>\n",
       "      <td>I've never been a big fan of Harris.</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56797 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# BERTopic: Relevant Sentence Filtering (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-11-08T13:59:25.664706Z",
     "start_time": "2024-11-08T13:53:13.121124Z"
    }
   },
   "source": [
    "def filter_relevant_sentences() -> tuple[DataFrame, BERTopic]:\n",
    "    # Get All Collected Sentences from Transcript and a Map with their Respective Possible State\n",
    "    df = pd.read_csv(transcript_sentences_filename, encoding_errors=\"ignore\")\n",
    "    sentences_possible_state = pd.Series(df['Possible_State'].values, index=df['Sentence']).to_dict()\n",
    "    sentences = df[\"Sentence\"].tolist()\n",
    "    \n",
    "    # Define Filter for Words as Possible Topics\n",
    "    def filter_possible_topics(text: str) -> list:\n",
    "        \"\"\"\n",
    "            Filter Words If it's a Possible Topic:\n",
    "                1) Only Nouns and Proper Nouns (e.g. Dollars, Currency)\n",
    "                2) No Stop Words (e.g. in, to)\n",
    "                3) Minimum of Two-Letter Words (e.g. Ox)\n",
    "                4) Exclude Numbers\n",
    "        \"\"\"\n",
    "        pos_tags = pos_tag(word_tokenize(text)) # POS Tagging\n",
    "        # Return Possible Topics\n",
    "        return [\n",
    "            token.lower() for token, pos in pos_tags\n",
    "            if pos in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"] # Nouns / Proper Nouns\n",
    "            and token.lower() not in stop_words # Exclude Stop Words\n",
    "            and len(token) > 1 # Exclude One-Letter Words (e.g. Included: Ox)\n",
    "            and not token.isnumeric() # Exclude Numbers\n",
    "        ]\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        ngram_range=(1, max_pair_of_words_for_topic),\n",
    "        tokenizer=filter_possible_topics\n",
    "    )\n",
    "\n",
    "    # Train BERTopic model\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        n_gram_range=(1, max_pair_of_words_for_topic),\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        seed_topic_list=presidential_candidates_combinations_in_2d,\n",
    "        zeroshot_topic_list=presidential_candidates_combinations,\n",
    "        zeroshot_min_similarity=min_similarity_of_topic_modeling,\n",
    "        nr_topics=\"auto\" if max_topic_count == \"auto\" else max(len(presidential_candidates_combinations), max_topic_count),\n",
    "        verbose=True\n",
    "    )\n",
    "    topic_ids, _ = topic_model.fit_transform(sentences)\n",
    "    \n",
    "    # Get BERTopic Results\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    topics_and_documents = pd.DataFrame({\"Topic\": topic_ids, \"Representative_Docs\": sentences})\n",
    "\n",
    "    # Initialize Lists for Relevant Sentences\n",
    "    list_of_relevant_sentences = []\n",
    "    \n",
    "    # Define Filters for Relevant Sentences\n",
    "    \"\"\"\n",
    "        Add Relevant Sentences Only If:\n",
    "            1) Only 1 Candidate is Mentioned in the Topic\n",
    "            2) No Other State is Mentioned in the Topic Different from Possible State\n",
    "            3) Sentence has Word Count Greater than N or 5\n",
    "    \"\"\"\n",
    "    def get_only_if_1_candidate_mentioned_in_the_topic(topic_ngramed_keywords: list[str]) ->  str | None:\n",
    "        # Collect Candidate Mentions in Topics\n",
    "        presidential_candidate_mentions = set() # Avoid Duplicates\n",
    "        for presidential_candidate, names in presidential_candidates.items():\n",
    "            if (\n",
    "                # Any Candidate is Mentioned in Topic\n",
    "                any(\n",
    "                    (\n",
    "                        presidential_candidate and ngramed_keyword\n",
    "                        and f' {presidential_candidate.strip().lower()} ' in f' {ngramed_keyword.strip().lower()} '\n",
    "                    ) or (\n",
    "                        presidential_candidate and word\n",
    "                        and presidential_candidate.strip().lower() == word.strip().lower()\n",
    "                    )\n",
    "                    for ngramed_keyword in topic_ngramed_keywords\n",
    "                    for word in ngramed_keyword.split(\" \")\n",
    "                )\n",
    "                # Any Other Candidate Names is Mentioned in Topic\n",
    "                or any(\n",
    "                    (\n",
    "                        name and ngramed_keyword\n",
    "                        and f' {name.strip().lower()} ' in f' {ngramed_keyword.strip().lower()} '\n",
    "                    ) or (\n",
    "                        name and word\n",
    "                        and name.strip().lower() == word.strip().lower()\n",
    "                    )\n",
    "                    for name in names\n",
    "                    for ngramed_keyword in topic_ngramed_keywords\n",
    "                    for word in ngramed_keyword.split(\" \")\n",
    "                )\n",
    "            ):\n",
    "                # Add The Candidate Mentioned\n",
    "                presidential_candidate_mentions.add(presidential_candidate)\n",
    "        # Return the Candidate If It's the Only 1 Mentioned\n",
    "        if len(presidential_candidate_mentions) == 1:\n",
    "            return presidential_candidate_mentions.pop()\n",
    "        else:\n",
    "            return None\n",
    "    def get_if_no_other_state_mentioned_in_topic_different_from_possible_state(topic_ngramed_keywords: list[str], sentence: str) ->  str | None:\n",
    "        # Get Possible State for the Sentence\n",
    "        possible_state = sentences_possible_state[sentence]\n",
    "        if possible_state not in state_cities: raise ValueError(f'This Sentence has Invalid Possible State ({possible_state}): \"{sentence}\"')\n",
    "        # Filter Sentence with Topic of [Other State] Not in [Arizona, Michigan, Pennsylvania]\n",
    "        if possible_state not in original_state_cities: return None\n",
    "        # Filter Sentence with Topic of [Other State] Different from its [Possible State]\n",
    "        other_states = [state for state in state_cities if state is not possible_state]\n",
    "        if any(\n",
    "            f' {other_state.strip().lower()} ' in f' {ngramed_keyword.strip().lower()} '\n",
    "            or (\n",
    "                word\n",
    "                and other_state.strip().lower() == word.strip().lower()\n",
    "            )\n",
    "            for other_state in other_states\n",
    "            for ngramed_keyword in topic_ngramed_keywords\n",
    "            for word in ngramed_keyword.split(\" \")\n",
    "        ): return None\n",
    "        # Filter Sentence with Topics of [Other States' Cities] Different from its [Possible State Cities]\n",
    "        other_state_cities = [\n",
    "            other_city\n",
    "            for other_cities in {\n",
    "                state: state_cities[state]\n",
    "                for state in state_cities\n",
    "                if state is not possible_state\n",
    "            }.values()\n",
    "            for other_city in other_cities\n",
    "            if other_city\n",
    "        ]\n",
    "        if any(\n",
    "            f' {other_city.strip().lower()} ' in f' {ngramed_keyword.strip().lower()} '\n",
    "            or (\n",
    "                word\n",
    "                and other_city.strip().lower() == word.strip().lower()\n",
    "            )\n",
    "            for other_city in other_state_cities\n",
    "            for ngramed_keyword in topic_ngramed_keywords\n",
    "            for word in ngramed_keyword.split(\" \")\n",
    "        ): return None\n",
    "        # Return the Possible State\n",
    "        return possible_state\n",
    "    def sentence_has_word_count_greater_than_n(sentence: str, min_number_of_word_in_relevant_sentence: int = min_number_of_word_in_relevant_sentence) -> bool:\n",
    "        # Only include word tags\n",
    "        word_tags = {\n",
    "            \"CC\",  # conjunctions (and, or, but)\n",
    "            \"CD\",  # cardinal numbers\n",
    "            \"DT\",  # determiners (the, a, this)\n",
    "            \"EX\",  # existential there\n",
    "            \"FW\",  # foreign words\n",
    "            \"IN\",  # prepositions\n",
    "            \"JJ\", \"JJR\", \"JJS\",  # adjectives\n",
    "            \"LS\",  # List markers (First, Second, One, Two, A, B, etc.)\n",
    "            \"MD\",  # modals (can, should)\n",
    "            \"NN\", \"NNP\", \"NNPS\", \"NNS\",  # nouns\n",
    "            \"PDT\",  # pre-determiners\n",
    "            \"PRP\", \"PRP$\",  # pronouns\n",
    "            \"RB\", \"RBR\", \"RBS\",  # adverbs\n",
    "            \"RP\",  # particles\n",
    "            \"TO\",  # to\n",
    "            \"UH\",  # interjections\n",
    "            \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\",  # verbs\n",
    "            \"WDT\", \"WP\", \"WP$\", \"WRB\"  # wh-words\n",
    "        }\n",
    "        pos_tags = pos_tag(word_tokenize(sentence)) # POS Tagging\n",
    "        word_count = sum(1 for word, pos in pos_tags if pos in word_tags)\n",
    "        return word_count >= min_number_of_word_in_relevant_sentence\n",
    "\n",
    "    # Get Relevant Sentences\n",
    "    for _, row in topic_info.iterrows():\n",
    "        topic_id = row[\"Topic\"]\n",
    "        if topic_id == -1: continue # Skip Outlier\n",
    "    \n",
    "        # Get List of Topics and their Sentences\n",
    "        topic_ngramed_keywords = [\n",
    "            ngramed_keyword \n",
    "            for ngramed_keyword in row[\"Representation\"]\n",
    "            if ngramed_keyword\n",
    "        ]\n",
    "        topic_sentences = topics_and_documents[topics_and_documents[\"Topic\"] == topic_id][\"Representative_Docs\"].tolist()\n",
    "        \n",
    "        for sentence in topic_sentences:\n",
    "            # Check and Get 1 Candidate from Topics\n",
    "            presidential_candidate = get_only_if_1_candidate_mentioned_in_the_topic(topic_ngramed_keywords)\n",
    "            if presidential_candidate is None: continue\n",
    "            \n",
    "            # Check and Get 1 State from Topics and [Possible State assigned in Sentence] \n",
    "            state = get_if_no_other_state_mentioned_in_topic_different_from_possible_state(topic_ngramed_keywords, sentence)\n",
    "            if state is None: continue\n",
    "            \n",
    "            # Check if sentence has word count greater than N (default: 5)\n",
    "            if not sentence_has_word_count_greater_than_n(sentence): continue\n",
    "            \n",
    "            # Add Relevant Sentence with their Respective Candidate and State\n",
    "            list_of_relevant_sentences.append({\n",
    "                \"Sentence\": sentence,\n",
    "                \"Presidential_Candidate\": presidential_candidate,\n",
    "                \"State\": state,\n",
    "                \"Topic_Keywords\": topic_ngramed_keywords\n",
    "            })\n",
    "    \n",
    "    # Save List of All Relevant Sentences into CSV file\n",
    "    df = pd.DataFrame(list_of_relevant_sentences)\n",
    "    df.to_csv(relevant_transcript_sentences_filename, index=False, errors=\"ignore\")\n",
    "    return df, topic_model\n",
    "\n",
    "list_of_relevant_sentences, bertopic_model = filter_relevant_sentences()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:38: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:38: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "C:\\Users\\MSI Laptop\\AppData\\Local\\Temp\\ipykernel_14368\\1470298538.py:38: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  nr_topics=\"auto\" if max_topic_count is \"auto\" else max(len(presidential_candidates_combinations), max_topic_count),\n",
      "2024-11-08 21:53:13,246 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1775 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4cd8f2d07528499785fef015a9fa86eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 21:55:46,234 - BERTopic - Embedding - Completed ✓\n",
      "2024-11-08 21:55:46,234 - BERTopic - Guided - Find embeddings highly related to seeded topics.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c4855a2add14490a2fa739b4e22137e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 21:55:46,573 - BERTopic - Guided - Completed ✓\n",
      "2024-11-08 21:55:46,573 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-11-08 21:56:55,052 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-11-08 21:56:55,052 - BERTopic - Zeroshot Step 1 - Finding documents that could be assigned to either one of the zero-shot topics\n",
      "2024-11-08 21:56:55,883 - BERTopic - Zeroshot Step 1 - Completed ✓\n",
      "2024-11-08 21:57:26,599 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-11-08 21:57:39,082 - BERTopic - Cluster - Completed ✓\n",
      "2024-11-08 21:57:39,082 - BERTopic - Zeroshot Step 2 - Combining topics from zero-shot topic modeling with topics from clustering...\n",
      "2024-11-08 21:57:39,223 - BERTopic - Zeroshot Step 2 - Completed ✓\n",
      "2024-11-08 21:57:39,238 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-11-08 21:58:06,482 - BERTopic - Representation - Completed ✓\n",
      "2024-11-08 21:58:06,482 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-11-08 21:58:33,384 - BERTopic - Topic reduction - Reduced number of topics from 591 to 120\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-11-08T13:59:25.792968Z",
     "start_time": "2024-11-08T13:59:25.773304Z"
    }
   },
   "source": "bertopic_model.get_topic_info().sort_values(by=\"Count\", ascending=False)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     Topic  Count                                               Name  \\\n",
       "0       -1  23924                 -1_trump_people_state_pennsylvania   \n",
       "1        0   9140                    0_harris_arizona_election_trump   \n",
       "2        1   4286       1_kamala_harris_kamala harris_vice president   \n",
       "3        2   2697                        2_biden_joe biden_joe_trump   \n",
       "4        3    935                       3_question_word_words_answer   \n",
       "..     ...    ...                                                ...   \n",
       "115    114     11            114_zelensky_shells_zz top_top lagrange   \n",
       "116    115     11               115_ivf_father ivf_ivf father_father   \n",
       "117    116     11  116_majority_majority one_minority majority_si...   \n",
       "118    117     11       117_years years_years_life years_today years   \n",
       "119    118     10             118_way guys_guys video_guys_video way   \n",
       "\n",
       "                                        Representation  \\\n",
       "0    [trump, people, state, pennsylvania, election,...   \n",
       "1    [harris, arizona, election, trump, poll, state...   \n",
       "2    [kamala, harris, kamala harris, vice president...   \n",
       "3    [biden, joe biden, joe, trump, president, pres...   \n",
       "4    [question, word, words, answer, okay, question...   \n",
       "..                                                 ...   \n",
       "115  [zelensky, shells, zz top, top lagrange, tower...   \n",
       "116  [ivf, father ivf, ivf father, father, treatmen...   \n",
       "117  [majority, majority one, minority majority, si...   \n",
       "118  [years years, years, life years, today years, ...   \n",
       "119  [way guys, guys video, guys, video way, video,...   \n",
       "\n",
       "                                   Representative_Docs  \n",
       "0    [I don't think many people appreciate that, an...  \n",
       "1    [That's not what Harris needed., So again, tha...  \n",
       "2    [Kamala Harris won, as far as I'm concerned., ...  \n",
       "3    [Why are you voting for Joe Biden?, You were w...  \n",
       "4    [And frankly, that's a good question., One las...  \n",
       "..                                                 ...  \n",
       "115  [He can talk to Zelensky., Zelensky hates him....  \n",
       "116  [But he called himself the father of IVF., He ...  \n",
       "117  [The flip side is that there is a silent major...  \n",
       "118  [What was life like four years ago?, Guardian,...  \n",
       "119  [But either way, guys, that is gonna do it for...  \n",
       "\n",
       "[120 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>23924</td>\n",
       "      <td>-1_trump_people_state_pennsylvania</td>\n",
       "      <td>[trump, people, state, pennsylvania, election,...</td>\n",
       "      <td>[I don't think many people appreciate that, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>9140</td>\n",
       "      <td>0_harris_arizona_election_trump</td>\n",
       "      <td>[harris, arizona, election, trump, poll, state...</td>\n",
       "      <td>[That's not what Harris needed., So again, tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4286</td>\n",
       "      <td>1_kamala_harris_kamala harris_vice president</td>\n",
       "      <td>[kamala, harris, kamala harris, vice president...</td>\n",
       "      <td>[Kamala Harris won, as far as I'm concerned., ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2697</td>\n",
       "      <td>2_biden_joe biden_joe_trump</td>\n",
       "      <td>[biden, joe biden, joe, trump, president, pres...</td>\n",
       "      <td>[Why are you voting for Joe Biden?, You were w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>935</td>\n",
       "      <td>3_question_word_words_answer</td>\n",
       "      <td>[question, word, words, answer, okay, question...</td>\n",
       "      <td>[And frankly, that's a good question., One las...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>114</td>\n",
       "      <td>11</td>\n",
       "      <td>114_zelensky_shells_zz top_top lagrange</td>\n",
       "      <td>[zelensky, shells, zz top, top lagrange, tower...</td>\n",
       "      <td>[He can talk to Zelensky., Zelensky hates him....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>115</td>\n",
       "      <td>11</td>\n",
       "      <td>115_ivf_father ivf_ivf father_father</td>\n",
       "      <td>[ivf, father ivf, ivf father, father, treatmen...</td>\n",
       "      <td>[But he called himself the father of IVF., He ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>116</td>\n",
       "      <td>11</td>\n",
       "      <td>116_majority_majority one_minority majority_si...</td>\n",
       "      <td>[majority, majority one, minority majority, si...</td>\n",
       "      <td>[The flip side is that there is a silent major...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>117</td>\n",
       "      <td>11</td>\n",
       "      <td>117_years years_years_life years_today years</td>\n",
       "      <td>[years years, years, life years, today years, ...</td>\n",
       "      <td>[What was life like four years ago?, Guardian,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>118</td>\n",
       "      <td>10</td>\n",
       "      <td>118_way guys_guys video_guys_video way</td>\n",
       "      <td>[way guys, guys video, guys, video way, video,...</td>\n",
       "      <td>[But either way, guys, that is gonna do it for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-11-08T13:59:25.850895Z",
     "start_time": "2024-11-08T13:59:25.841203Z"
    }
   },
   "source": "list_of_relevant_sentences",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               Sentence  \\\n",
       "0     Because Kamala Harris just got a debate prep i...   \n",
       "1     So Saturday, Kamala Harris is going to be camp...   \n",
       "2     The only person who could control the weather ...   \n",
       "3     Why does Charlemagne talk crap about Harris wi...   \n",
       "4     I guess it's possible that maybe she wins ever...   \n",
       "...                                                 ...   \n",
       "9065  Don't believe a word Donald Trump says about a...   \n",
       "9066  Matt, I want to bring up some recent comments ...   \n",
       "9067               He called himself the father of IVF.   \n",
       "9068                      Oh, I want to talk about IVF.   \n",
       "9069                             I'm the father of IVF.   \n",
       "\n",
       "     Presidential_Candidate         State  \\\n",
       "0             Kamala Harris  Pennsylvania   \n",
       "1             Kamala Harris      Michigan   \n",
       "2             Kamala Harris  Pennsylvania   \n",
       "3             Kamala Harris  Pennsylvania   \n",
       "4             Kamala Harris  Pennsylvania   \n",
       "...                     ...           ...   \n",
       "9065           Donald Trump  Pennsylvania   \n",
       "9066           Donald Trump  Pennsylvania   \n",
       "9067           Donald Trump       Arizona   \n",
       "9068           Donald Trump  Pennsylvania   \n",
       "9069           Donald Trump  Pennsylvania   \n",
       "\n",
       "                                         Topic_Keywords  \n",
       "0     [kamala, harris, kamala harris, vice president...  \n",
       "1     [kamala, harris, kamala harris, vice president...  \n",
       "2     [kamala, harris, kamala harris, vice president...  \n",
       "3     [kamala, harris, kamala harris, vice president...  \n",
       "4     [kamala, harris, kamala harris, vice president...  \n",
       "...                                                 ...  \n",
       "9065  [ivf, father ivf, ivf father, father, treatmen...  \n",
       "9066  [ivf, father ivf, ivf father, father, treatmen...  \n",
       "9067  [ivf, father ivf, ivf father, father, treatmen...  \n",
       "9068  [ivf, father ivf, ivf father, father, treatmen...  \n",
       "9069  [ivf, father ivf, ivf father, father, treatmen...  \n",
       "\n",
       "[9070 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Presidential_Candidate</th>\n",
       "      <th>State</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Because Kamala Harris just got a debate prep i...</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[kamala, harris, kamala harris, vice president...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So Saturday, Kamala Harris is going to be camp...</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>[kamala, harris, kamala harris, vice president...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The only person who could control the weather ...</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[kamala, harris, kamala harris, vice president...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why does Charlemagne talk crap about Harris wi...</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[kamala, harris, kamala harris, vice president...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I guess it's possible that maybe she wins ever...</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[kamala, harris, kamala harris, vice president...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9065</th>\n",
       "      <td>Don't believe a word Donald Trump says about a...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[ivf, father ivf, ivf father, father, treatmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9066</th>\n",
       "      <td>Matt, I want to bring up some recent comments ...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[ivf, father ivf, ivf father, father, treatmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9067</th>\n",
       "      <td>He called himself the father of IVF.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>[ivf, father ivf, ivf father, father, treatmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9068</th>\n",
       "      <td>Oh, I want to talk about IVF.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[ivf, father ivf, ivf father, father, treatmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9069</th>\n",
       "      <td>I'm the father of IVF.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[ivf, father ivf, ivf father, father, treatmen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9070 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-11-08T13:59:25.904083Z",
     "start_time": "2024-11-08T13:59:25.861415Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Sa tingin ko need natin 5k sentences minimum for Relevant Sentences di lang for gathered.\n",
    "Kasi mamaya 5k Random Sentences nakuha natin tas 100 lang dun Relevant with candidate & state.\n",
    "\n",
    "Ang naiisip ko since meron 6 Combinations = 3 candidate * 2 state\n",
    "Gawin natin 5000/6 = 834 Relevant Sentences required set natin as minimum per Combination\n",
    "\n",
    "Trump  - Arizona      = 834 Relevant Sentences\n",
    "Harris - Arizona      = 834 Relevant Sentences\n",
    "Trump  - Michigan     = 834 Relevant Sentences\n",
    "Harris - Michigan     = 834 Relevant Sentences\n",
    "Trump  - Pennsylvania = 834 Relevant Sentences\n",
    "Harris - Pennsylvania = 834 Relevant Sentences\n",
    "               -------------------------------\n",
    "               Total: ~5000 Relevant Sentences\n",
    "\"\"\"\n",
    "def print_statistics():\n",
    "    try:\n",
    "        grouped_df = (\n",
    "            list_of_relevant_sentences\n",
    "            .groupby([\"Presidential_Candidate\", \"State\"])\n",
    "            .size()\n",
    "            .reset_index(name=\"count\")\n",
    "        )\n",
    "        total_count = grouped_df[\"count\"].sum()\n",
    "        total_row = pd.DataFrame({\"Presidential_Candidate\": [\"\"], \"State\": [\"Total\"], \"count\": [total_count]})\n",
    "        grouped_df = pd.concat([grouped_df, total_row], ignore_index=True)\n",
    "        return grouped_df.style.hide(axis=\"index\")\n",
    "    except: return \"No Relevant Sentences\"\n",
    "print_statistics()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x24303265b10>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_e6dbd\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_e6dbd_level0_col0\" class=\"col_heading level0 col0\" >Presidential_Candidate</th>\n",
       "      <th id=\"T_e6dbd_level0_col1\" class=\"col_heading level0 col1\" >State</th>\n",
       "      <th id=\"T_e6dbd_level0_col2\" class=\"col_heading level0 col2\" >count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_e6dbd_row0_col0\" class=\"data row0 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_e6dbd_row0_col1\" class=\"data row0 col1\" >Arizona</td>\n",
       "      <td id=\"T_e6dbd_row0_col2\" class=\"data row0 col2\" >1119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e6dbd_row1_col0\" class=\"data row1 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_e6dbd_row1_col1\" class=\"data row1 col1\" >Michigan</td>\n",
       "      <td id=\"T_e6dbd_row1_col2\" class=\"data row1 col2\" >1598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e6dbd_row2_col0\" class=\"data row2 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_e6dbd_row2_col1\" class=\"data row2 col1\" >Pennsylvania</td>\n",
       "      <td id=\"T_e6dbd_row2_col2\" class=\"data row2 col2\" >2235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e6dbd_row3_col0\" class=\"data row3 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_e6dbd_row3_col1\" class=\"data row3 col1\" >Arizona</td>\n",
       "      <td id=\"T_e6dbd_row3_col2\" class=\"data row3 col2\" >847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e6dbd_row4_col0\" class=\"data row4 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_e6dbd_row4_col1\" class=\"data row4 col1\" >Michigan</td>\n",
       "      <td id=\"T_e6dbd_row4_col2\" class=\"data row4 col2\" >1306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e6dbd_row5_col0\" class=\"data row5 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_e6dbd_row5_col1\" class=\"data row5 col1\" >Pennsylvania</td>\n",
       "      <td id=\"T_e6dbd_row5_col2\" class=\"data row5 col2\" >1965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e6dbd_row6_col0\" class=\"data row6 col0\" ></td>\n",
       "      <td id=\"T_e6dbd_row6_col1\" class=\"data row6 col1\" >Total</td>\n",
       "      <td id=\"T_e6dbd_row6_col2\" class=\"data row6 col2\" >9070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:59:26.052594Z",
     "start_time": "2024-11-08T13:59:26.049852Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
