{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-11-14T16:35:42.926969Z",
     "start_time": "2024-11-14T16:35:29.308946Z"
    }
   },
   "source": [
    "# Import Dependencies\n",
    "import os, nltk, spacy, neuralcoref\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Additional Downloads\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "# Load Spacy Language Model with Sentencizer and NeuralCoref \n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.add_pipe(nlp.create_pipe(\"sentencizer\"))\n",
    "neuralcoref.add_to_pipe(nlp)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1f3b3dc2dd8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Define Utilities"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-11-14T16:35:43.041574Z",
     "start_time": "2024-11-14T16:35:42.979064Z"
    }
   },
   "source": [
    "def read_unique_items_from_file(file: str) -> list:\n",
    "    if os.path.exists(file):\n",
    "        with open(file, \"r\", errors=\"ignore\") as f:\n",
    "            return list(set(e.strip() for e in f.readlines() if e.strip()))\n",
    "    return []"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Set Configurations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-11-14T16:35:43.361030Z",
     "start_time": "2024-11-14T16:35:43.234228Z"
    }
   },
   "source": [
    "# File Names\n",
    "transcript_documents_filename = \"transcript_documents.csv\"\n",
    "relevant_transcript_sentences_filename = \"relevant_transcript_sentences.csv\"\n",
    "\n",
    "# Folder Names\n",
    "transcription_path = \"Transcription\"\n",
    "cities_transcription_paths = {\n",
    "    \"Michigan\": os.path.join(transcription_path, \"Michigan\"),\n",
    "    \"Arizona\": os.path.join(transcription_path, \"Arizona\"),\n",
    "    \"Pennsylvania\": os.path.join(transcription_path, \"Pennsylvania\"),\n",
    "}\n",
    "cities_path = \"State Cities\"\n",
    "\n",
    "# Numeric Constants \n",
    "max_pair_of_words_for_main_subject_mention = 3\n",
    "\"\"\"\n",
    "    > Maximum words to consider for main subject mentions\n",
    "        1: Unigram (e.g., \"Donald\")\n",
    "        2: Bigram (e.g., \"Donald Trump\")\n",
    "\"\"\"\n",
    "\n",
    "min_similarity_of_topic_modeling = 0.7\n",
    "\"\"\"\n",
    "    > Minimum similarity threshold for topic matching\n",
    "        Range: [0.1, 1.0]\n",
    "    Note: Higher values require closer matches\n",
    "    Example: 0.7 = 70% similarity required\n",
    "\"\"\"\n",
    "\n",
    "# Sentence Categories\n",
    "presidential_candidates = {\n",
    "    \"Donald Trump\": [\n",
    "        \"Donald\", \"Trump\",\n",
    "        \"Trump Donald\", \"Donald John\", \"John Trump\",\n",
    "        \"Donald J\", \"J. Donald\", \"J. Trump\", \"Trump J\",\n",
    "        \"Trump D\", \"D. Trump\", \"John D\", \"D. John\",\n",
    "        \"Donald T\", \"T. Donald\", \"John T\", \"T. John\",\n",
    "        \"Donald John Trump\", \"Donald J Trump\", \"D. J. Trump\", \n",
    "        \"President Donald\", \"President Trump\",\n",
    "        \"President Donald Trump\"\n",
    "    ],\n",
    "    \"Kamala Harris\": [\n",
    "        \"Kamala\", \"Harris\",\n",
    "        \"Harris Kamala\", \"Kamala Devi\", \"Devi Harris\",\n",
    "        \"Kamala D\", \"D. Kamala\", \"D. Harris\", \"Harris D\",\n",
    "        \"Harris K\", \"K. Harris\", \"Devi K\", \"K. Devi\",\n",
    "        \"Kamala H\", \"H. Kamala\", \"Devi H\", \"H. Devi\",\n",
    "        \"Kamala Devi Harris\", \"Kamala D Harris\", \"K. D. Harris\",  \n",
    "        \"President Kamala\", \"President Harris\",\n",
    "        \"President Kamala Harris\"\n",
    "    ]\n",
    "}\n",
    "original_state_cities = [\"Arizona\", \"Michigan\", \"Pennsylvania\"]\n",
    "state_cities = {\n",
    "    \"Arizona\": read_unique_items_from_file(os.path.join(cities_path, \"arizona-cities.txt\")),\n",
    "    \"Michigan\": read_unique_items_from_file(os.path.join(cities_path, \"michigan-cities.txt\")),\n",
    "    \"Pennsylvania\": read_unique_items_from_file(os.path.join(cities_path, \"pennsylvania-cities.txt\")),\n",
    "    \"Alabama\": [\"AL\", \"A.L\"],\n",
    "    \"Alaska\": [\"AK\", \"A.K\"],\n",
    "    \"Arkansas\": [\"AR\", \"A.R\"],\n",
    "    \"California\": [\"CA\", \"C.A\"],\n",
    "    \"Colorado\": [\"CO\", \"C.O\"],\n",
    "    \"Connecticut\": [\"CT\", \"C.T\"],\n",
    "    \"Delaware\": [\"DE\", \"D.E\"],\n",
    "    \"Florida\": [\"FL\", \"F.L\"],\n",
    "    \"Georgia\": [\"GA\", \"G.A\"],\n",
    "    \"Hawaii\": [\"HI\", \"H.I\"],\n",
    "    \"Idaho\": [\"ID\", \"I.D\"],\n",
    "    \"Illinois\": [\"IL\", \"I.L\"],\n",
    "    \"Indiana\": [\"IN\", \"I.N\"],\n",
    "    \"Iowa\": [\"IA\", \"I.A\"],\n",
    "    \"Kansas\": [\"KS\", \"K.S\"],\n",
    "    \"Kentucky\": [\"KY\", \"K.Y\"],\n",
    "    \"Louisiana\": [\"LA\", \"L.A\"],\n",
    "    \"Maine\": [\"ME\", \"M.E\"],\n",
    "    \"Maryland\": [\"MD\", \"M.D\"],\n",
    "    \"Massachusetts\": [\"MA\", \"M.A\"],\n",
    "    \"Minnesota\": [\"MN\", \"M.N\"],\n",
    "    \"Mississippi\": [\"MS\", \"M.S\"],\n",
    "    \"Missouri\": [\"MO\", \"M.O\"],\n",
    "    \"Montana\": [\"MT\", \"M.T\"],\n",
    "    \"Nebraska\": [\"NE\", \"N.E\"],\n",
    "    \"Nevada\": [\"NV\", \"N.V\"],\n",
    "    \"New Hampshire\": [\"NH\", \"N.H\"],\n",
    "    \"New Jersey\": [\"NJ\", \"N.J\"],\n",
    "    \"New Mexico\": [\"NM\", \"N.M\"],\n",
    "    \"New York\": [\"NY\", \"N.Y\"],\n",
    "    \"North Carolina\": [\"NC\", \"N.C\"],\n",
    "    \"North Dakota\": [\"ND\", \"N.D\"],\n",
    "    \"Ohio\": [\"OH\", \"O.H\"],\n",
    "    \"Oklahoma\": [\"OK\", \"O.K\"],\n",
    "    \"Oregon\": [\"OR\", \"O.R\"],\n",
    "    \"Rhode Island\": [\"RI\", \"R.I\"],\n",
    "    \"South Carolina\": [\"SC\", \"S.C\"],\n",
    "    \"South Dakota\": [\"SD\", \"S.D\"],\n",
    "    \"Tennessee\": [\"TN\", \"T.N\"],\n",
    "    \"Texas\": [\"TX\", \"T.X\"],\n",
    "    \"Utah\": [\"UT\", \"U.T\"],\n",
    "    \"Vermont\": [\"VT\", \"V.T\"],\n",
    "    \"Virginia\": [\"VA\", \"V.A\"],\n",
    "    \"Washington\": [\"WA\", \"W.A\"],\n",
    "    \"West Virginia\": [\"WV\", \"W.V\"],\n",
    "    \"Wisconsin\": [\"WI\", \"W.I\"],\n",
    "    \"Wyoming\": [\"WY\", \"W.Y\"],\n",
    "}\n",
    "\n",
    "# Words for Sentence Filtering\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Additional Preprocessing of Configurations\n",
    "presidential_candidates = {presidential_candidate: list(set(names)) for presidential_candidate, names in presidential_candidates.items()}"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Sentence Extraction (Transcripts to CSV)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-11-14T16:36:54.508713Z",
     "start_time": "2024-11-14T16:35:43.392289Z"
    }
   },
   "source": [
    "def preprocess_transcripts_into_csv_of_documents() -> pd.DataFrame:\n",
    "    # Initialize list of sentences and possible states\n",
    "    list_of_documents = []\n",
    "    \n",
    "    # Collect documents from each state's transcription files\n",
    "    for state, path in cities_transcription_paths.items():\n",
    "        transcription_files = os.listdir(path)\n",
    "        total_transcription_files = len(transcription_files)\n",
    "\n",
    "        with tqdm(total=total_transcription_files, desc=f'Preprocessing Documents for {state} [0/{total_transcription_files} Transcript]') as pbar:\n",
    "            for index, filename in enumerate(transcription_files):\n",
    "                current = f'{index + 1}/{total_transcription_files}'\n",
    "                if filename == \".ipynb_checkpoints\":\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                pbar.set_description(f'Preprocessing Documents for {state} [{current} Transcript]')\n",
    "\n",
    "                # Open transcription file\n",
    "                file_path = os.path.join(path, filename)\n",
    "                with open(file_path, \"r\", errors=\"ignore\") as file:\n",
    "                    transcription = file.read()\n",
    "\n",
    "                    # Split transcript into tokenized sentences\n",
    "                    tokenized_sentences = sent_tokenize(transcription)\n",
    "\n",
    "                    # Merge related sentence tokens to complete sentence\n",
    "                    sentences = []\n",
    "                    previous_sentence_token = \"\"\n",
    "                    \n",
    "                    for i, tokenized_sentence in enumerate(tokenized_sentences):\n",
    "                        # Strip Unnecessary White Spaces\n",
    "                        tokenized_sentence = tokenized_sentence.strip()\n",
    "                        \n",
    "                        # Remove if Tokenized Sentence without Punctuation is Empty\n",
    "                        if tokenized_sentence[:-1].strip() == \"\": continue\n",
    "                        \n",
    "                        # Remove consecutive duplicates (Whisper Hallucination)\n",
    "                        if i != 0 and tokenized_sentence == tokenized_sentences[i - 1]: continue\n",
    "                        \n",
    "                        \"\"\"\n",
    "                            Add Current Sentence Token to Previous If Either:\n",
    "                                1) Merged Sentences Is Still Incomplete \n",
    "                                2) Previous Sentence is a question\n",
    "                                3) Previous Sentence ends with ellipsis\n",
    "                        \"\"\"\n",
    "                        def is_sentence_not_complete(sentence_token: str):\n",
    "                            pos_tags = pos_tag(word_tokenize(sentence_token))\n",
    "                        \n",
    "                            subject_count = 0\n",
    "                            predicate_count = 0\n",
    "                        \n",
    "                            for word, tag in pos_tags:\n",
    "                                if tag in {\"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PRP\"}: # At least 2 Subject\n",
    "                                    subject_count += 1\n",
    "                                elif tag.startswith(\"V\"):  # At least 1 Verb\n",
    "                                    predicate_count += 1\n",
    "                                # Early Check and Return\n",
    "                                if subject_count >= 2 and predicate_count >= 1:\n",
    "                                    return False\n",
    "                        \n",
    "                            return True\n",
    "                        \n",
    "                        current_merged_sentence_tokens = sentences[-1] if i > 0 else None\n",
    "                        if (\n",
    "                            current_merged_sentence_tokens is not None\n",
    "                            and (\n",
    "                                # Incomplete Merged Sentence Tokens\n",
    "                                is_sentence_not_complete(current_merged_sentence_tokens) \n",
    "                                # Previous Sentence Token is a Question\n",
    "                                or previous_sentence_token.endswith(\"?\")\n",
    "                                # Previous Sentence Token Ends with Ellipsis\n",
    "                                or previous_sentence_token.endswith(\"...\")\n",
    "                                or previous_sentence_token.endswith(\"..\") \n",
    "                            )\n",
    "                        ):\n",
    "                            # If conditions are met, connect with the previous sentence\n",
    "                            if previous_sentence_token.endswith(\"...\"):\n",
    "                                sentences[-1] = f'{current_merged_sentence_tokens[:-3]}, {tokenized_sentence[:1].lower()}{tokenized_sentence[1:]}'\n",
    "                            elif previous_sentence_token.endswith(\"..\"):\n",
    "                                sentences[-1] = f'{current_merged_sentence_tokens[:-2]}, {tokenized_sentence[:1].lower()}{tokenized_sentence[1:]}'\n",
    "                            elif previous_sentence_token.endswith(\".\"):\n",
    "                                sentences[-1] = f'{current_merged_sentence_tokens[:-1]}, {tokenized_sentence[:1].lower()}{tokenized_sentence[1:]}'\n",
    "                            elif previous_sentence_token.endswith(\"?\"):\n",
    "                                if tokenized_sentence.endswith(\"?\"):\n",
    "                                    sentences[-1] = f'{current_merged_sentence_tokens[:-1]}, {tokenized_sentence[:1].lower()}{tokenized_sentence[1:]}'\n",
    "                                else:\n",
    "                                 sentences[-1] = f'{current_merged_sentence_tokens[:-1]}: {tokenized_sentence[:1].lower()}{tokenized_sentence[1:]}'\n",
    "                            else:\n",
    "                                sentences[-1] = f'{current_merged_sentence_tokens} {tokenized_sentence}'\n",
    "                        else:\n",
    "                            # Otherwise, treat as a new sentence\n",
    "                            sentences.append(tokenized_sentence)\n",
    "                \n",
    "                        # Update the previous sentence\n",
    "                        previous_sentence_token = tokenized_sentence\n",
    "                        \n",
    "                    # Append each transcription documents with the possible-state\n",
    "                    list_of_documents.append((\" \".join(sentences).strip(), state))\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Convert the list of sentences and states into a DataFrame\n",
    "    df = pd.DataFrame(list_of_documents, columns=[\"Document\", \"Possible_State\"])\n",
    "    df.to_csv(transcript_documents_filename, index=False, errors=\"ignore\")\n",
    "    return df\n",
    "\n",
    "list_of_documents = preprocess_transcripts_into_csv_of_documents()\n",
    "print(f'Number of Documents: {len(list_of_documents)}')\n",
    "list_of_documents"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Preprocessing Documents for Michigan [0/260 Transcript]:   0%|          | 0/260 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e0572c9b61964199a26a1215f5c16fb7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Preprocessing Documents for Arizona [0/168 Transcript]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cfff3e089633402fa4b3951ef4f1d146"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Preprocessing Documents for Pennsylvania [0/268 Transcript]:   0%|          | 0/268 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0dd53385c6994eb1b82a0221074ad7d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents: 696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                              Document Possible_State\n",
       "0    This morning we are continuing our post debate...       Michigan\n",
       "1    Kamala Harris leads in key battleground states...       Michigan\n",
       "2    The Biden administration said on Tuesday it wo...       Michigan\n",
       "3    Election night on Sky News is going to be very...       Michigan\n",
       "4    Hey guys, welcome back to today's video. Today...       Michigan\n",
       "..                                                 ...            ...\n",
       "691  We're here live in Butler, PA at another merch...   Pennsylvania\n",
       "692  Pennsylvania's latest poll results are just un...   Pennsylvania\n",
       "693  When you watch this clip, I want you to ask yo...   Pennsylvania\n",
       "694  And so we have this Quinnipiac poll and this w...   Pennsylvania\n",
       "695  All you hear is that the Democrats are the one...   Pennsylvania\n",
       "\n",
       "[696 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Possible_State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This morning we are continuing our post debate...</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kamala Harris leads in key battleground states...</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Biden administration said on Tuesday it wo...</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Election night on Sky News is going to be very...</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hey guys, welcome back to today's video. Today...</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>We're here live in Butler, PA at another merch...</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>Pennsylvania's latest poll results are just un...</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>When you watch this clip, I want you to ask yo...</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>And so we have this Quinnipiac poll and this w...</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>All you hear is that the Democrats are the one...</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>696 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# BERTopic: Relevant Sentence Filtering (CSV)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T17:03:57.006602Z",
     "start_time": "2024-11-14T16:36:54.907066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def filter_relevant_sentences() -> pd.DataFrame:\n",
    "    # Get All Collected Sentences from Transcript and a Map with their Respective Possible State\n",
    "    df = pd.read_csv(transcript_documents_filename)\n",
    "    documents = pd.Series(df['Possible_State'].values, index=df['Document']).to_dict()\n",
    "\n",
    "    # Initialize Lists for Relevant Sentences\n",
    "    list_of_relevant_sentences = []\n",
    "    seen_relevant_sentences = set()\n",
    "\n",
    "    document_items = documents.items()\n",
    "    total_document_items = len(document_items)\n",
    "    processed_document = 0\n",
    "    with tqdm(total=total_document_items, desc=f'Extracting Relevant Sentences [0/{total_document_items} Documents]') as pbar:\n",
    "        for document, possible_state in document_items:\n",
    "            # Get Document Object from Language Model\n",
    "            document_obj = nlp(document)\n",
    "\n",
    "            # Get Relevant Sentences for Candidates\n",
    "            for sentence_obj in document_obj.sents:\n",
    "                presidential_candidate_mentions = set() # Avoid Duplicates\n",
    "                no_other_state_mentioned_different_from_possible_state = True\n",
    "                possible_relevant_sentence = None\n",
    "\n",
    "                # Define Methods for Relevant Sentences\n",
    "                def get_mentions(sentence_object: str, max_ngrams: int = max_pair_of_words_for_main_subject_mention) -> dict:\n",
    "                    mentions = {}\n",
    "                    for n in range(1, max_ngrams + 1): # Try n-gram mentions sizes from 1 to max_n\n",
    "                        for idx in range(n, len(sentence_object) + 1): # Form n-gram length mentions from sentences\n",
    "                            span = sentence_object[idx - n:idx] # n-gram span\n",
    "                            if span._.is_coref: # Add only if span has coreference info\n",
    "                                main_mention = span._.coref_cluster.main.text # Get string from span object\n",
    "                                mentions[idx-n] = main_mention # Use the index of the first word in mentioned n-gram\n",
    "                                \n",
    "                    return mentions\n",
    "\n",
    "                def add_and_get_presidential_candidate_mentions(ngramed_mention: str) -> set:\n",
    "                    for presidential_candidate, names in presidential_candidates.items():\n",
    "                        if (\n",
    "                            any(\n",
    "                                (\n",
    "                                    presidential_candidate and ngramed_mention\n",
    "                                    and f' {presidential_candidate.strip().lower()} ' in f' {ngramed_mention.strip().lower()} '\n",
    "                                ) or (\n",
    "                                    presidential_candidate and word\n",
    "                                    and presidential_candidate.strip().lower() == word.strip().lower()\n",
    "                                )\n",
    "                                for word in ngramed_mention.split(\" \")\n",
    "                            )\n",
    "                            # Any Other Candidate Names is Mentioned in Topic\n",
    "                            or any(\n",
    "                                (\n",
    "                                    name and ngramed_mention\n",
    "                                    and f' {name.strip().lower()} ' in f' {ngramed_mention.strip().lower()} '\n",
    "                                ) or (\n",
    "                                    name and word\n",
    "                                    and name.strip().lower() == word.strip().lower()\n",
    "                                )\n",
    "                                for name in names\n",
    "                                for word in ngramed_mention.split(\" \")\n",
    "                            )\n",
    "                        ):\n",
    "                            presidential_candidate_mentions.add(presidential_candidate)\n",
    "\n",
    "                    return presidential_candidate_mentions\n",
    "\n",
    "                def check_mentioned_state_different_from_possible_state(ngramed_mention: str) -> bool:\n",
    "                    if possible_state not in state_cities: raise ValueError(f'This Sentence has Invalid Possible State ({possible_state}): \"{sentence}\"')\n",
    "                    # Filter Sentence with Topic of [Other State] Not in [Arizona, Michigan, Pennsylvania]\n",
    "                    if possible_state not in original_state_cities: return False\n",
    "                    # Filter Sentence with Topic of [Other State] Different from its [Possible State]\n",
    "                    other_states = [state for state in state_cities if state is not possible_state]\n",
    "                    if any(\n",
    "                        f' {other_state.strip().lower()} ' in f' {ngramed_mention.strip().lower()} '\n",
    "                        or (\n",
    "                            word\n",
    "                            and other_state.strip().lower() == word.strip().lower()\n",
    "                        )\n",
    "                        for other_state in other_states\n",
    "                        for word in ngramed_mention.split(\" \")\n",
    "                    ): return False\n",
    "                    # Filter Sentence with Topics of [Other States' Cities] Different from its [Possible State Cities]\n",
    "                    other_state_cities = [\n",
    "                        other_city\n",
    "                        for other_cities in {\n",
    "                            state: state_cities[state]\n",
    "                            for state in state_cities\n",
    "                            if state is not possible_state\n",
    "                        }.values()\n",
    "                        for other_city in other_cities\n",
    "                        if other_city\n",
    "                    ]\n",
    "                    if any(\n",
    "                        f' {other_city.strip().lower()} ' in f' {ngramed_mention.strip().lower()} '\n",
    "                        or (\n",
    "                            word\n",
    "                            and other_city.strip().lower() == word.strip().lower()\n",
    "                        )\n",
    "                        for other_city in other_state_cities\n",
    "                        for word in ngramed_mention.split(\" \")\n",
    "                    ): return False\n",
    "                    return True\n",
    "\n",
    "                mentions = get_mentions(sentence_obj)\n",
    "                for mention_idx, ngramed_mention in mentions.items():\n",
    "                    # Ensure No Other State are Mentioned Different from Possible State\n",
    "                    if no_other_state_mentioned_different_from_possible_state:\n",
    "                        no_other_state_mentioned_different_from_possible_state = check_mentioned_state_different_from_possible_state(ngramed_mention)\n",
    "                    else: break\n",
    "\n",
    "                    # Add Mentioned Candidate and Ensure Only 1 Candidate is Mentioned\n",
    "                    if len(add_and_get_presidential_candidate_mentions(ngramed_mention)) > 1: break\n",
    "\n",
    "                    sentence = sentence_obj.text.strip()\n",
    "                    word = nlp(sentence)\n",
    "                    for idx, token in enumerate(word):\n",
    "                        if mention_idx == idx and token.dep_ in {\n",
    "                            \"nsubj\",\n",
    "                            \"nsubjpass\",\n",
    "                            \"compound\",\n",
    "                            \"dobj\",\n",
    "                            \"poss\"\n",
    "                        }:\n",
    "                            possible_relevant_sentence = sentence\n",
    "                            break\n",
    "\n",
    "                    if possible_relevant_sentence is not None: break\n",
    "\n",
    "                if (\n",
    "                    # Relevant Sentence was Found and Unique \n",
    "                    possible_relevant_sentence is not None\n",
    "                    and possible_relevant_sentence not in seen_relevant_sentences\n",
    "                    # Re-ensure No Other State is Mentioned Aside from Possible State\n",
    "                    and no_other_state_mentioned_different_from_possible_state\n",
    "                    # Re-ensure Only 1 Candidate is Mentioned\n",
    "                    and len(presidential_candidate_mentions) == 1\n",
    "                ):\n",
    "                    # Additional Cleaning\n",
    "                    if possible_relevant_sentence.startswith(\", \"):\n",
    "                        possible_relevant_sentence = possible_relevant_sentence[2:]\n",
    "                    # Get Presidential Candidate Mentioned\n",
    "                    presidential_candidate = presidential_candidate_mentions.pop()\n",
    "                    # Add Relevant Sentence\n",
    "                    list_of_relevant_sentences.append({\n",
    "                        \"Sentence\": possible_relevant_sentence,\n",
    "                        \"Presidential_Candidate\": presidential_candidate,\n",
    "                        \"State\": possible_state\n",
    "                    })\n",
    "                    seen_relevant_sentences.add(possible_relevant_sentence)\n",
    "\n",
    "            processed_document += 1\n",
    "            pbar.set_description(f'Extracting Relevant Sentences [{processed_document}/{total_document_items} Documents]')\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Save List of All Relevant Sentences into CSV file\n",
    "    df = pd.DataFrame(list_of_relevant_sentences)\n",
    "    df.to_csv(relevant_transcript_sentences_filename, index=False, errors=\"ignore\")\n",
    "    return df\n",
    "\n",
    "list_of_relevant_sentences = filter_relevant_sentences()\n",
    "list_of_relevant_sentences"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Extracting Relevant Sentences [0/694 Documents]:   0%|          | 0/694 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "280e8833475844209f7a6fc5e5ba61eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "                                               Sentence  \\\n",
       "0     Are you a previous Trump supporter: the first ...   \n",
       "1     Okay, then what happened: and then I just felt...   \n",
       "2         I like the fact that he was not a politician.   \n",
       "3     Oh absolutely, small business owner Andrew Cin...   \n",
       "4     Before Harris stepped in I thought it was goin...   \n",
       "...                                                 ...   \n",
       "8186  So 48, 14, 18, 22, 26 minus three is 23, 12345...   \n",
       "8187  Harris 48, Trump 47, so that one essentially a...   \n",
       "8188                 It just kept going up after Trump.   \n",
       "8189  Ace, I really hope the nonsense of trying to h...   \n",
       "8190  We need to simmer down now when it comes to so...   \n",
       "\n",
       "     Presidential_Candidate         State  \n",
       "0              Donald Trump      Michigan  \n",
       "1              Donald Trump      Michigan  \n",
       "2              Donald Trump      Michigan  \n",
       "3              Donald Trump      Michigan  \n",
       "4             Kamala Harris      Michigan  \n",
       "...                     ...           ...  \n",
       "8186          Kamala Harris  Pennsylvania  \n",
       "8187          Kamala Harris  Pennsylvania  \n",
       "8188          Kamala Harris  Pennsylvania  \n",
       "8189           Donald Trump  Pennsylvania  \n",
       "8190           Donald Trump  Pennsylvania  \n",
       "\n",
       "[8191 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Presidential_Candidate</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are you a previous Trump supporter: the first ...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Okay, then what happened: and then I just felt...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I like the fact that he was not a politician.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oh absolutely, small business owner Andrew Cin...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Before Harris stepped in I thought it was goin...</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8186</th>\n",
       "      <td>So 48, 14, 18, 22, 26 minus three is 23, 12345...</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8187</th>\n",
       "      <td>Harris 48, Trump 47, so that one essentially a...</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8188</th>\n",
       "      <td>It just kept going up after Trump.</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8189</th>\n",
       "      <td>Ace, I really hope the nonsense of trying to h...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8190</th>\n",
       "      <td>We need to simmer down now when it comes to so...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8191 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-11-14T17:03:57.384868Z",
     "start_time": "2024-11-14T17:03:57.337992Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Sa tingin ko need natin 5k sentences minimum for Relevant Sentences di lang for gathered.\n",
    "Kasi mamaya 5k Random Sentences nakuha natin tas 100 lang dun Relevant with candidate & state.\n",
    "\n",
    "Ang naiisip ko since meron 6 Combinations = 3 candidate * 2 state\n",
    "Gawin natin 5000/6 = 834 Relevant Sentences required set natin as minimum per Combination\n",
    "\n",
    "Trump  - Arizona      = 834 Relevant Sentences\n",
    "Harris - Arizona      = 834 Relevant Sentences\n",
    "Trump  - Michigan     = 834 Relevant Sentences\n",
    "Harris - Michigan     = 834 Relevant Sentences\n",
    "Trump  - Pennsylvania = 834 Relevant Sentences\n",
    "Harris - Pennsylvania = 834 Relevant Sentences\n",
    "               -------------------------------\n",
    "               Total: ~5000 Relevant Sentences\n",
    "\"\"\"\n",
    "def print_statistics():\n",
    "    try:\n",
    "        grouped_df = (\n",
    "            list_of_relevant_sentences\n",
    "            .groupby([\"Presidential_Candidate\", \"State\"])\n",
    "            .size()\n",
    "            .reset_index(name=\"count\")\n",
    "        )\n",
    "        total_count = grouped_df[\"count\"].sum()\n",
    "        total_row = pd.DataFrame({\"Presidential_Candidate\": [\"\"], \"State\": [\"Total\"], \"count\": [total_count]})\n",
    "        grouped_df = pd.concat([grouped_df, total_row], ignore_index=True)\n",
    "        return grouped_df\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        return \"No Relevant Sentences\"\n",
    "print_statistics()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Presidential_Candidate         State  count\n",
       "0           Donald Trump       Arizona   1092\n",
       "1           Donald Trump      Michigan   1644\n",
       "2           Donald Trump  Pennsylvania   2165\n",
       "3          Kamala Harris       Arizona    528\n",
       "4          Kamala Harris      Michigan   1117\n",
       "5          Kamala Harris  Pennsylvania   1645\n",
       "6                                Total   8191"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Presidential_Candidate</th>\n",
       "      <th>State</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>1092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>1644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>2165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>1117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>1645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>Total</td>\n",
       "      <td>8191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T17:03:58.124166Z",
     "start_time": "2024-11-14T17:03:58.108541Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
