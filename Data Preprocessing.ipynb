{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T07:28:26.108783Z",
     "start_time": "2024-12-07T07:28:13.220043Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x2d0b1d69f98>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "import os, nltk, spacy, neuralcoref\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Additional Downloads\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "# Load Spacy Language Model with Sentencizer and NeuralCoref \n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.add_pipe(nlp.create_pipe(\"sentencizer\"))\n",
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Define Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T07:28:26.289324Z",
     "start_time": "2024-12-07T07:28:26.213088Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_unique_items_from_file(file: str) -> list:\n",
    "    if os.path.exists(file):\n",
    "        with open(file, \"r\", errors=\"ignore\") as f:\n",
    "            return list(set(e.strip() for e in f.readlines() if e.strip()))\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Set Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T07:28:26.777214Z",
     "start_time": "2024-12-07T07:28:26.622899Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# File Names\n",
    "transcript_documents_filename = \"transcript_documents.csv\"\n",
    "relevant_transcript_sentences_filename = \"relevant_transcript_sentences.csv\"\n",
    "\n",
    "# Folder Names\n",
    "transcription_path = \"Transcription\"\n",
    "cities_transcription_paths = {\n",
    "    \"Michigan\": os.path.join(transcription_path, \"Michigan\"),\n",
    "    \"Arizona\": os.path.join(transcription_path, \"Arizona\"),\n",
    "    \"Pennsylvania\": os.path.join(transcription_path, \"Pennsylvania\"),\n",
    "}\n",
    "cities_path = \"State Cities\"\n",
    "\n",
    "# Numeric Constants \n",
    "max_pair_of_words_for_main_subject_mention = 3\n",
    "\"\"\"\n",
    "    > Maximum words to consider for main subject mentions\n",
    "        1: Unigram (e.g., \"Donald\")\n",
    "        2: Bigram (e.g., \"Donald Trump\")\n",
    "\"\"\"\n",
    "\n",
    "# Sentence Categories\n",
    "presidential_candidates = {\n",
    "    \"Donald Trump\": [\n",
    "        \"Donald\", \"Trump\",\n",
    "        \"Trump Donald\", \"Donald John\", \"John Trump\",\n",
    "        \"Donald J\", \"J. Donald\", \"J. Trump\", \"Trump J\",\n",
    "        \"Trump D\", \"D. Trump\", \"John D\", \"D. John\",\n",
    "        \"Donald T\", \"T. Donald\", \"John T\", \"T. John\",\n",
    "        \"Donald John Trump\", \"Donald J Trump\", \"D. J. Trump\", \n",
    "        \"President Donald\", \"President Trump\",\n",
    "        \"President Donald Trump\"\n",
    "    ],\n",
    "    \"Kamala Harris\": [\n",
    "        \"Kamala\", \"Harris\",\n",
    "        \"Harris Kamala\", \"Kamala Devi\", \"Devi Harris\",\n",
    "        \"Kamala D\", \"D. Kamala\", \"D. Harris\", \"Harris D\",\n",
    "        \"Harris K\", \"K. Harris\", \"Devi K\", \"K. Devi\",\n",
    "        \"Kamala H\", \"H. Kamala\", \"Devi H\", \"H. Devi\",\n",
    "        \"Kamala Devi Harris\", \"Kamala D Harris\", \"K. D. Harris\",  \n",
    "        \"President Kamala\", \"President Harris\",\n",
    "        \"President Kamala Harris\"\n",
    "    ]\n",
    "}\n",
    "original_state_cities = [\"Arizona\", \"Michigan\", \"Pennsylvania\"]\n",
    "state_cities = {\n",
    "    \"Arizona\": read_unique_items_from_file(os.path.join(cities_path, \"arizona-cities.txt\")),\n",
    "    \"Michigan\": read_unique_items_from_file(os.path.join(cities_path, \"michigan-cities.txt\")),\n",
    "    \"Pennsylvania\": read_unique_items_from_file(os.path.join(cities_path, \"pennsylvania-cities.txt\")),\n",
    "    \"Alabama\": [\"AL\", \"A.L\"],\n",
    "    \"Alaska\": [\"AK\", \"A.K\"],\n",
    "    \"Arkansas\": [\"AR\", \"A.R\"],\n",
    "    \"California\": [\"CA\", \"C.A\"],\n",
    "    \"Colorado\": [\"CO\", \"C.O\"],\n",
    "    \"Connecticut\": [\"CT\", \"C.T\"],\n",
    "    \"Delaware\": [\"DE\", \"D.E\"],\n",
    "    \"Florida\": [\"FL\", \"F.L\"],\n",
    "    \"Georgia\": [\"GA\", \"G.A\"],\n",
    "    \"Hawaii\": [\"HI\", \"H.I\"],\n",
    "    \"Idaho\": [\"ID\", \"I.D\"],\n",
    "    \"Illinois\": [\"IL\", \"I.L\"],\n",
    "    \"Indiana\": [\"IN\", \"I.N\"],\n",
    "    \"Iowa\": [\"IA\", \"I.A\"],\n",
    "    \"Kansas\": [\"KS\", \"K.S\"],\n",
    "    \"Kentucky\": [\"KY\", \"K.Y\"],\n",
    "    \"Louisiana\": [\"LA\", \"L.A\"],\n",
    "    \"Maine\": [\"ME\", \"M.E\"],\n",
    "    \"Maryland\": [\"MD\", \"M.D\"],\n",
    "    \"Massachusetts\": [\"MA\", \"M.A\"],\n",
    "    \"Minnesota\": [\"MN\", \"M.N\"],\n",
    "    \"Mississippi\": [\"MS\", \"M.S\"],\n",
    "    \"Missouri\": [\"MO\", \"M.O\"],\n",
    "    \"Montana\": [\"MT\", \"M.T\"],\n",
    "    \"Nebraska\": [\"NE\", \"N.E\"],\n",
    "    \"Nevada\": [\"NV\", \"N.V\"],\n",
    "    \"New Hampshire\": [\"NH\", \"N.H\"],\n",
    "    \"New Jersey\": [\"NJ\", \"N.J\"],\n",
    "    \"New Mexico\": [\"NM\", \"N.M\"],\n",
    "    \"New York\": [\"NY\", \"N.Y\"],\n",
    "    \"North Carolina\": [\"NC\", \"N.C\"],\n",
    "    \"North Dakota\": [\"ND\", \"N.D\"],\n",
    "    \"Ohio\": [\"OH\", \"O.H\"],\n",
    "    \"Oklahoma\": [\"OK\", \"O.K\"],\n",
    "    \"Oregon\": [\"OR\", \"O.R\"],\n",
    "    \"Rhode Island\": [\"RI\", \"R.I\"],\n",
    "    \"South Carolina\": [\"SC\", \"S.C\"],\n",
    "    \"South Dakota\": [\"SD\", \"S.D\"],\n",
    "    \"Tennessee\": [\"TN\", \"T.N\"],\n",
    "    \"Texas\": [\"TX\", \"T.X\"],\n",
    "    \"Utah\": [\"UT\", \"U.T\"],\n",
    "    \"Vermont\": [\"VT\", \"V.T\"],\n",
    "    \"Virginia\": [\"VA\", \"V.A\"],\n",
    "    \"Washington\": [\"WA\", \"W.A\"],\n",
    "    \"West Virginia\": [\"WV\", \"W.V\"],\n",
    "    \"Wisconsin\": [\"WI\", \"W.I\"],\n",
    "    \"Wyoming\": [\"WY\", \"W.Y\"],\n",
    "}\n",
    "\n",
    "# Additional Preprocessing of Configurations\n",
    "presidential_candidates = {presidential_candidate: list(set(names)) for presidential_candidate, names in presidential_candidates.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Transcripts (Documents) to CSV: Extraction and Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T07:29:42.873179Z",
     "start_time": "2024-12-07T07:28:27.184980Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae46a2bd03074556beb80975e9222b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing Documents for Michigan [0/320 Transcript]:   0%|          | 0/320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d2f0eaf02648b99c68e4c2a40967ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing Documents for Arizona [0/305 Transcript]:   0%|          | 0/305 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0021964187c34207844e3060eccfaf84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing Documents for Pennsylvania [0/268 Transcript]:   0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents: 893\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Possible_State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here's your blue wall blitz, Ms. Perino. All r...</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This morning we are continuing our post debate...</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kamala Harris leads in key battleground states...</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Biden administration said on Tuesday it wo...</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Election night on Sky News is going to be very...</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>We're here live in Butler, PA at another merch...</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>Pennsylvania's latest poll results are just un...</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>When you watch this clip, I want you to ask yo...</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>And so we have this Quinnipiac poll and this w...</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>All you hear is that the Democrats are the one...</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>893 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Document Possible_State\n",
       "0    Here's your blue wall blitz, Ms. Perino. All r...       Michigan\n",
       "1    This morning we are continuing our post debate...       Michigan\n",
       "2    Kamala Harris leads in key battleground states...       Michigan\n",
       "3    The Biden administration said on Tuesday it wo...       Michigan\n",
       "4    Election night on Sky News is going to be very...       Michigan\n",
       "..                                                 ...            ...\n",
       "888  We're here live in Butler, PA at another merch...   Pennsylvania\n",
       "889  Pennsylvania's latest poll results are just un...   Pennsylvania\n",
       "890  When you watch this clip, I want you to ask yo...   Pennsylvania\n",
       "891  And so we have this Quinnipiac poll and this w...   Pennsylvania\n",
       "892  All you hear is that the Democrats are the one...   Pennsylvania\n",
       "\n",
       "[893 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_transcripts_into_csv_of_documents() -> pd.DataFrame:\n",
    "    # Initialize list of sentences and possible states\n",
    "    list_of_documents = []\n",
    "    \n",
    "    # Collect documents from each state's transcription files\n",
    "    for state, path in cities_transcription_paths.items():\n",
    "        transcription_files = os.listdir(path)\n",
    "        total_transcription_files = len(transcription_files)\n",
    "\n",
    "        with tqdm(total=total_transcription_files, desc=f'Preprocessing Documents for {state} [0/{total_transcription_files} Transcript]') as pbar:\n",
    "            for index, filename in enumerate(transcription_files):\n",
    "                current = f'{index + 1}/{total_transcription_files}'\n",
    "                if filename == \".ipynb_checkpoints\":\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                pbar.set_description(f'Preprocessing Documents for {state} [{current} Transcript]')\n",
    "\n",
    "                # Open transcription file\n",
    "                file_path = os.path.join(path, filename)\n",
    "                with open(file_path, \"r\", errors=\"ignore\") as file:\n",
    "                    transcription = file.read()\n",
    "\n",
    "                    # Split transcript into tokenized sentences\n",
    "                    tokenized_sentences = sent_tokenize(transcription)\n",
    "\n",
    "                    # Merge related sentence tokens to complete sentence\n",
    "                    sentences = []\n",
    "                    previous_sentence_token = \"\"\n",
    "                    \n",
    "                    for i, tokenized_sentence in enumerate(tokenized_sentences):\n",
    "                        # Strip Unnecessary White Spaces\n",
    "                        tokenized_sentence = tokenized_sentence.strip()\n",
    "                        \n",
    "                        # Remove if Tokenized Sentence without Punctuation is Empty\n",
    "                        if tokenized_sentence[:-1].strip() == \"\": continue\n",
    "                        \n",
    "                        # Remove consecutive duplicates (Whisper Hallucination)\n",
    "                        if i != 0 and tokenized_sentence == tokenized_sentences[i - 1]: continue\n",
    "                        \n",
    "                        \"\"\"\n",
    "                            Add Current Sentence Token to Previous If Either:\n",
    "                                1) Merged Sentences Is Still Incomplete \n",
    "                                2) Previous Sentence is a question\n",
    "                                3) Previous Sentence ends with ellipsis\n",
    "                        \"\"\"\n",
    "                        def is_sentence_not_complete(sentence_token: str):\n",
    "                            pos_tags = pos_tag(word_tokenize(sentence_token))\n",
    "                        \n",
    "                            subject_count = 0\n",
    "                            predicate_count = 0\n",
    "                        \n",
    "                            for word, tag in pos_tags:\n",
    "                                if tag in {\"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PRP\"}: # At least 2 Subject\n",
    "                                    subject_count += 1\n",
    "                                elif tag.startswith(\"V\"):  # At least 1 Verb\n",
    "                                    predicate_count += 1\n",
    "                                # Early Check and Return\n",
    "                                if subject_count >= 2 and predicate_count >= 1:\n",
    "                                    return False\n",
    "                        \n",
    "                            return True\n",
    "                        \n",
    "                        current_merged_sentence_tokens = sentences[-1] if i > 0 else None\n",
    "                        if (\n",
    "                            current_merged_sentence_tokens is not None\n",
    "                            and (\n",
    "                                # Incomplete Merged Sentence Tokens\n",
    "                                is_sentence_not_complete(current_merged_sentence_tokens) \n",
    "                                # Previous Sentence Token is a Question\n",
    "                                or previous_sentence_token.endswith(\"?\")\n",
    "                                # Previous Sentence Token Ends with Ellipsis\n",
    "                                or previous_sentence_token.endswith(\"...\")\n",
    "                                or previous_sentence_token.endswith(\"..\") \n",
    "                            )\n",
    "                        ):\n",
    "                            # If conditions are met, connect with the previous sentence\n",
    "                            if previous_sentence_token.endswith(\"...\"):\n",
    "                                sentences[-1] = f'{current_merged_sentence_tokens[:-3]}, {tokenized_sentence[:1].lower()}{tokenized_sentence[1:]}'\n",
    "                            elif previous_sentence_token.endswith(\"..\"):\n",
    "                                sentences[-1] = f'{current_merged_sentence_tokens[:-2]}, {tokenized_sentence[:1].lower()}{tokenized_sentence[1:]}'\n",
    "                            elif previous_sentence_token.endswith(\".\"):\n",
    "                                sentences[-1] = f'{current_merged_sentence_tokens[:-1]}, {tokenized_sentence[:1].lower()}{tokenized_sentence[1:]}'\n",
    "                            elif previous_sentence_token.endswith(\"?\"):\n",
    "                                if tokenized_sentence.endswith(\"?\"):\n",
    "                                    sentences[-1] = f'{current_merged_sentence_tokens[:-1]}, {tokenized_sentence[:1].lower()}{tokenized_sentence[1:]}'\n",
    "                                else:\n",
    "                                 sentences[-1] = f'{current_merged_sentence_tokens[:-1]}: {tokenized_sentence[:1].lower()}{tokenized_sentence[1:]}'\n",
    "                            else:\n",
    "                                sentences[-1] = f'{current_merged_sentence_tokens} {tokenized_sentence}'\n",
    "                        else:\n",
    "                            # Otherwise, treat as a new sentence\n",
    "                            sentences.append(tokenized_sentence)\n",
    "                \n",
    "                        # Update the previous sentence\n",
    "                        previous_sentence_token = tokenized_sentence\n",
    "                        \n",
    "                    # Append each transcription documents with the possible-state\n",
    "                    list_of_documents.append((\" \".join(sentences).strip(), state))\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Convert the list of sentences and states into a DataFrame\n",
    "    df = pd.DataFrame(list_of_documents, columns=[\"Document\", \"Possible_State\"])\n",
    "    df.to_csv(transcript_documents_filename, index=False, errors=\"ignore\")\n",
    "    return df\n",
    "\n",
    "list_of_documents = preprocess_transcripts_into_csv_of_documents()\n",
    "print(f'Number of Documents: {len(list_of_documents)}')\n",
    "list_of_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Filtered Relevant Sentences to CSV: Coreference Resolution and Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T08:10:37.731949Z",
     "start_time": "2024-12-07T07:29:42.931197Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4882a4674fb94d68ba74174308370e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting Relevant Sentences [0/888 Documents]:   0%|          | 0/888 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Presidential_Candidate</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We know that Harris has no current plans to ca...</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The data is clear, you must separate yourself,...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I tried to tell Peter Alexander this on Friday...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michigan's only boomerang county that backed O...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Okay, then what happened: and then I just felt...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8408</th>\n",
       "      <td>Ace, I really hope the nonsense of trying to h...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8409</th>\n",
       "      <td>We need to simmer down now when it comes to so...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8410</th>\n",
       "      <td>He seems like a dude who's just obsessed with ...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8411</th>\n",
       "      <td>Donald Trump won Pennsylvania by 40,000 votes.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8412</th>\n",
       "      <td>And veterans furthermore do not take kindly to...</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8413 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence  \\\n",
       "0     We know that Harris has no current plans to ca...   \n",
       "1     The data is clear, you must separate yourself,...   \n",
       "2     I tried to tell Peter Alexander this on Friday...   \n",
       "3     Michigan's only boomerang county that backed O...   \n",
       "4     Okay, then what happened: and then I just felt...   \n",
       "...                                                 ...   \n",
       "8408  Ace, I really hope the nonsense of trying to h...   \n",
       "8409  We need to simmer down now when it comes to so...   \n",
       "8410  He seems like a dude who's just obsessed with ...   \n",
       "8411     Donald Trump won Pennsylvania by 40,000 votes.   \n",
       "8412  And veterans furthermore do not take kindly to...   \n",
       "\n",
       "     Presidential_Candidate         State  \n",
       "0             Kamala Harris      Michigan  \n",
       "1              Donald Trump      Michigan  \n",
       "2              Donald Trump      Michigan  \n",
       "3              Donald Trump      Michigan  \n",
       "4              Donald Trump      Michigan  \n",
       "...                     ...           ...  \n",
       "8408           Donald Trump  Pennsylvania  \n",
       "8409           Donald Trump  Pennsylvania  \n",
       "8410           Donald Trump  Pennsylvania  \n",
       "8411           Donald Trump  Pennsylvania  \n",
       "8412          Kamala Harris  Pennsylvania  \n",
       "\n",
       "[8413 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_relevant_sentences() -> pd.DataFrame:\n",
    "    # Get All Collected Sentences from Transcript and a Map with their Respective Possible State\n",
    "    df = pd.read_csv(transcript_documents_filename)\n",
    "    documents = pd.Series(df['Possible_State'].values, index=df['Document']).to_dict()\n",
    "\n",
    "    # Initialize Lists for Relevant Sentences\n",
    "    list_of_relevant_sentences = []\n",
    "    seen_relevant_sentences = set()\n",
    "\n",
    "    document_items = documents.items()\n",
    "    total_document_items = len(document_items)\n",
    "    processed_document = 0\n",
    "    with tqdm(total=total_document_items, desc=f'Extracting Relevant Sentences [0/{total_document_items} Documents]') as pbar:\n",
    "        for document, possible_state in document_items:\n",
    "            # Get Document Object from Language Model\n",
    "            document_obj = nlp(document)\n",
    "\n",
    "            # Get Relevant Sentences for Candidates\n",
    "            for sentence_obj in document_obj.sents:\n",
    "                sentence = sentence_obj.text.strip() # Get Text in String\n",
    "                presidential_candidate_mentions = set() # Avoid Duplicates\n",
    "                no_other_state_mentioned_different_from_possible_state = True\n",
    "                possible_relevant_sentence = None\n",
    "\n",
    "                # Define Methods for Relevant Sentences\n",
    "                def get_mentions(sentence_object: str, max_ngrams: int = max_pair_of_words_for_main_subject_mention) -> dict:\n",
    "                    mentions = {}\n",
    "                    for n in range(1, max_ngrams + 1): # Try n-gram mentions sizes from 1 to max_n\n",
    "                        for idx in range(n, len(sentence_object) + 1): # Form n-gram length mentions from sentences\n",
    "                            span = sentence_object[idx - n:idx] # n-gram span\n",
    "                            if span._.is_coref: # Add only if span has coreference info\n",
    "                                main_mention = span._.coref_cluster.main.text # Get string from span object\n",
    "                                mentions[idx-n] = main_mention # Use the index of the first word in mentioned n-gram\n",
    "                                \n",
    "                    return mentions\n",
    "\n",
    "                def add_and_get_presidential_candidate_mentions(ngramed_mention: str) -> set:\n",
    "                    for presidential_candidate, names in presidential_candidates.items():\n",
    "                        if (\n",
    "                            any(\n",
    "                                # Check in Mentions and Words\n",
    "                                (\n",
    "                                    ngramed_mention and presidential_candidate\n",
    "                                    and f' {presidential_candidate.strip().lower()} ' in f' {ngramed_mention.strip().lower()} '\n",
    "                                ) or (\n",
    "                                    word and presidential_candidate\n",
    "                                    and presidential_candidate.strip().lower() == word.strip().lower()\n",
    "                                )\n",
    "                                for word in ngramed_mention.split(\" \") + word_tokenize(sentence)\n",
    "                            )\n",
    "                            # Any Other Candidate Names is Mentioned in Topic\n",
    "                            or any(\n",
    "                                # Check in Mentions and Words\n",
    "                                (\n",
    "                                    ngramed_mention and name\n",
    "                                    and f' {name.strip().lower()} ' in f' {ngramed_mention.strip().lower()} '\n",
    "                                ) or (\n",
    "                                    word and name\n",
    "                                    and name.strip().lower() == word.strip().lower()\n",
    "                                )\n",
    "                                for name in names\n",
    "                                for word in ngramed_mention.split(\" \") + word_tokenize(sentence)\n",
    "                            )\n",
    "                        ):\n",
    "                            presidential_candidate_mentions.add(presidential_candidate)\n",
    "\n",
    "                    return presidential_candidate_mentions\n",
    "\n",
    "                def check_possible_state_is_mentioned(ngramed_mention: str) -> bool:\n",
    "                    if possible_state not in state_cities: raise ValueError(f'This Sentence has Invalid Possible State ({possible_state}): \"{sentence}\"')\n",
    "                    # Filter Sentence with Topic of [Other State] Not in [Arizona, Michigan, Pennsylvania]\n",
    "                    if possible_state not in original_state_cities: return False\n",
    "                \n",
    "                    other_possible_state_is_mentioned = False\n",
    "                    # Filter Sentence with Topic of [Other State] Different from its [Possible State]\n",
    "                    other_states = [state for state in state_cities if state is not possible_state]\n",
    "                    if any(\n",
    "                        # Check in Mentions and Words\n",
    "                        (\n",
    "                            ngramed_mention and other_state\n",
    "                            and f' {other_state.strip().lower()} ' in f' {ngramed_mention.strip().lower()} '\n",
    "                        ) or (\n",
    "                            word and other_state\n",
    "                            and other_state.strip().lower() == word.strip().lower()\n",
    "                        )\n",
    "                        for other_state in other_states\n",
    "                        for word in ngramed_mention.split(\" \") + word_tokenize(sentence)\n",
    "                    ): other_possible_state_is_mentioned = True\n",
    "\n",
    "                    # Filter Sentence with Topics of [Other States' Cities] Different from its [Possible State Cities]\n",
    "                    other_state_cities = [\n",
    "                        other_city\n",
    "                        for other_cities in {\n",
    "                            state: state_cities[state]\n",
    "                            for state in state_cities\n",
    "                            if state is not possible_state\n",
    "                        }.values()\n",
    "                        for other_city in other_cities\n",
    "                        if other_city\n",
    "                    ]\n",
    "                    if any(\n",
    "                        # Check in Mentions and Words\n",
    "                        (\n",
    "                            ngramed_mention and other_city\n",
    "                            and f' {other_city.strip().lower()} ' in f' {ngramed_mention.strip().lower()} '\n",
    "                        ) or (\n",
    "                            word and other_city\n",
    "                            and other_city.strip().lower() == word.strip().lower()\n",
    "                        )\n",
    "                        for other_city in other_state_cities\n",
    "                        for word in ngramed_mention.split(\" \") + word_tokenize(sentence)\n",
    "                    ): other_possible_state_is_mentioned = True\n",
    "                    \n",
    "                    if other_possible_state_is_mentioned:\n",
    "                        # Check if possible state is mentioned too\n",
    "                        if any(\n",
    "                            # Check in Mentions and Words\n",
    "                            (\n",
    "                                ngramed_mention and possible_state\n",
    "                                and f' {possible_state.strip().lower()} ' in f' {ngramed_mention.strip().lower()} '\n",
    "                            ) or (\n",
    "                                word and possible_state\n",
    "                                and possible_state.strip().lower() == word.strip().lower()\n",
    "                            )\n",
    "                            for word in ngramed_mention.split(\" \") + word_tokenize(sentence)\n",
    "                        ): return True\n",
    "                        # Check if cities in possible state is mentioned too\n",
    "                        possible_state_cities = state_cities[possible_state]\n",
    "                        if any(\n",
    "                            # Check in Mentions and Words\n",
    "                            (\n",
    "                                ngramed_mention and possible_state_city\n",
    "                                and f' {possible_state_city.strip().lower()} ' in f' {ngramed_mention.strip().lower()} '\n",
    "                            ) or (\n",
    "                                word and possible_state_city\n",
    "                                and possible_state_city.strip().lower() == word.strip().lower()\n",
    "                            )\n",
    "                            for possible_state_city in possible_state_cities\n",
    "                            for word in ngramed_mention.split(\" \") + word_tokenize(sentence)\n",
    "                        ): return True\n",
    "                    \n",
    "                        return False\n",
    "                    \n",
    "                    return True\n",
    "\n",
    "                mentions = get_mentions(sentence_obj)\n",
    "                for mention_idx, ngramed_mention in mentions.items():\n",
    "                    # Ensure No Other State are Mentioned Different from Possible State\n",
    "                    if no_other_state_mentioned_different_from_possible_state:\n",
    "                        no_other_state_mentioned_different_from_possible_state = check_possible_state_is_mentioned(ngramed_mention)\n",
    "                        if not no_other_state_mentioned_different_from_possible_state: break\n",
    "                    else: break\n",
    "\n",
    "                    # Add Mentioned Candidate and Ensure Only 1 Candidate is Mentioned\n",
    "                    if len(add_and_get_presidential_candidate_mentions(ngramed_mention)) > 1: break\n",
    "                    \n",
    "                    if possible_relevant_sentence is None:\n",
    "                        word = nlp(sentence)\n",
    "                        for idx, token in enumerate(word):\n",
    "                            if mention_idx == idx and token.dep_ in {\n",
    "                                \"nsubj\",\n",
    "                                \"nsubjpass\",\n",
    "                                \"compound\",\n",
    "                                \"dobj\",\n",
    "                                \"poss\"\n",
    "                            }: possible_relevant_sentence = sentence\n",
    "\n",
    "                if (\n",
    "                    # Relevant Sentence was Found and Unique \n",
    "                    possible_relevant_sentence is not None\n",
    "                    and possible_relevant_sentence not in seen_relevant_sentences\n",
    "                    # Re-ensure No Other State is Mentioned Aside from Possible State\n",
    "                    and no_other_state_mentioned_different_from_possible_state\n",
    "                    # Re-ensure Only 1 Candidate is Mentioned\n",
    "                    and len(presidential_candidate_mentions) == 1\n",
    "                ):\n",
    "                    # Additional Cleaning\n",
    "                    if possible_relevant_sentence.startswith(\", \"):\n",
    "                        possible_relevant_sentence = possible_relevant_sentence[2:]\n",
    "                    # Get Presidential Candidate Mentioned\n",
    "                    presidential_candidate = presidential_candidate_mentions.pop()\n",
    "                    # Add Relevant Sentence\n",
    "                    list_of_relevant_sentences.append({\n",
    "                        \"Sentence\": possible_relevant_sentence,\n",
    "                        \"Presidential_Candidate\": presidential_candidate,\n",
    "                        \"State\": possible_state\n",
    "                    })\n",
    "                    seen_relevant_sentences.add(possible_relevant_sentence)\n",
    "\n",
    "            processed_document += 1\n",
    "            pbar.set_description(f'Extracting Relevant Sentences [{processed_document}/{total_document_items} Documents]')\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Save List of All Relevant Sentences into CSV file\n",
    "    df = pd.DataFrame(list_of_relevant_sentences)\n",
    "    df.to_csv(relevant_transcript_sentences_filename, index=False, errors=\"ignore\")\n",
    "    return df\n",
    "\n",
    "list_of_relevant_sentences = filter_relevant_sentences()\n",
    "list_of_relevant_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T08:10:38.060102Z",
     "start_time": "2024-12-07T08:10:38.028850Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Presidential_Candidate</th>\n",
       "      <th>State</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>1478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>1587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>1904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>1161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>1339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>Total</td>\n",
       "      <td>8413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Presidential_Candidate         State  count\n",
       "0           Donald Trump       Arizona   1478\n",
       "1           Donald Trump      Michigan   1587\n",
       "2           Donald Trump  Pennsylvania   1904\n",
       "3          Kamala Harris       Arizona    944\n",
       "4          Kamala Harris      Michigan   1161\n",
       "5          Kamala Harris  Pennsylvania   1339\n",
       "6                                Total   8413"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sa tingin ko need natin 5k sentences minimum for Relevant Sentences di lang for gathered.\n",
    "Kasi mamaya 5k Random Sentences nakuha natin tas 100 lang dun Relevant with candidate & state.\n",
    "\n",
    "Ang naiisip ko since meron 6 Combinations = 3 candidate * 2 state\n",
    "Gawin natin 5000/6 = 834 Relevant Sentences required set natin as minimum per Combination\n",
    "\n",
    "Trump  - Arizona      = 834 Relevant Sentences\n",
    "Harris - Arizona      = 834 Relevant Sentences\n",
    "Trump  - Michigan     = 834 Relevant Sentences\n",
    "Harris - Michigan     = 834 Relevant Sentences\n",
    "Trump  - Pennsylvania = 834 Relevant Sentences\n",
    "Harris - Pennsylvania = 834 Relevant Sentences\n",
    "               -------------------------------\n",
    "               Total: ~5000 Relevant Sentences\n",
    "\"\"\"\n",
    "def print_statistics():\n",
    "    try:\n",
    "        grouped_df = (\n",
    "            list_of_relevant_sentences\n",
    "            .groupby([\"Presidential_Candidate\", \"State\"])\n",
    "            .size()\n",
    "            .reset_index(name=\"count\")\n",
    "        )\n",
    "        total_count = grouped_df[\"count\"].sum()\n",
    "        total_row = pd.DataFrame({\"Presidential_Candidate\": [\"\"], \"State\": [\"Total\"], \"count\": [total_count]})\n",
    "        grouped_df = pd.concat([grouped_df, total_row], ignore_index=True)\n",
    "        return grouped_df\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        return \"No Relevant Sentences\"\n",
    "print_statistics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
