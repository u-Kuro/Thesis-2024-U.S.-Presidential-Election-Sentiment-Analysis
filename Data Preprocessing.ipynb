{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "import os, re, torch, nltk\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Additional Downloads\n",
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename: str) -> str:\n",
    "    # Escape Double Quotes\n",
    "    filename = filename.replace('\"', '\\\\\"')\n",
    "\n",
    "    # Replace Invalid Characters with \"_\"\n",
    "    invalid_chars = re.compile(r'[<>:\"/\\\\|?*]')\n",
    "    sanitized_filename = invalid_chars.sub(\"_\", filename)\n",
    "\n",
    "    return sanitized_filename\n",
    "    \n",
    "def read_unique_items_from_file(file: str) -> list:\n",
    "    if os.path.exists(file):\n",
    "        with open(file, \"r\") as f:\n",
    "            return list(set(e.strip() for e in f.readlines() if e.strip()))\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# File Names\n",
    "transcript_sentences_filename = \"transcript_sentences.csv\"\n",
    "relevant_transcript_sentences_filename = \"relevant_transcript_sentences.csv\"\n",
    "\n",
    "# Folder Names\n",
    "transcription_output_path = \"Transcription\"\n",
    "cities_path = \"State Cities\"\n",
    "\n",
    "# Numeric Constants \n",
    "max_consecutive_words_for_topic = 2 # e.g. Unigram: \"Donald\" | Bigram: \"Donald Trump\"\n",
    "min_number_of_word_in_relevant_sentence = 5 # e.g. 5-words: \"This is a nice place\"\n",
    "min_similarity_of_topic_modeling = 0.7 # Allow Sentences 70% Similar to Candidate-State Combination\n",
    "\n",
    "# Sentence Categories\n",
    "presidential_candidates = {\n",
    "    \"Donald Trump\": [\n",
    "        \"Donald\", \"Trump\"\n",
    "    ],\n",
    "    \"Kamala Harris\": [\n",
    "        \"Kamala\", \"Harris\"\n",
    "    ]\n",
    "}\n",
    "state_cities = {\n",
    "    \"Michigan\": read_unique_items_from_file(os.path.join(cities_path, \"michigan-cities.txt\")),\n",
    "    \"Arizona\": read_unique_items_from_file(os.path.join(cities_path, \"arizona-cities.txt\")),\n",
    "    \"Pennsylvania\": read_unique_items_from_file(os.path.join(cities_path, \"pennsylvania-cities.txt\"))\n",
    "}\n",
    "\n",
    "# Words for Sentence Filtering\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Additional Preprocessing of Configurations\n",
    "presidential_candidates = {presidential_candidate: list(set(names)) for presidential_candidate, names in presidential_candidates.items()}\n",
    "presidential_candidates_and_states_combinations = [\n",
    "    f\"{location}_{name}\".lower()\n",
    "    for full_name, names in presidential_candidates.items() \n",
    "    for location in [state for state in state_cities] + [city for cities in state_cities.values() for city in cities]\n",
    "    for name in [full_name] + names\n",
    "]\n",
    "presidential_candidates_and_states_combinations_in_2d = [\n",
    "    [location.lower(), full_name.lower()] + [name.lower() for name in names]\n",
    "    for full_name, names in presidential_candidates.items() \n",
    "    for location in [state for state in state_cities] + [city for cities in state_cities.values() for city in cities]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Extraction (Transcripts to CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f6d0b1959c4512a407581f24c6a5ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Sentences [0/295 Transcript]:   0%|          | 0/295 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences: 26452\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our daughter Sophia was born into this world w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She says these are the words of an autocrat, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15 states, plus the 2nd congressional distric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I will sell my car right now.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm going to zoom in here on the Philadelphia ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence\n",
       "0  Our daughter Sophia was born into this world w...\n",
       "1  She says these are the words of an autocrat, s...\n",
       "2   15 states, plus the 2nd congressional distric...\n",
       "3                      I will sell my car right now.\n",
       "4  I'm going to zoom in here on the Philadelphia ..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_transcripts_into_csv_of_sentences() -> DataFrame:    \n",
    "    # Initialize List of Sentences\n",
    "    list_of_sentences = []\n",
    "        \n",
    "    # Collect List of Sentences from Transcription Files\n",
    "    transcription_files = os.listdir(transcription_output_path)\n",
    "    total_transcription_file = len(transcription_files)\n",
    "    with tqdm(total=total_transcription_file, desc=f'Collecting Sentences [0/{total_transcription_file} Transcript]') as pbar:\n",
    "        for index, filename in enumerate(transcription_files):\n",
    "            current = f'{index+1}/{total_transcription_file}'\n",
    "            if filename == \".ipynb_checkpoints\":\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            pbar.set_description(f'Collecting Sentences [{current} Transcript]')\n",
    "\n",
    "            # Open Transcription File\n",
    "            file_path = os.path.join(transcription_output_path, filename)\n",
    "            with open(file_path, \"r\") as file:\n",
    "                transcription = file.read()\n",
    "                \n",
    "                # Split Transcript into Sentences\n",
    "                sentences = sent_tokenize(transcription)\n",
    "\n",
    "                # Remove Consecutive Duplicates (Caused by Whisper)\n",
    "                sentences = [sentence for i, sentence in enumerate(sentences) if i == 0 or sentence != sentences[i-1]]\n",
    "                \n",
    "                # Add the Sentences\n",
    "                list_of_sentences.extend(sentences)\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "    # Save List of All Sentences into CSV file\n",
    "    df = pd.DataFrame(list(set(list_of_sentences)), columns=[\"Sentence\"])\n",
    "    df.to_csv(transcript_sentences_filename, index=False, errors=\"ignore\")\n",
    "    return df\n",
    "\n",
    "list_of_sentences = process_transcripts_into_csv_of_sentences()\n",
    "print(f'Number of Sentences: {len(list_of_sentences)}')\n",
    "list_of_sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic: Relevant Sentence Filtering (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 19:14:56,985 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aca57f1537e4f6aba3391edf301e36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/827 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 19:16:26,571 - BERTopic - Embedding - Completed ✓\n",
      "2024-10-22 19:16:26,574 - BERTopic - Guided - Find embeddings highly related to seeded topics.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf00c4f3bd464eaa9996e96792e41daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 19:16:30,492 - BERTopic - Guided - Completed ✓\n",
      "2024-10-22 19:16:30,505 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-10-22 19:17:10,220 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-10-22 19:17:10,235 - BERTopic - Zeroshot Step 1 - Finding documents that could be assigned to either one of the zero-shot topics\n",
      "2024-10-22 19:17:15,367 - BERTopic - Zeroshot Step 1 - Completed ✓\n",
      "2024-10-22 19:17:32,757 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-10-22 19:17:40,518 - BERTopic - Cluster - Completed ✓\n",
      "2024-10-22 19:17:40,520 - BERTopic - Zeroshot Step 2 - Combining topics from zero-shot topic modeling with topics from clustering...\n",
      "2024-10-22 19:17:40,696 - BERTopic - Zeroshot Step 2 - Completed ✓\n",
      "2024-10-22 19:17:40,696 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-10-22 19:18:03,389 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Relevant Sentences: 1816\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Presidential_Candidate</th>\n",
       "      <th>State</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>On the counter, women under 40 and women acros...</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[counter women, areas past, past areas, harris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In our Commonwealth, Kamala Harris just seems ...</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[counter women, areas past, past areas, harris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona is incredibly close.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>[arizona, arizona arizona, arizona lead, trump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Well, hello Arizona.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>[arizona, arizona arizona, arizona lead, trump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump's winning Arizona, according to the Real...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>[arizona, arizona arizona, arizona lead, trump...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Presidential_Candidate  \\\n",
       "0  On the counter, women under 40 and women acros...          Kamala Harris   \n",
       "1  In our Commonwealth, Kamala Harris just seems ...          Kamala Harris   \n",
       "2                       Arizona is incredibly close.           Donald Trump   \n",
       "3                               Well, hello Arizona.           Donald Trump   \n",
       "4  Trump's winning Arizona, according to the Real...           Donald Trump   \n",
       "\n",
       "          State                                     Topic_Keywords  \n",
       "0  Pennsylvania  [counter women, areas past, past areas, harris...  \n",
       "1  Pennsylvania  [counter women, areas past, past areas, harris...  \n",
       "2       Arizona  [arizona, arizona arizona, arizona lead, trump...  \n",
       "3       Arizona  [arizona, arizona arizona, arizona lead, trump...  \n",
       "4       Arizona  [arizona, arizona arizona, arizona lead, trump...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_relevant_sentences() -> tuple[DataFrame, BERTopic]:\n",
    "    # Get All Collected Sentences from Transcript\n",
    "    df = pd.read_csv(transcript_sentences_filename, encoding_errors=\"ignore\")\n",
    "    sentences = df[\"Sentence\"].tolist()\n",
    "    \n",
    "    # Set Filter for Words as Possible Topics\n",
    "    def filter_possible_topics(text: str) -> list:\n",
    "        \"\"\"\n",
    "        Filter Words If its a Possible Topic:\n",
    "            1) Only Nouns and Proper Nouns (e.g. Dollars, Currency)\n",
    "            2) No Stop Words (e.g. in, to)\n",
    "            3) No Generic Abstract Nouns (e.g. thing, stuff)\n",
    "            4) Minumum of Three Letter Words (e.g. USA)\n",
    "            5) Exclude Numbers\n",
    "        \"\"\"\n",
    "        \n",
    "        pos_tags = pos_tag(word_tokenize(text)) # POS Tagging\n",
    "        possible_topics = [\n",
    "            token.lower() for token, pos in pos_tags\n",
    "            if pos in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"] # Nouns / Proper Nouns\n",
    "            and token.lower() not in stop_words # Exclude Stop Words\n",
    "            and len(token) > 1 # Exclude One Letter Words (e.g. Included: Ox)\n",
    "            and not token.isnumeric() # Exclude Numbers\n",
    "        ]\n",
    "        \n",
    "        return possible_topics\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        ngram_range=(1, max_consecutive_words_for_topic),\n",
    "        tokenizer=filter_possible_topics\n",
    "    )\n",
    "\n",
    "    # Train BERTopic model\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        n_gram_range=(1, max_consecutive_words_for_topic),\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        seed_topic_list=presidential_candidates_and_states_combinations_in_2d,\n",
    "        zeroshot_topic_list=presidential_candidates_and_states_combinations,\n",
    "        zeroshot_min_similarity=min_similarity_of_topic_modeling,\n",
    "        verbose=True\n",
    "    )\n",
    "    topics, _ = topic_model.fit_transform(sentences)\n",
    "    \n",
    "    # Get BERTopic Results\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    topics_and_documents = pd.DataFrame({\"Topic\": topics, \"Representative_Docs\": sentences})\n",
    "    \n",
    "    # Initialize Lists for Relevant Sentences\n",
    "    list_of_relevant_sentences = []\n",
    "\n",
    "    def is_sentence_complete(sentence: str) -> bool:\n",
    "         # Exclude Sentence with Less than 5 or N Words\n",
    "        return len(word_tokenize(sentence)) < minimum_number_of_word_in_relevant_sentence\n",
    "        \n",
    "    # Get Relevant Sentences\n",
    "    for _, row in topic_info.iterrows():\n",
    "        topic = row[\"Topic\"]\n",
    "        if topic == -1: continue # Skip Outlier\n",
    "\n",
    "        # Get List of Relevant Topics and Sentences\n",
    "        topic_keywords = row[\"Representation\"]\n",
    "        relevant_sentences = topics_and_documents[topics_and_documents[\"Topic\"] == topic][\"Representative_Docs\"].tolist()\n",
    "        \n",
    "        # Check Candidate Mentions in Topics\n",
    "        presidential_candidate_mentions = set() # Avoid Duplicates\n",
    "        for presidential_candidate, names in presidential_candidates.items():\n",
    "            if (\n",
    "                any(name.lower() in keyword.lower() for name in names for keyword in topic_keywords) \n",
    "                or any(presidential_candidate.lower() in keyword.lower() for keyword in topic_keywords)\n",
    "            ): \n",
    "                presidential_candidate_mentions.add(presidential_candidate)\n",
    "        \n",
    "        # Make Sure Only 1 Candidate is Mentioned\n",
    "        if len(presidential_candidate_mentions) != 1: continue\n",
    "\n",
    "        # Check State Mentions in Topics (Including Cities)\n",
    "        state_mentions = set() # Avoid Duplicates\n",
    "        for state, cities in state_cities.items():\n",
    "            if (\n",
    "                any(city.lower() in keyword.lower() for city in cities for keyword in topic_keywords) \n",
    "                or any(state.lower() in keyword.lower() for keyword in topic_keywords)\n",
    "            ): \n",
    "                state_mentions.add(state)\n",
    "\n",
    "        # Make Sure Only 1 State is Mentioned\n",
    "        if len(state_mentions) != 1: continue\n",
    "        \"\"\"\n",
    "        Add Relevant Sentences Only If:\n",
    "            1) Only 1 Candidate is Mentioned\n",
    "            2) Only 1 State is Mentioned\n",
    "        \"\"\"\n",
    "        if (\n",
    "            len(presidential_candidate_mentions) == 1\n",
    "            and len(state_mentions) == 1\n",
    "        ):\n",
    "            presidential_candidate = presidential_candidate_mentions.pop()\n",
    "            state = state_mentions.pop()\n",
    "            \n",
    "            # Add All Relevant Sentences with their Corresponding Presidential Candidate, State, and Topic Keywords\n",
    "            for sentence in relevant_sentences:\n",
    "                # Filter Complete Sentence\n",
    "                if len(word_tokenize(sentence)) >= min_number_of_word_in_relevant_sentence:\n",
    "                    list_of_relevant_sentences.append({\n",
    "                        \"Sentence\": sentence,\n",
    "                        \"Presidential_Candidate\": presidential_candidate,\n",
    "                        \"State\": state,\n",
    "                        \"Topic_Keywords\": topic_keywords\n",
    "                    })\n",
    "    \n",
    "    # Save List of All Relevant Sentences into CSV file\n",
    "    df = pd.DataFrame(list_of_relevant_sentences)\n",
    "    df.to_csv(relevant_transcript_sentences_filename, index=False, errors=\"ignore\")\n",
    "    return df, topic_model\n",
    "\n",
    "list_of_relevant_sentences, bertopic_model = filter_relevant_sentences()\n",
    "print(f'Number of Relevant Sentences: {len(list_of_relevant_sentences)}')\n",
    "list_of_relevant_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>10936</td>\n",
       "      <td>-1_trump_candidates_people_day</td>\n",
       "      <td>[trump, candidates, people, day, election, dem...</td>\n",
       "      <td>[But do you care more about the election now?,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>michigan_donald trump</td>\n",
       "      <td>[ways party, dominance election, polls leads, ...</td>\n",
       "      <td>[And if we recall the 2016 election, it wasn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>michigan_donald</td>\n",
       "      <td>[michigan state, today michigan, look michigan...</td>\n",
       "      <td>[And if you look at, let's just look at Michig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>michigan_trump</td>\n",
       "      <td>[michigan, votes michigan, michigan michigan, ...</td>\n",
       "      <td>[A one point swing in Wisconsin and Michigan m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>elizabethtown_kamala harris</td>\n",
       "      <td>[counter women, areas past, past areas, harris...</td>\n",
       "      <td>[In our Commonwealth, Kamala Harris just seems...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>521</td>\n",
       "      <td>14</td>\n",
       "      <td>521_person beliefs_mates person_brothers shop_...</td>\n",
       "      <td>[person beliefs, mates person, brothers shop, ...</td>\n",
       "      <td>[You vote for a leader who sees you, who has a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>522</td>\n",
       "      <td>14</td>\n",
       "      <td>522_people something_november vote_election tr...</td>\n",
       "      <td>[people something, november vote, election try...</td>\n",
       "      <td>[And then once you've got a plan, then you've ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>523</td>\n",
       "      <td>41</td>\n",
       "      <td>523_vote vote_vote_vote people_people vote</td>\n",
       "      <td>[vote vote, vote, vote people, people vote, no...</td>\n",
       "      <td>[Put down your vote and do what?, Vote for vot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>524</td>\n",
       "      <td>33</td>\n",
       "      <td>524_voting_voting mail_states voting_vote elec...</td>\n",
       "      <td>[voting, voting mail, states voting, vote elec...</td>\n",
       "      <td>[Early voting., The early voting looks very go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>525</td>\n",
       "      <td>55</td>\n",
       "      <td>525_ballots_mail_envelope_number ballots</td>\n",
       "      <td>[ballots, mail, envelope, number ballots, requ...</td>\n",
       "      <td>[Historically, mail-in ballots have been a sig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>527 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Count                                               Name  \\\n",
       "0       -1  10936                     -1_trump_candidates_people_day   \n",
       "1        0      3                              michigan_donald trump   \n",
       "2        1      4                                    michigan_donald   \n",
       "3        2     13                                     michigan_trump   \n",
       "4        3      2                        elizabethtown_kamala harris   \n",
       "..     ...    ...                                                ...   \n",
       "522    521     14  521_person beliefs_mates person_brothers shop_...   \n",
       "523    522     14  522_people something_november vote_election tr...   \n",
       "524    523     41         523_vote vote_vote_vote people_people vote   \n",
       "525    524     33  524_voting_voting mail_states voting_vote elec...   \n",
       "526    525     55           525_ballots_mail_envelope_number ballots   \n",
       "\n",
       "                                        Representation  \\\n",
       "0    [trump, candidates, people, day, election, dem...   \n",
       "1    [ways party, dominance election, polls leads, ...   \n",
       "2    [michigan state, today michigan, look michigan...   \n",
       "3    [michigan, votes michigan, michigan michigan, ...   \n",
       "4    [counter women, areas past, past areas, harris...   \n",
       "..                                                 ...   \n",
       "522  [person beliefs, mates person, brothers shop, ...   \n",
       "523  [people something, november vote, election try...   \n",
       "524  [vote vote, vote, vote people, people vote, no...   \n",
       "525  [voting, voting mail, states voting, vote elec...   \n",
       "526  [ballots, mail, envelope, number ballots, requ...   \n",
       "\n",
       "                                   Representative_Docs  \n",
       "0    [But do you care more about the election now?,...  \n",
       "1    [And if we recall the 2016 election, it wasn't...  \n",
       "2    [And if you look at, let's just look at Michig...  \n",
       "3    [A one point swing in Wisconsin and Michigan m...  \n",
       "4    [In our Commonwealth, Kamala Harris just seems...  \n",
       "..                                                 ...  \n",
       "522  [You vote for a leader who sees you, who has a...  \n",
       "523  [And then once you've got a plan, then you've ...  \n",
       "524  [Put down your vote and do what?, Vote for vot...  \n",
       "525  [Early voting., The early voting looks very go...  \n",
       "526  [Historically, mail-in ballots have been a sig...  \n",
       "\n",
       "[527 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertopic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_9b701\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_9b701_level0_col0\" class=\"col_heading level0 col0\" >Presidential_Candidate</th>\n",
       "      <th id=\"T_9b701_level0_col1\" class=\"col_heading level0 col1\" >State</th>\n",
       "      <th id=\"T_9b701_level0_col2\" class=\"col_heading level0 col2\" >count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_9b701_row0_col0\" class=\"data row0 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_9b701_row0_col1\" class=\"data row0 col1\" >Arizona</td>\n",
       "      <td id=\"T_9b701_row0_col2\" class=\"data row0 col2\" >19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9b701_row1_col0\" class=\"data row1 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_9b701_row1_col1\" class=\"data row1 col1\" >Michigan</td>\n",
       "      <td id=\"T_9b701_row1_col2\" class=\"data row1 col2\" >17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9b701_row2_col0\" class=\"data row2 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_9b701_row2_col1\" class=\"data row2 col1\" >Pennsylvania</td>\n",
       "      <td id=\"T_9b701_row2_col2\" class=\"data row2 col2\" >1098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9b701_row3_col0\" class=\"data row3 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_9b701_row3_col1\" class=\"data row3 col1\" >Arizona</td>\n",
       "      <td id=\"T_9b701_row3_col2\" class=\"data row3 col2\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9b701_row4_col0\" class=\"data row4 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_9b701_row4_col1\" class=\"data row4 col1\" >Michigan</td>\n",
       "      <td id=\"T_9b701_row4_col2\" class=\"data row4 col2\" >16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9b701_row5_col0\" class=\"data row5 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_9b701_row5_col1\" class=\"data row5 col1\" >Pennsylvania</td>\n",
       "      <td id=\"T_9b701_row5_col2\" class=\"data row5 col2\" >665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x22fa3254ad0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sa tingin ko need natin 5k sentences minimum for Relevant Sentences di lang for gathered.\n",
    "Kasi mamaya 5k Random Sentences nakuha natin tas 100 lang dun Relevant with candidate & state.\n",
    "\n",
    "Ang naiisip ko since meron 6 Combinations = 3 candidate * 2 state\n",
    "Gawin natin 5000/6 = 834 Relevant Sentences required set natin as minimum per Combination\n",
    "\n",
    "Trump  - Arizona      = 834 Relevant Sentences\n",
    "Harris - Arizona      = 834 Relevant Sentences\n",
    "Trump  - Michigan     = 834 Relevant Sentences\n",
    "Harris - Michigan     = 834 Relevant Sentences\n",
    "Trump  - Pennsylvania = 834 Relevant Sentences\n",
    "Harris - Pennsylvania = 834 Relevant Sentences\n",
    "                     --------------------------\n",
    "                      ~5000 Relevant Sentences\n",
    "\"\"\"\n",
    "def print_statistics():\n",
    "    try:\n",
    "        return (\n",
    "            pd\n",
    "            .read_csv(relevant_transcript_sentences_filename, encoding_errors=\"ignore\")\n",
    "            .groupby([\"Presidential_Candidate\", \"State\"])\n",
    "            .size()\n",
    "            .reset_index(name=\"count\")\n",
    "            .style.hide(axis=\"index\")\n",
    "        )\n",
    "    except: return \"No Relevant Sentences\"\n",
    "        \n",
    "print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
