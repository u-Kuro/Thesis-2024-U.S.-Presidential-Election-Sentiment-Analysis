{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "import os, re, torch, nltk\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Additional Downloads\n",
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename: str) -> str:\n",
    "    # Escape Double Quotes\n",
    "    filename = filename.replace('\"', '\\\\\"')\n",
    "\n",
    "    # Replace Invalid Characters with \"_\"\n",
    "    invalid_chars = re.compile(r'[<>:\"/\\\\|?*]')\n",
    "    sanitized_filename = invalid_chars.sub(\"_\", filename)\n",
    "\n",
    "    return sanitized_filename\n",
    "    \n",
    "def read_unique_items_from_file(file: str) -> list:\n",
    "    with open(file, \"r\") as f:\n",
    "        return list(set(url.strip() for url in f.readlines() if url.strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# File Names\n",
    "transcript_sentences_filename = \"transcript_sentences.csv\"\n",
    "relevant_transcript_sentences_filename = \"relevant_transcript_sentences.csv\"\n",
    "\n",
    "# Folder Names\n",
    "transcription_output_path = \"Transcription\"\n",
    "cities_path = \"State Cities\"\n",
    "\n",
    "# Numeric Constants \n",
    "max_consecutive_words_for_topic = 2 # e.g. Unigram: \"Donald\" | Bigram: \"Donald Trump\"\n",
    "min_number_of_word_in_relevant_sentence = 5 # e.g. 5-words: \"This is a nice place\"\n",
    "min_similarity_of_topic_modeling = 0.7 # Allow Sentences 70% Similar to Candidate-State Combination\n",
    "\n",
    "# Sentence Categories\n",
    "presidential_candidates = {\n",
    "    \"Donald Trump\": [\n",
    "        \"Donald\", \"Trump\"\n",
    "    ],\n",
    "    \"Kamala Harris\": [\n",
    "        \"Kamala\", \"Harris\"\n",
    "    ]\n",
    "}\n",
    "state_cities = {\n",
    "    \"Michigan\": read_unique_items_from_file(os.path.join(cities_path, \"michigan-cities.txt\")),\n",
    "    \"Arizona\": read_unique_items_from_file(os.path.join(cities_path, \"arizona-cities.txt\")),\n",
    "    \"Pennsylvania\": read_unique_items_from_file(os.path.join(cities_path, \"pennsylvania-cities.txt\"))\n",
    "}\n",
    "\n",
    "# Words for Sentence Filtering\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Additional Preprocessing of Configurations\n",
    "presidential_candidates = {presidential_candidate: list(set(names)) for presidential_candidate, names in presidential_candidates.items()}\n",
    "presidential_candidates_and_states_combinations = [\n",
    "    f\"{pattern}_{loc}\".lower() for name, parts in presidential_candidates.items() \n",
    "    for loc in [state for state in state_cities] + [city for cities in state_cities.values() for city in cities]\n",
    "    for pattern in [name, '_'.join(parts)] + parts\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Extraction (Transcripts to CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4958c71498945a58e01f3d2aa0b6703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Sentences [0/269 Transcript]:   0%|          | 0/269 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences: 24575\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That's sacrifice.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clearly, her campaign is trying to shake stuff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She's smart She's effective She's white And un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And so after all these years, we know who Dona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>And this is worrisome for her campaign for obv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence\n",
       "0                                  That's sacrifice.\n",
       "1  Clearly, her campaign is trying to shake stuff...\n",
       "2  She's smart She's effective She's white And un...\n",
       "3  And so after all these years, we know who Dona...\n",
       "4  And this is worrisome for her campaign for obv..."
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_transcripts_into_csv_of_sentences() -> DataFrame:    \n",
    "    # Initialize List of Sentences\n",
    "    list_of_sentences = []\n",
    "        \n",
    "    # Collect List of Sentences from Transcription Files\n",
    "    transcription_files = os.listdir(transcription_output_path)\n",
    "    total_transcription_file = len(transcription_files)\n",
    "    with tqdm(total=total_transcription_file, desc=f'Collecting Sentences [0/{total_transcription_file} Transcript]') as pbar:\n",
    "        for index, filename in enumerate(transcription_files):\n",
    "            current = f'{index+1}/{total_transcription_file}'\n",
    "            if filename == \".ipynb_checkpoints\":\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            pbar.set_description(f'Collecting Sentences [{current} Transcript]')\n",
    "\n",
    "            # Open Transcription File\n",
    "            file_path = os.path.join(transcription_output_path, filename)\n",
    "            with open(file_path, \"r\") as file:\n",
    "                transcription = file.read()\n",
    "                \n",
    "                # Split Transcript into Sentences\n",
    "                sentences = sent_tokenize(transcription)\n",
    "\n",
    "                # Remove Consecutive Duplicates (Caused by Whisper)\n",
    "                sentences = [sentence for i, sentence in enumerate(sentences) if i == 0 or sentence != sentences[i-1]]\n",
    "                \n",
    "                # Add the Sentences\n",
    "                list_of_sentences.extend(sentences)\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "    # Save List of All Sentences into CSV file\n",
    "    df = pd.DataFrame(list(set(list_of_sentences)), columns=[\"Sentence\"])\n",
    "    df.to_csv(transcript_sentences_filename, index=False)\n",
    "    return df\n",
    "\n",
    "list_of_sentences = process_transcripts_into_csv_of_sentences()\n",
    "print(f'Number of Sentences: {len(list_of_sentences)}')\n",
    "list_of_sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic: Relevant Sentence Filtering (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 21:28:25,100 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14531437494e48a9adc6f7b7d37a932a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/768 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 21:29:27,009 - BERTopic - Embedding - Completed âœ“\n",
      "2024-10-21 21:29:27,009 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-10-21 21:29:40,386 - BERTopic - Dimensionality - Completed âœ“\n",
      "2024-10-21 21:29:40,389 - BERTopic - Zeroshot Step 1 - Finding documents that could be assigned to either one of the zero-shot topics\n",
      "2024-10-21 21:29:45,237 - BERTopic - Zeroshot Step 1 - Completed âœ“\n",
      "2024-10-21 21:29:55,437 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-10-21 21:29:56,615 - BERTopic - Cluster - Completed âœ“\n",
      "2024-10-21 21:29:56,615 - BERTopic - Zeroshot Step 2 - Combining topics from zero-shot topic modeling with topics from clustering...\n",
      "2024-10-21 21:29:56,682 - BERTopic - Zeroshot Step 2 - Completed âœ“\n",
      "2024-10-21 21:29:56,682 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-10-21 21:30:08,316 - BERTopic - Representation - Completed âœ“\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Relevant Sentences: 937\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Presidential_Candidate</th>\n",
       "      <th>State</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump putting in the work also in Penns...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[trump work, work pennsylvania, work, donald t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump is in the lead in Pennsylvania.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[leads pennsylvania, trump leads, pennsylvania...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is winning Pennsylvania.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[leads pennsylvania, trump leads, pennsylvania...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>But now Donald Trump leads in Pennsylvania.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[leads pennsylvania, trump leads, pennsylvania...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump is taking Pennsylvania.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[significance part, pennsylvania significance,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Presidential_Candidate  \\\n",
       "0  Donald Trump putting in the work also in Penns...           Donald Trump   \n",
       "1       Donald Trump is in the lead in Pennsylvania.           Donald Trump   \n",
       "2              Donald Trump is winning Pennsylvania.           Donald Trump   \n",
       "3        But now Donald Trump leads in Pennsylvania.           Donald Trump   \n",
       "4                      Trump is taking Pennsylvania.           Donald Trump   \n",
       "\n",
       "          State                                     Topic_Keywords  \n",
       "0  Pennsylvania  [trump work, work pennsylvania, work, donald t...  \n",
       "1  Pennsylvania  [leads pennsylvania, trump leads, pennsylvania...  \n",
       "2  Pennsylvania  [leads pennsylvania, trump leads, pennsylvania...  \n",
       "3  Pennsylvania  [leads pennsylvania, trump leads, pennsylvania...  \n",
       "4  Pennsylvania  [significance part, pennsylvania significance,...  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_relevant_sentences() -> tuple[DataFrame, BERTopic]:\n",
    "    # Get All Collected Sentences from Transcript\n",
    "    df = pd.read_csv(transcript_sentences_filename)\n",
    "    sentences = df[\"Sentence\"].tolist()\n",
    "    \n",
    "    # Set Filter for Words as Possible Topics\n",
    "    def filter_possible_topics(text: str) -> list:\n",
    "        \"\"\"\n",
    "        Filter Words If its a Possible Topic:\n",
    "            1) Only Nouns and Proper Nouns (e.g. Dollars, Currency)\n",
    "            2) No Stop Words (e.g. in, to)\n",
    "            3) No Generic Abstract Nouns (e.g. thing, stuff)\n",
    "            4) Minumum of Three Letter Words (e.g. USA)\n",
    "            5) Exclude Numbers\n",
    "        \"\"\"\n",
    "        \n",
    "        pos_tags = pos_tag(word_tokenize(text)) # POS Tagging\n",
    "        possible_topics = [\n",
    "            token.lower() for token, pos in pos_tags\n",
    "            if pos in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"] # Nouns / Proper Nouns\n",
    "            and token.lower() not in stop_words # Exclude Stop Words\n",
    "            and len(token) > 1 # Exclude One Letter Words (e.g. Included: Ox)\n",
    "            and not token.isnumeric() # Exclude Numbers\n",
    "        ]\n",
    "        \n",
    "        return possible_topics\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        ngram_range=(1, max_consecutive_words_for_topic),\n",
    "        tokenizer=filter_possible_topics\n",
    "    )\n",
    "\n",
    "    # Train BERTopic model\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        n_gram_range=(1, max_consecutive_words_for_topic),\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        zeroshot_topic_list=presidential_candidates_and_states_combinations,\n",
    "        zeroshot_min_similarity=topic_modeling_threshold,\n",
    "        verbose=True\n",
    "    )\n",
    "    topics, _ = topic_model.fit_transform(sentences)\n",
    "    \n",
    "    # Get BERTopic Results\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    topics_and_documents = pd.DataFrame({\"Topic\": topics, \"Representative_Docs\": sentences})\n",
    "    \n",
    "    # Initialize Lists for Relevant Sentences\n",
    "    list_of_relevant_sentences = []\n",
    "\n",
    "    def is_sentence_complete(sentence: str) -> bool:\n",
    "         # Exclude Sentence with Less than 5 or N Words\n",
    "        return len(word_tokenize(sentence)) < minimum_number_of_word_in_relevant_sentence\n",
    "        \n",
    "    # Get Relevant Sentences\n",
    "    for _, row in topic_info.iterrows():\n",
    "        topic = row[\"Topic\"]\n",
    "        if topic == -1: continue # Skip Outlier\n",
    "\n",
    "        # Get List of Relevant Topics and Sentences\n",
    "        topic_keywords = row[\"Representation\"]\n",
    "        relevant_sentences = topics_and_documents[topics_and_documents[\"Topic\"] == topic][\"Representative_Docs\"].tolist()\n",
    "        \n",
    "        # Check Candidate Mentions in Topics\n",
    "        presidential_candidate_mentions = set() # Avoid Duplicates\n",
    "        for presidential_candidate, names in presidential_candidates.items():\n",
    "            if (\n",
    "                any(name.lower() in keyword.lower() for name in names for keyword in topic_keywords) \n",
    "                or any(presidential_candidate.lower() in keyword.lower() for keyword in topic_keywords)\n",
    "            ): \n",
    "                presidential_candidate_mentions.add(presidential_candidate)\n",
    "        \n",
    "        # Make Sure Only 1 Candidate is Mentioned\n",
    "        if len(presidential_candidate_mentions) != 1: continue\n",
    "\n",
    "        # Check State Mentions in Topics (Including Cities)\n",
    "        state_mentions = set() # Avoid Duplicates\n",
    "        for state, cities in state_cities.items():\n",
    "            if (\n",
    "                any(city.lower() in keyword.lower() for city in cities for keyword in topic_keywords) \n",
    "                or any(state.lower() in keyword.lower() for keyword in topic_keywords)\n",
    "            ): \n",
    "                state_mentions.add(state)\n",
    "\n",
    "        # Make Sure Only 1 State is Mentioned\n",
    "        if len(state_mentions) != 1: continue\n",
    "        \"\"\"\n",
    "        Add Relevant Sentences Only If:\n",
    "            1) Only 1 Candidate is Mentioned\n",
    "            2) Only 1 State is Mentioned\n",
    "        \"\"\"\n",
    "        if (\n",
    "            len(presidential_candidate_mentions) == 1\n",
    "            and len(state_mentions) == 1\n",
    "        ):\n",
    "            presidential_candidate = presidential_candidate_mentions.pop()\n",
    "            state = state_mentions.pop()\n",
    "            \n",
    "            # Add All Relevant Sentences with their Corresponding Presidential Candidate, State, and Topic Keywords\n",
    "            for sentence in relevant_sentences:\n",
    "                # Filter Complete Sentence\n",
    "                if len(word_tokenize(sentence)) >= min_number_of_word_in_relevant_sentence:\n",
    "                    list_of_relevant_sentences.append({\n",
    "                        \"Sentence\": sentence,\n",
    "                        \"Presidential_Candidate\": presidential_candidate,\n",
    "                        \"State\": state,\n",
    "                        \"Topic_Keywords\": topic_keywords\n",
    "                    })\n",
    "    \n",
    "    # Save List of All Relevant Sentences into CSV file\n",
    "    df = pd.DataFrame(list_of_relevant_sentences)\n",
    "    df.to_csv(relevant_transcript_sentences_filename, index=False)\n",
    "    return df, topic_model\n",
    "\n",
    "list_of_relevant_sentences, bertopic_model = filter_relevant_sentences()\n",
    "print(f'Number of Relevant Sentences: {len(list_of_relevant_sentences)}')\n",
    "list_of_relevant_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>11417</td>\n",
       "      <td>-1_state_trump_harris_states</td>\n",
       "      <td>[state, trump, harris, states, pennsylvania, p...</td>\n",
       "      <td>[Kamala Harris is too exotic to flip Pennsylva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>trump_michigan</td>\n",
       "      <td>[trump way, president trump, way, president, t...</td>\n",
       "      <td>[President Trump is on his way to Michigan sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>kamala harris_washington</td>\n",
       "      <td>[harris rest, rest left, lives parents, lives ...</td>\n",
       "      <td>[Now you have Kamala Harris versus Trump., Kam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>kamala harris_nogales</td>\n",
       "      <td>[harris today, wins harris, today leads, today...</td>\n",
       "      <td>[Today, Kamala Harris leads by one., Ain't nob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>donald trump_pennsylvania</td>\n",
       "      <td>[trump work, work pennsylvania, work, donald t...</td>\n",
       "      <td>[Donald Trump putting in the work also in Penn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>327</td>\n",
       "      <td>38</td>\n",
       "      <td>327_question question_question_questions quest...</td>\n",
       "      <td>[question question, question, questions questi...</td>\n",
       "      <td>[Well, it's a great question., So I'm going to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>328</td>\n",
       "      <td>39</td>\n",
       "      <td>328_ah_ah ah_huh_gosh</td>\n",
       "      <td>[ah, ah ah, huh, gosh, goodness, god, ooh, son...</td>\n",
       "      <td>[Huh., Ah, ah, ah, ah., Ah hell yeah.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>329</td>\n",
       "      <td>11</td>\n",
       "      <td>329_pivot sir_man pivot_pivot_sir</td>\n",
       "      <td>[pivot sir, man pivot, pivot, sir, man, , , , , ]</td>\n",
       "      <td>[No, man., Yes, sir., Pivot to, yeah.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>330</td>\n",
       "      <td>22</td>\n",
       "      <td>330_move tomorrow_okay move_folks let_end right</td>\n",
       "      <td>[move tomorrow, okay move, folks let, end righ...</td>\n",
       "      <td>[Let's pause right here., Let's end on that., ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>331</td>\n",
       "      <td>36</td>\n",
       "      <td>331_let_let let_let something_look let</td>\n",
       "      <td>[let, let let, let something, look let, quick ...</td>\n",
       "      <td>[Let's see what else is going on here., Let's ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>333 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Count                                               Name  \\\n",
       "0       -1  11417                       -1_state_trump_harris_states   \n",
       "1        0      1                                     trump_michigan   \n",
       "2        1      5                           kamala harris_washington   \n",
       "3        2     10                              kamala harris_nogales   \n",
       "4        3      1                          donald trump_pennsylvania   \n",
       "..     ...    ...                                                ...   \n",
       "328    327     38  327_question question_question_questions quest...   \n",
       "329    328     39                              328_ah_ah ah_huh_gosh   \n",
       "330    329     11                  329_pivot sir_man pivot_pivot_sir   \n",
       "331    330     22    330_move tomorrow_okay move_folks let_end right   \n",
       "332    331     36             331_let_let let_let something_look let   \n",
       "\n",
       "                                        Representation  \\\n",
       "0    [state, trump, harris, states, pennsylvania, p...   \n",
       "1    [trump way, president trump, way, president, t...   \n",
       "2    [harris rest, rest left, lives parents, lives ...   \n",
       "3    [harris today, wins harris, today leads, today...   \n",
       "4    [trump work, work pennsylvania, work, donald t...   \n",
       "..                                                 ...   \n",
       "328  [question question, question, questions questi...   \n",
       "329  [ah, ah ah, huh, gosh, goodness, god, ooh, son...   \n",
       "330  [pivot sir, man pivot, pivot, sir, man, , , , , ]   \n",
       "331  [move tomorrow, okay move, folks let, end righ...   \n",
       "332  [let, let let, let something, look let, quick ...   \n",
       "\n",
       "                                   Representative_Docs  \n",
       "0    [Kamala Harris is too exotic to flip Pennsylva...  \n",
       "1    [President Trump is on his way to Michigan sho...  \n",
       "2    [Now you have Kamala Harris versus Trump., Kam...  \n",
       "3    [Today, Kamala Harris leads by one., Ain't nob...  \n",
       "4    [Donald Trump putting in the work also in Penn...  \n",
       "..                                                 ...  \n",
       "328  [Well, it's a great question., So I'm going to...  \n",
       "329             [Huh., Ah, ah, ah, ah., Ah hell yeah.]  \n",
       "330             [No, man., Yes, sir., Pivot to, yeah.]  \n",
       "331  [Let's pause right here., Let's end on that., ...  \n",
       "332  [Let's see what else is going on here., Let's ...  \n",
       "\n",
       "[333 rows x 5 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertopic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_8e902\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_8e902_level0_col0\" class=\"col_heading level0 col0\" >Presidential_Candidate</th>\n",
       "      <th id=\"T_8e902_level0_col1\" class=\"col_heading level0 col1\" >State</th>\n",
       "      <th id=\"T_8e902_level0_col2\" class=\"col_heading level0 col2\" >count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_8e902_row0_col0\" class=\"data row0 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_8e902_row0_col1\" class=\"data row0 col1\" >Michigan</td>\n",
       "      <td id=\"T_8e902_row0_col2\" class=\"data row0 col2\" >66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_8e902_row1_col0\" class=\"data row1 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_8e902_row1_col1\" class=\"data row1 col1\" >Pennsylvania</td>\n",
       "      <td id=\"T_8e902_row1_col2\" class=\"data row1 col2\" >734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_8e902_row2_col0\" class=\"data row2 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_8e902_row2_col1\" class=\"data row2 col1\" >Pennsylvania</td>\n",
       "      <td id=\"T_8e902_row2_col2\" class=\"data row2 col2\" >137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1a729bb2a10>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sa tingin ko need natin 5k sentences minimum for Relevant Sentences di lang for gathered.\n",
    "Kasi mamaya 5k Random Sentences nakuha natin tas 100 lang dun Relevant with candidate & state.\n",
    "\n",
    "Ang naiisip ko since 6 Combination = 3 candidate * 2 state\n",
    "Gawin natin 5000/6 = 834 Relevant Sentences required set natin as minimum per Combination\n",
    "\n",
    "Trump  - Arizona      = 834 Relevant Sentences\n",
    "Harris - Arizona      = 834 Relevant Sentences\n",
    "Trump  - Michigan     = 834 Relevant Sentences\n",
    "Harris - Michigan     = 834 Relevant Sentences\n",
    "Trump  - Pennsylvania = 834 Relevant Sentences\n",
    "Harris - Pennsylvania = 834 Relevant Sentences\n",
    "                     --------------------------\n",
    "                      ~5000 Relevant Sentences\n",
    "\"\"\"\n",
    "def print_statistics():\n",
    "    try:\n",
    "        return (\n",
    "            pd\n",
    "            .read_csv(relevant_transcript_sentences_filename)\n",
    "            .groupby([\"Presidential_Candidate\", \"State\"])\n",
    "            .size()\n",
    "            .reset_index(name=\"count\")\n",
    "            .style.hide(axis=\"index\")\n",
    "        )\n",
    "    except: return \"No Relevant Sentences\"\n",
    "        \n",
    "print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
