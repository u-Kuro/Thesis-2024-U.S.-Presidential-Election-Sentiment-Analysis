{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\---\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "import os, re, torch, nltk\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Additional Downloads\n",
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename: str) -> str:\n",
    "    # Escape Double Quotes\n",
    "    filename = filename.replace('\"', '\\\\\"')\n",
    "\n",
    "    # Replace Invalid Characters with \"_\"\n",
    "    invalid_chars = re.compile(r'[<>:\"/\\\\|?*]')\n",
    "    sanitized_filename = invalid_chars.sub(\"_\", filename)\n",
    "\n",
    "    return sanitized_filename\n",
    "    \n",
    "def read_unique_items_from_file(file: str) -> list:\n",
    "    if os.path.exists(file):\n",
    "        with open(file, \"r\") as f:\n",
    "            return list(set(e.strip() for e in f.readlines() if e.strip()))\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# File Names\n",
    "transcript_sentences_filename = \"transcript_sentences.csv\"\n",
    "relevant_transcript_sentences_filename = \"relevant_transcript_sentences.csv\"\n",
    "\n",
    "# Folder Names\n",
    "transcription_output_path = \"Transcription\"\n",
    "cities_path = \"State Cities\"\n",
    "\n",
    "# Numeric Constants \n",
    "max_pair_of_words_for_topic = 2 # e.g. Unigram: \"Donald\" | Bigram: \"Donald Trump\"\n",
    "min_number_of_word_in_relevant_sentence = 5 # Only Accepts 5-Word Sentence as Relevant | e.g. \"This is a nice place\"\n",
    "min_similarity_of_topic_modeling = 0.1 # Range:[0.1, 1] | Minimum Similarity for Topic Assignment | Higher Value Means Stricter Match\n",
    "\n",
    "# Sentence Categories\n",
    "presidential_candidates = {\n",
    "    \"Donald Trump\": [\n",
    "        \"Donald\", \"Trump\"\n",
    "    ],\n",
    "    \"Kamala Harris\": [\n",
    "        \"Kamala\", \"Harris\"\n",
    "    ]\n",
    "}\n",
    "state_cities = {\n",
    "    \"Michigan\": read_unique_items_from_file(os.path.join(cities_path, \"michigan-cities.txt\")),\n",
    "    \"Arizona\": read_unique_items_from_file(os.path.join(cities_path, \"arizona-cities.txt\")),\n",
    "    \"Pennsylvania\": read_unique_items_from_file(os.path.join(cities_path, \"pennsylvania-cities.txt\"))\n",
    "}\n",
    "\n",
    "# Words for Sentence Filtering\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Additional Preprocessing of Configurations\n",
    "presidential_candidates = {presidential_candidate: list(set(names)) for presidential_candidate, names in presidential_candidates.items()}\n",
    "presidential_candidates_and_states_combinations = [\n",
    "    f\"{location}_{name}\".lower()\n",
    "    for full_name, names in presidential_candidates.items() \n",
    "    for location in [state for state in state_cities] + [city for cities in state_cities.values() for city in cities]\n",
    "    for name in [full_name] + names\n",
    "]\n",
    "presidential_candidates_and_states_combinations_in_2d = [\n",
    "    [location.lower(), full_name.lower()] + [name.lower() for name in names]\n",
    "    for full_name, names in presidential_candidates.items() \n",
    "    for location in [state for state in state_cities] + [city for cities in state_cities.values() for city in cities]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Extraction (Transcripts to CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9675a67b481d46ffa46eb53fdb448d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Sentences [0/608 Transcript]:   0%|          | 0/608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences: 50221\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He could win by 21 but still like to say that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clinton did manage to hold Minnesota by a poin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Do you think it changed the race?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And on Zoom, we are joined by Republican consu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How you doing, Danielle?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence\n",
       "0  He could win by 21 but still like to say that ...\n",
       "1  Clinton did manage to hold Minnesota by a poin...\n",
       "2                  Do you think it changed the race?\n",
       "3  And on Zoom, we are joined by Republican consu...\n",
       "4                           How you doing, Danielle?"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_transcripts_into_csv_of_sentences() -> DataFrame:    \n",
    "    # Initialize List of Sentences\n",
    "    list_of_sentences = []\n",
    "        \n",
    "    # Collect List of Sentences from Transcription Files\n",
    "    transcription_files = os.listdir(transcription_output_path)\n",
    "    total_transcription_file = len(transcription_files)\n",
    "    with tqdm(total=total_transcription_file, desc=f'Collecting Sentences [0/{total_transcription_file} Transcript]') as pbar:\n",
    "        for index, filename in enumerate(transcription_files):\n",
    "            current = f'{index+1}/{total_transcription_file}'\n",
    "            if filename == \".ipynb_checkpoints\":\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            pbar.set_description(f'Collecting Sentences [{current} Transcript]')\n",
    "\n",
    "            # Open Transcription File\n",
    "            file_path = os.path.join(transcription_output_path, filename)\n",
    "            with open(file_path, \"r\") as file:\n",
    "                transcription = file.read()\n",
    "                \n",
    "                # Split Transcript into Sentences\n",
    "                sentences = sent_tokenize(transcription)\n",
    "\n",
    "                # Remove Consecutive Duplicates (Caused by Whisper)\n",
    "                sentences = [sentence for i, sentence in enumerate(sentences) if i == 0 or sentence != sentences[i-1]]\n",
    "                \n",
    "                # Add the Sentences\n",
    "                list_of_sentences.extend(sentences)\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "    # Save List of All Sentences into CSV file\n",
    "    df = pd.DataFrame(list(set(list_of_sentences)), columns=[\"Sentence\"])\n",
    "    df.to_csv(transcript_sentences_filename, index=False, errors=\"ignore\")\n",
    "    return df\n",
    "\n",
    "list_of_sentences = process_transcripts_into_csv_of_sentences()\n",
    "print(f'Number of Sentences: {len(list_of_sentences)}')\n",
    "list_of_sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic: Relevant Sentence Filtering (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 00:35:00,385 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05837274db584c42a811e991bbbb7fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 00:37:15,782 - BERTopic - Embedding - Completed ✓\n",
      "2024-10-30 00:37:15,782 - BERTopic - Guided - Find embeddings highly related to seeded topics.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4514d0b34bb4431a918e13f962965145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 00:37:20,040 - BERTopic - Guided - Completed ✓\n",
      "2024-10-30 00:37:20,050 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-10-30 00:38:19,619 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-10-30 00:38:19,620 - BERTopic - Zeroshot Step 1 - Finding documents that could be assigned to either one of the zero-shot topics\n",
      "2024-10-30 00:38:23,500 - BERTopic - Zeroshot Step 1 - Completed ✓\n",
      "2024-10-30 00:38:48,641 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-10-30 00:38:48,654 - BERTopic - Cluster - Completed ✓\n",
      "2024-10-30 00:38:48,660 - BERTopic - Zeroshot Step 2 - Combining topics from zero-shot topic modeling with topics from clustering...\n",
      "2024-10-30 00:38:48,782 - BERTopic - Zeroshot Step 2 - Completed ✓\n",
      "2024-10-30 00:38:48,785 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-10-30 00:39:22,030 - BERTopic - Representation - Completed ✓\n",
      "2024-10-30 00:39:22,031 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-10-30 00:39:22,035 - BERTopic - Topic reduction - Reduced number of topics from 2243 to 2243\n"
     ]
    }
   ],
   "source": [
    "def filter_relevant_sentences() -> tuple[DataFrame, BERTopic]:\n",
    "    # Get All Collected Sentences from Transcript\n",
    "    df = pd.read_csv(transcript_sentences_filename, encoding_errors=\"ignore\")\n",
    "    sentences = df[\"Sentence\"].tolist()\n",
    "    \n",
    "    # Set Filter for Words as Possible Topics\n",
    "    def filter_possible_topics(text: str) -> list:\n",
    "        \"\"\"\n",
    "        Filter Words If its a Possible Topic:\n",
    "            1) Only Nouns and Proper Nouns (e.g. Dollars, Currency)\n",
    "            2) No Stop Words (e.g. in, to)\n",
    "            3) Minumum of Two Letter Words (e.g. Ox)\n",
    "            4) Exclude Numbers\n",
    "        \"\"\"\n",
    "        \n",
    "        pos_tags = pos_tag(word_tokenize(text)) # POS Tagging\n",
    "        possible_topics = [\n",
    "            token.lower() for token, pos in pos_tags\n",
    "            if pos in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"] # Nouns / Proper Nouns\n",
    "            and token.lower() not in stop_words # Exclude Stop Words\n",
    "            and len(token) > 1 # Exclude One Letter Words (e.g. Included: Ox)\n",
    "            and not token.isnumeric() # Exclude Numbers\n",
    "        ]\n",
    "        \n",
    "        return possible_topics\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        ngram_range=(1, max_pair_of_words_for_topic),\n",
    "        tokenizer=filter_possible_topics\n",
    "    )\n",
    "\n",
    "    # Train BERTopic model\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        n_gram_range=(1, max_pair_of_words_for_topic),\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        seed_topic_list=presidential_candidates_and_states_combinations_in_2d,\n",
    "        zeroshot_topic_list=presidential_candidates_and_states_combinations,\n",
    "        zeroshot_min_similarity=min_similarity_of_topic_modeling,\n",
    "        nr_topics=len(presidential_candidates_and_states_combinations),\n",
    "        verbose=True\n",
    "    )\n",
    "    topic_ids, _ = topic_model.fit_transform(sentences)\n",
    "    \n",
    "    # Get BERTopic Results\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    topics_and_documents = pd.DataFrame({\"Topic\": topic_ids, \"Representative_Docs\": sentences})\n",
    "    \n",
    "    # Initialize Lists for Relevant Sentences\n",
    "    list_of_relevant_sentences = []\n",
    "\n",
    "    def is_sentence_complete(sentence: str) -> bool:\n",
    "        # Exclude Sentence with Less than 5 or N Words\n",
    "        return len(word_tokenize(sentence)) >= min_number_of_word_in_relevant_sentence\n",
    "        \n",
    "    # Get Relevant Sentences\n",
    "    for _, row in topic_info.iterrows():\n",
    "        topic_id = row[\"Topic\"]\n",
    "        if topic_id == -1: continue # Skip Outlier\n",
    "    \n",
    "        # Get List of Relevant Topics and Sentences\n",
    "        topic_ngramed_keywords = [\n",
    "            ngramed_keyword \n",
    "            for ngramed_keyword in row[\"Representation\"]\n",
    "            if ngramed_keyword\n",
    "        ]\n",
    "        relevant_sentences = topics_and_documents[topics_and_documents[\"Topic\"] == topic_id][\"Representative_Docs\"].tolist()\n",
    "        \n",
    "        # Check Candidate Mentions in Topics\n",
    "        presidential_candidate_mentions = set() # Avoid Duplicates\n",
    "        for presidential_candidate, names in presidential_candidates.items():\n",
    "            if (\n",
    "                any(\n",
    "                    word and presidential_candidate\n",
    "                    and presidential_candidate.strip().lower() == word.strip().lower()\n",
    "                    for ngramed_keyword in topic_ngramed_keywords\n",
    "                    for word in ngramed_keyword.split(\" \")\n",
    "                )\n",
    "                or any(\n",
    "                    word and name\n",
    "                    and name.strip().lower() == word.strip().lower()\n",
    "                    for name in names\n",
    "                    for ngramed_keyword in topic_ngramed_keywords\n",
    "                    for word in ngramed_keyword.split(\" \")\n",
    "                )\n",
    "            ): \n",
    "                presidential_candidate_mentions.add(presidential_candidate)\n",
    "        \n",
    "        # Make Sure Only 1 Candidate is Mentioned\n",
    "        if len(presidential_candidate_mentions) != 1: continue\n",
    "\n",
    "        # Check State Mentions in Topics (Including Cities)\n",
    "        state_mentions = set() # Avoid Duplicates\n",
    "        for state, cities in state_cities.items():\n",
    "            if (\n",
    "                any(\n",
    "                    word and state\n",
    "                    and state.strip().lower() == word.strip().lower()\n",
    "                    for ngramed_keyword in topic_ngramed_keywords\n",
    "                    for word in ngramed_keyword.split(\" \")\n",
    "                )\n",
    "                or any(\n",
    "                    word and city\n",
    "                    and city.strip().lower() == word.strip().lower()\n",
    "                    for city in cities\n",
    "                    for ngramed_keyword in topic_ngramed_keywords\n",
    "                    for word in ngramed_keyword.split(\" \")\n",
    "                )\n",
    "            ): \n",
    "                state_mentions.add(state)\n",
    "\n",
    "        # Make Sure Only 1 State is Mentioned\n",
    "        if len(state_mentions) != 1: continue\n",
    "        \"\"\"\n",
    "        Add Relevant Sentences Only If:\n",
    "            1) Only 1 Candidate is Mentioned in the Topic\n",
    "            2) Only 1 State is Mentioned in the Topic\n",
    "        \"\"\"\n",
    "        if (\n",
    "            len(presidential_candidate_mentions) == 1\n",
    "            and len(state_mentions) == 1\n",
    "        ):\n",
    "            presidential_candidate = presidential_candidate_mentions.pop()\n",
    "            state = state_mentions.pop()\n",
    "            \n",
    "            # Add All Relevant Sentences with their Corresponding Presidential Candidate, State, and Topic Keywords\n",
    "            for sentence in relevant_sentences:\n",
    "                # Filter Complete Sentence with Word Count >= 5 or N\n",
    "                if is_sentence_complete(sentence):\n",
    "                    list_of_relevant_sentences.append({\n",
    "                        \"Sentence\": sentence,\n",
    "                        \"Presidential_Candidate\": presidential_candidate,\n",
    "                        \"State\": state,\n",
    "                        \"Topic_Keywords\": topic_ngramed_keywords\n",
    "                    })\n",
    "    \n",
    "    # Save List of All Relevant Sentences into CSV file\n",
    "    df = pd.DataFrame(list_of_relevant_sentences)\n",
    "    df.to_csv(relevant_transcript_sentences_filename, index=False, errors=\"ignore\")\n",
    "    return df, topic_model\n",
    "\n",
    "list_of_relevant_sentences, bertopic_model = filter_relevant_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>905</td>\n",
       "      <td>pennsylvania_trump</td>\n",
       "      <td>[pennsylvania, trump pennsylvania, pennsylvani...</td>\n",
       "      <td>[Trump is taking Pennsylvania., So technically...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>163</td>\n",
       "      <td>856</td>\n",
       "      <td>ishpeming_trump</td>\n",
       "      <td>[trump democrats, problem trump, democracy, po...</td>\n",
       "      <td>[We use his own words a lot of the time to rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>292</td>\n",
       "      <td>819</td>\n",
       "      <td>belding_trump</td>\n",
       "      <td>[argument, reality, bullshit, evidence, truth,...</td>\n",
       "      <td>[Sources say Trump meanwhile has had less form...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>1728</td>\n",
       "      <td>800</td>\n",
       "      <td>coolidge_kamala harris</td>\n",
       "      <td>[clinton, joe biden, joe, democrat, clinton ba...</td>\n",
       "      <td>[Another formerly reliably Republican state, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>741</td>\n",
       "      <td>michigan_trump</td>\n",
       "      <td>[michigan, state michigan, michigan michigan, ...</td>\n",
       "      <td>[This is probably more probable than winning M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Topic  Count                    Name  \\\n",
       "8         7    905      pennsylvania_trump   \n",
       "164     163    856         ishpeming_trump   \n",
       "293     292    819           belding_trump   \n",
       "1729   1728    800  coolidge_kamala harris   \n",
       "2         1    741          michigan_trump   \n",
       "\n",
       "                                         Representation  \\\n",
       "8     [pennsylvania, trump pennsylvania, pennsylvani...   \n",
       "164   [trump democrats, problem trump, democracy, po...   \n",
       "293   [argument, reality, bullshit, evidence, truth,...   \n",
       "1729  [clinton, joe biden, joe, democrat, clinton ba...   \n",
       "2     [michigan, state michigan, michigan michigan, ...   \n",
       "\n",
       "                                    Representative_Docs  \n",
       "8     [Trump is taking Pennsylvania., So technically...  \n",
       "164   [We use his own words a lot of the time to rem...  \n",
       "293   [Sources say Trump meanwhile has had less form...  \n",
       "1729  [Another formerly reliably Republican state, B...  \n",
       "2     [This is probably more probable than winning M...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertopic_model.get_topic_info().sort_values(by='Count', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Presidential_Candidate</th>\n",
       "      <th>State</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Now Trump won Michigan back in 2016, but lost ...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>[michigan trump, trump michigan, state michiga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With the state's primary election just a few ...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>[michigan trump, trump michigan, state michiga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michigan had it tied between Trump and Harris,...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>[michigan trump, trump michigan, state michiga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Additionally, Sky News reports that Arab Ameri...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>[michigan trump, trump michigan, state michiga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump has taken an unusual tack in Michigan la...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>[michigan trump, trump michigan, state michiga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Presidential_Candidate  \\\n",
       "0  Now Trump won Michigan back in 2016, but lost ...           Donald Trump   \n",
       "1   With the state's primary election just a few ...           Donald Trump   \n",
       "2  Michigan had it tied between Trump and Harris,...           Donald Trump   \n",
       "3  Additionally, Sky News reports that Arab Ameri...           Donald Trump   \n",
       "4  Trump has taken an unusual tack in Michigan la...           Donald Trump   \n",
       "\n",
       "      State                                     Topic_Keywords  \n",
       "0  Michigan  [michigan trump, trump michigan, state michiga...  \n",
       "1  Michigan  [michigan trump, trump michigan, state michiga...  \n",
       "2  Michigan  [michigan trump, trump michigan, state michiga...  \n",
       "3  Michigan  [michigan trump, trump michigan, state michiga...  \n",
       "4  Michigan  [michigan trump, trump michigan, state michiga...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_relevant_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_4f4e7\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_4f4e7_level0_col0\" class=\"col_heading level0 col0\" >Presidential_Candidate</th>\n",
       "      <th id=\"T_4f4e7_level0_col1\" class=\"col_heading level0 col1\" >State</th>\n",
       "      <th id=\"T_4f4e7_level0_col2\" class=\"col_heading level0 col2\" >count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_4f4e7_row0_col0\" class=\"data row0 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_4f4e7_row0_col1\" class=\"data row0 col1\" >Arizona</td>\n",
       "      <td id=\"T_4f4e7_row0_col2\" class=\"data row0 col2\" >678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4f4e7_row1_col0\" class=\"data row1 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_4f4e7_row1_col1\" class=\"data row1 col1\" >Michigan</td>\n",
       "      <td id=\"T_4f4e7_row1_col2\" class=\"data row1 col2\" >414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4f4e7_row2_col0\" class=\"data row2 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_4f4e7_row2_col1\" class=\"data row2 col1\" >Pennsylvania</td>\n",
       "      <td id=\"T_4f4e7_row2_col2\" class=\"data row2 col2\" >1831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4f4e7_row3_col0\" class=\"data row3 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_4f4e7_row3_col1\" class=\"data row3 col1\" >Arizona</td>\n",
       "      <td id=\"T_4f4e7_row3_col2\" class=\"data row3 col2\" >325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4f4e7_row4_col0\" class=\"data row4 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_4f4e7_row4_col1\" class=\"data row4 col1\" >Michigan</td>\n",
       "      <td id=\"T_4f4e7_row4_col2\" class=\"data row4 col2\" >64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4f4e7_row5_col0\" class=\"data row5 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_4f4e7_row5_col1\" class=\"data row5 col1\" >Pennsylvania</td>\n",
       "      <td id=\"T_4f4e7_row5_col2\" class=\"data row5 col2\" >1009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4f4e7_row6_col0\" class=\"data row6 col0\" ></td>\n",
       "      <td id=\"T_4f4e7_row6_col1\" class=\"data row6 col1\" >Total</td>\n",
       "      <td id=\"T_4f4e7_row6_col2\" class=\"data row6 col2\" >4321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x22981af7fd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sa tingin ko need natin 5k sentences minimum for Relevant Sentences di lang for gathered.\n",
    "Kasi mamaya 5k Random Sentences nakuha natin tas 100 lang dun Relevant with candidate & state.\n",
    "\n",
    "Ang naiisip ko since meron 6 Combinations = 3 candidate * 2 state\n",
    "Gawin natin 5000/6 = 834 Relevant Sentences required set natin as minimum per Combination\n",
    "\n",
    "Trump  - Arizona      = 834 Relevant Sentences\n",
    "Harris - Arizona      = 834 Relevant Sentences\n",
    "Trump  - Michigan     = 834 Relevant Sentences\n",
    "Harris - Michigan     = 834 Relevant Sentences\n",
    "Trump  - Pennsylvania = 834 Relevant Sentences\n",
    "Harris - Pennsylvania = 834 Relevant Sentences\n",
    "                     --------------------------\n",
    "                      ~5000 Relevant Sentences\n",
    "\"\"\"\n",
    "def print_statistics():\n",
    "    try:\n",
    "        grouped_df = (\n",
    "            list_of_relevant_sentences\n",
    "            .groupby([\"Presidential_Candidate\", \"State\"])\n",
    "            .size()\n",
    "            .reset_index(name=\"count\")\n",
    "        )\n",
    "        total_count = grouped_df[\"count\"].sum()\n",
    "        total_row = pd.DataFrame({\"Presidential_Candidate\": [\"\"], \"State\": [\"Total\"], \"count\": [total_count]})\n",
    "        grouped_df = pd.concat([grouped_df, total_row], ignore_index=True)\n",
    "        return grouped_df.style.hide(axis=\"index\")\n",
    "    except: return \"No Relevant Sentences\"\n",
    "\n",
    "print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
