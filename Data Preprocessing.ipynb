{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T00:09:37.046049Z",
     "start_time": "2024-11-09T00:09:35.691753Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\---\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "import os, re, nltk\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Additional Downloads\n",
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T00:09:37.064679Z",
     "start_time": "2024-11-09T00:09:37.051553Z"
    }
   },
   "outputs": [],
   "source": [
    "def sanitize_filename(filename: str) -> str:\n",
    "    # Escape Double Quotes\n",
    "    filename = filename.replace('\"', '\\\\\"')\n",
    "\n",
    "    # Replace Invalid Characters with \"_\"\n",
    "    invalid_chars = re.compile(r'[<>:\"/\\\\|?*]')\n",
    "    sanitized_filename = invalid_chars.sub(\"_\", filename)\n",
    "\n",
    "    return sanitized_filename\n",
    "    \n",
    "def read_unique_items_from_file(file: str) -> list:\n",
    "    if os.path.exists(file):\n",
    "        with open(file, \"r\", errors=\"ignore\") as f:\n",
    "            return list(set(e.strip() for e in f.readlines() if e.strip()))\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T00:17:16.273357Z",
     "start_time": "2024-11-09T00:17:16.252982Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# File Names\n",
    "transcript_sentences_filename = \"transcript_sentences.csv\"\n",
    "relevant_transcript_sentences_filename = \"relevant_transcript_sentences.csv\"\n",
    "\n",
    "# Folder Names\n",
    "transcription_path = \"Transcription\"\n",
    "cities_transcription_paths = {\n",
    "    \"Michigan\": os.path.join(transcription_path, \"Michigan\"),\n",
    "    \"Arizona\": os.path.join(transcription_path, \"Arizona\"),\n",
    "    \"Pennsylvania\": os.path.join(transcription_path, \"Pennsylvania\"),\n",
    "}\n",
    "cities_path = \"State Cities\"\n",
    "\n",
    "# Numeric Constants \n",
    "max_pair_of_words_for_topic = 2\n",
    "\"\"\"\n",
    "    > Maximum words to consider for topic extraction\n",
    "        1: Unigram (e.g., \"Donald\")\n",
    "        2: Bigram (e.g., \"Donald Trump\")\n",
    "\"\"\"\n",
    "\n",
    "min_number_of_word_in_relevant_sentence = 5\n",
    "\"\"\"\n",
    "    > Minimum word count required for a sentence to be considered relevant\n",
    "    Example: \"This is a nice place\" = 5 words\n",
    "\"\"\"\n",
    "\n",
    "min_similarity_of_topic_modeling = 0.7\n",
    "\"\"\"\n",
    "    > Minimum similarity threshold for topic matching\n",
    "        Range: [0.1, 1.0]\n",
    "    Note: Higher values require closer matches\n",
    "    Example: 0.7 = 70% similarity required\n",
    "\"\"\"\n",
    "\n",
    "max_topic_count = None\n",
    "\"\"\"\n",
    "    Topic count limiter for dimensionality reduction\n",
    "        None: No reduction, keep all discovered topics\n",
    "        \"auto\": Automatically Reduces Topic Count\n",
    "        Number: Force reduce to specified number of topics\n",
    "    Note: (1) Using Number for Numeric reduction may merge unrelated topics together\n",
    "          (2) Lower Number may increase precision but risk missing relevant topics\n",
    "\"\"\"\n",
    "\n",
    "# Sentence Categories\n",
    "presidential_candidates = {\n",
    "    \"Donald Trump\": [\n",
    "        \"Donald\", \"Trump\",\n",
    "        \"Trump Donald\", \"Donald John\", \"John Trump\",\n",
    "        \"Donald J\", \"J. Donald\", \"J. Trump\", \"Trump J\",\n",
    "        \"Trump D\", \"D. Trump\", \"John D\", \"D. John\",\n",
    "        \"Donald T\", \"T. Donald\", \"John T\", \"T. John\",\n",
    "        \"Donald John Trump\", \"Donald J Trump\", \"D. J. Trump\", \n",
    "        \"President Donald\", \"President Trump\",\n",
    "        \"President Donald Trump\"\n",
    "    ],\n",
    "    \"Kamala Harris\": [\n",
    "        \"Kamala\", \"Harris\",\n",
    "        \"Harris Kamala\", \"Kamala Devi\", \"Devi Harris\",\n",
    "        \"Kamala D\", \"D. Kamala\", \"D. Harris\", \"Harris D\",\n",
    "        \"Harris K\", \"K. Harris\", \"Devi K\", \"K. Devi\",\n",
    "        \"Kamala H\", \"H. Kamala\", \"Devi H\", \"H. Devi\",\n",
    "        \"Kamala Devi Harris\", \"Kamala D Harris\", \"K. D. Harris\",  \n",
    "        \"President Kamala\", \"President Harris\",\n",
    "        \"President Kamala Harris\"\n",
    "    ]\n",
    "}\n",
    "original_state_cities = [\"Arizona\", \"Michigan\", \"Pennsylvania\"]\n",
    "state_cities = {\n",
    "    \"Arizona\": read_unique_items_from_file(os.path.join(cities_path, \"arizona-cities.txt\")),\n",
    "    \"Michigan\": read_unique_items_from_file(os.path.join(cities_path, \"michigan-cities.txt\")),\n",
    "    \"Pennsylvania\": read_unique_items_from_file(os.path.join(cities_path, \"pennsylvania-cities.txt\")),\n",
    "    \"Alabama\": [\"AL\", \"A.L\"],\n",
    "    \"Alaska\": [\"AK\", \"A.K\"],\n",
    "    \"Arkansas\": [\"AR\", \"A.R\"],\n",
    "    \"California\": [\"CA\", \"C.A\"],\n",
    "    \"Colorado\": [\"CO\", \"C.O\"],\n",
    "    \"Connecticut\": [\"CT\", \"C.T\"],\n",
    "    \"Delaware\": [\"DE\", \"D.E\"],\n",
    "    \"Florida\": [\"FL\", \"F.L\"],\n",
    "    \"Georgia\": [\"GA\", \"G.A\"],\n",
    "    \"Hawaii\": [\"HI\", \"H.I\"],\n",
    "    \"Idaho\": [\"ID\", \"I.D\"],\n",
    "    \"Illinois\": [\"IL\", \"I.L\"],\n",
    "    \"Indiana\": [\"IN\", \"I.N\"],\n",
    "    \"Iowa\": [\"IA\", \"I.A\"],\n",
    "    \"Kansas\": [\"KS\", \"K.S\"],\n",
    "    \"Kentucky\": [\"KY\", \"K.Y\"],\n",
    "    \"Louisiana\": [\"LA\", \"L.A\"],\n",
    "    \"Maine\": [\"ME\", \"M.E\"],\n",
    "    \"Maryland\": [\"MD\", \"M.D\"],\n",
    "    \"Massachusetts\": [\"MA\", \"M.A\"],\n",
    "    \"Minnesota\": [\"MN\", \"M.N\"],\n",
    "    \"Mississippi\": [\"MS\", \"M.S\"],\n",
    "    \"Missouri\": [\"MO\", \"M.O\"],\n",
    "    \"Montana\": [\"MT\", \"M.T\"],\n",
    "    \"Nebraska\": [\"NE\", \"N.E\"],\n",
    "    \"Nevada\": [\"NV\", \"N.V\"],\n",
    "    \"New Hampshire\": [\"NH\", \"N.H\"],\n",
    "    \"New Jersey\": [\"NJ\", \"N.J\"],\n",
    "    \"New Mexico\": [\"NM\", \"N.M\"],\n",
    "    \"New York\": [\"NY\", \"N.Y\"],\n",
    "    \"North Carolina\": [\"NC\", \"N.C\"],\n",
    "    \"North Dakota\": [\"ND\", \"N.D\"],\n",
    "    \"Ohio\": [\"OH\", \"O.H\"],\n",
    "    \"Oklahoma\": [\"OK\", \"O.K\"],\n",
    "    \"Oregon\": [\"OR\", \"O.R\"],\n",
    "    \"Rhode Island\": [\"RI\", \"R.I\"],\n",
    "    \"South Carolina\": [\"SC\", \"S.C\"],\n",
    "    \"South Dakota\": [\"SD\", \"S.D\"],\n",
    "    \"Tennessee\": [\"TN\", \"T.N\"],\n",
    "    \"Texas\": [\"TX\", \"T.X\"],\n",
    "    \"Utah\": [\"UT\", \"U.T\"],\n",
    "    \"Vermont\": [\"VT\", \"V.T\"],\n",
    "    \"Virginia\": [\"VA\", \"V.A\"],\n",
    "    \"Washington\": [\"WA\", \"W.A\"],\n",
    "    \"West Virginia\": [\"WV\", \"W.V\"],\n",
    "    \"Wisconsin\": [\"WI\", \"W.I\"],\n",
    "    \"Wyoming\": [\"WY\", \"W.Y\"],\n",
    "}\n",
    "\n",
    "# Words for Sentence Filtering\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Additional Preprocessing of Configurations\n",
    "presidential_candidates = {presidential_candidate: list(set(names)) for presidential_candidate, names in presidential_candidates.items()}\n",
    "presidential_candidates_combinations = [\n",
    "    name.lower()\n",
    "    for full_name, names in presidential_candidates.items()\n",
    "    for name in ([full_name] if len(full_name.split()) <= max_pair_of_words_for_topic else []) + [\n",
    "        name for name in names\n",
    "        if len(name.split()) <= max_pair_of_words_for_topic\n",
    "    ]\n",
    "]\n",
    "presidential_candidates_combinations_in_2d = [\n",
    "    ([full_name.lower()] if len(full_name.split()) <= max_pair_of_words_for_topic else []) + [\n",
    "        name.lower()\n",
    "        for name in names\n",
    "        if len(name.split()) <= max_pair_of_words_for_topic\n",
    "    ]\n",
    "    for full_name, names in presidential_candidates.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Extraction (Transcripts to CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T00:09:38.969010Z",
     "start_time": "2024-11-09T00:09:37.125594Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98b05a54bb84ea5a66f7ea1fc6a5916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Sentences for Michigan [0/260 Transcript]:   0%|          | 0/260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38e6811658f4cfca31ba93fd33ec398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Sentences for Arizona [0/168 Transcript]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee9e44d89644e2686c97894a4149e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Sentences for Pennsylvania [0/268 Transcript]:   0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences: 56797\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Possible_State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>But voting for that man and having him elected...</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Through it all, your life won't stop.</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But here are the latest overnight results.</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And have you been paying attention to stuff?</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arizona remains a battleground state with Trum...</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56792</th>\n",
       "      <td>So he's up by two points, according to Emerson.</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56793</th>\n",
       "      <td>Next thing he grabbed he grabbed the back of h...</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56794</th>\n",
       "      <td>She wants to limit all immigration.</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56795</th>\n",
       "      <td>I am not a veteran.</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56796</th>\n",
       "      <td>If you enjoyed this video and you'd like to se...</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56797 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence Possible_State\n",
       "0      But voting for that man and having him elected...   Pennsylvania\n",
       "1                  Through it all, your life won't stop.       Michigan\n",
       "2             But here are the latest overnight results.   Pennsylvania\n",
       "3           And have you been paying attention to stuff?       Michigan\n",
       "4      Arizona remains a battleground state with Trum...       Michigan\n",
       "...                                                  ...            ...\n",
       "56792    So he's up by two points, according to Emerson.   Pennsylvania\n",
       "56793  Next thing he grabbed he grabbed the back of h...   Pennsylvania\n",
       "56794                She wants to limit all immigration.   Pennsylvania\n",
       "56795                                I am not a veteran.       Michigan\n",
       "56796  If you enjoyed this video and you'd like to se...   Pennsylvania\n",
       "\n",
       "[56797 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_transcripts_into_csv_of_sentences() -> pd.DataFrame:\n",
    "    # Initialize list of sentences and possible states\n",
    "    list_of_sentences = []\n",
    "\n",
    "    # Collect sentences from each state's transcription files\n",
    "    for state, path in cities_transcription_paths.items():\n",
    "        transcription_files = os.listdir(path)\n",
    "        total_transcription_files = len(transcription_files)\n",
    "\n",
    "        with tqdm(total=total_transcription_files, desc=f'Collecting Sentences for {state} [0/{total_transcription_files} Transcript]') as pbar:\n",
    "            for index, filename in enumerate(transcription_files):\n",
    "                current = f'{index + 1}/{total_transcription_files}'\n",
    "                if filename == \".ipynb_checkpoints\":\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                pbar.set_description(f'Collecting Sentences for {state} [{current} Transcript]')\n",
    "\n",
    "                # Open transcription file\n",
    "                file_path = os.path.join(path, filename)\n",
    "                with open(file_path, \"r\", errors=\"ignore\") as file:\n",
    "                    transcription = file.read()\n",
    "\n",
    "                    # Split transcript into sentences\n",
    "                    sentences = sent_tokenize(transcription)\n",
    "\n",
    "                    # Remove consecutive duplicates\n",
    "                    sentences = [sentence for i, sentence in enumerate(sentences) if i == 0 or sentence != sentences[i - 1]]\n",
    "\n",
    "                    # Append each sentence with the state name\n",
    "                    list_of_sentences.extend([(sentence, state) for sentence in sentences])\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Convert the list of sentences and states into a DataFrame\n",
    "    df = pd.DataFrame(list(set(list_of_sentences)), columns=[\"Sentence\", \"Possible_State\"])\n",
    "    df.to_csv(transcript_sentences_filename, index=False, errors=\"ignore\")\n",
    "    return df\n",
    "\n",
    "# Run the function and print summary\n",
    "list_of_sentences = process_transcripts_into_csv_of_sentences()\n",
    "print(f'Number of Sentences: {len(list_of_sentences)}')\n",
    "list_of_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic: Relevant Sentence Filtering (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T00:14:45.691850Z",
     "start_time": "2024-11-09T00:09:39.005027Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 13:43:29,333 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a636d4939b7b4036bb610d0723377e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1775 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 13:45:28,892 - BERTopic - Embedding - Completed ✓\n",
      "2024-11-09 13:45:28,892 - BERTopic - Guided - Find embeddings highly related to seeded topics.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab519b84fe24b738790955416ceba6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 13:45:29,139 - BERTopic - Guided - Completed ✓\n",
      "2024-11-09 13:45:29,140 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-11-09 13:46:19,296 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-11-09 13:46:19,298 - BERTopic - Zeroshot Step 1 - Finding documents that could be assigned to either one of the zero-shot topics\n",
      "2024-11-09 13:46:19,606 - BERTopic - Zeroshot Step 1 - Completed ✓\n",
      "2024-11-09 13:46:57,044 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-11-09 13:47:01,744 - BERTopic - Cluster - Completed ✓\n",
      "2024-11-09 13:47:01,745 - BERTopic - Zeroshot Step 2 - Combining topics from zero-shot topic modeling with topics from clustering...\n",
      "2024-11-09 13:47:01,842 - BERTopic - Zeroshot Step 2 - Completed ✓\n",
      "2024-11-09 13:47:01,847 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-11-09 13:47:31,062 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "source": [
    "def filter_relevant_sentences() -> tuple[DataFrame, BERTopic]:\n",
    "    # Get All Collected Sentences from Transcript and a Map with their Respective Possible State\n",
    "    df = pd.read_csv(transcript_sentences_filename, encoding_errors=\"ignore\")\n",
    "    sentences_possible_state = pd.Series(df['Possible_State'].values, index=df['Sentence']).to_dict()\n",
    "    sentences = df[\"Sentence\"].tolist()\n",
    "    \n",
    "    # Define Filter for Words as Possible Topics\n",
    "    def filter_possible_topics(text: str) -> list:\n",
    "        \"\"\"\n",
    "            Filter Words If it's a Possible Topic:\n",
    "                1) Only Nouns and Proper Nouns (e.g. Dollars, Currency)\n",
    "                2) No Stop Words (e.g. in, to)\n",
    "                3) Minimum of Two-Letter Words (e.g. Ox)\n",
    "                4) Exclude Numbers\n",
    "        \"\"\"\n",
    "        pos_tags = pos_tag(word_tokenize(text)) # POS Tagging\n",
    "        # Return Possible Topics\n",
    "        return [\n",
    "            token.lower() for token, pos in pos_tags\n",
    "            if pos in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"] # Nouns / Proper Nouns\n",
    "            and token.lower() not in stop_words # Exclude Stop Words\n",
    "            and len(token) > 1 # Exclude One-Letter Words (e.g. Included: Ox)\n",
    "            and not token.isnumeric() # Exclude Numbers\n",
    "        ]\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        ngram_range=(1, max_pair_of_words_for_topic),\n",
    "        tokenizer=filter_possible_topics\n",
    "    )\n",
    "\n",
    "    # Train BERTopic model\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        n_gram_range=(1, max_pair_of_words_for_topic),\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        seed_topic_list=presidential_candidates_combinations_in_2d,\n",
    "        zeroshot_topic_list=presidential_candidates_combinations,\n",
    "        zeroshot_min_similarity=min_similarity_of_topic_modeling,\n",
    "        nr_topics=None if max_topic_count is None else \"auto\" if max_topic_count == \"auto\" else max(len(presidential_candidates_combinations), max_topic_count),\n",
    "        verbose=True\n",
    "    )\n",
    "    topic_ids, _ = topic_model.fit_transform(sentences)\n",
    "    \n",
    "    # Get BERTopic Results\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    topics_and_documents = pd.DataFrame({\"Topic\": topic_ids, \"Representative_Docs\": sentences})\n",
    "\n",
    "    # Initialize Lists for Relevant Sentences\n",
    "    list_of_relevant_sentences = []\n",
    "    \n",
    "    # Define Filters for Relevant Sentences\n",
    "    \"\"\"\n",
    "        Add Relevant Sentences Only If:\n",
    "            1) Only 1 Candidate is Mentioned in the Topic\n",
    "            2) No Other State is Mentioned in the Topic Different from Possible State\n",
    "            3) Sentence has Word Count Greater than N or 5\n",
    "    \"\"\"\n",
    "    def get_only_if_1_candidate_mentioned_in_the_topic(topic_ngramed_keywords: list[str]) ->  str | None:\n",
    "        # Collect Candidate Mentions in Topics\n",
    "        presidential_candidate_mentions = set() # Avoid Duplicates\n",
    "        for presidential_candidate, names in presidential_candidates.items():\n",
    "            if (\n",
    "                # Any Candidate is Mentioned in Topic\n",
    "                any(\n",
    "                    (\n",
    "                        presidential_candidate and ngramed_keyword\n",
    "                        and f' {presidential_candidate.strip().lower()} ' in f' {ngramed_keyword.strip().lower()} '\n",
    "                    ) or (\n",
    "                        presidential_candidate and word\n",
    "                        and presidential_candidate.strip().lower() == word.strip().lower()\n",
    "                    )\n",
    "                    for ngramed_keyword in topic_ngramed_keywords\n",
    "                    for word in ngramed_keyword.split(\" \")\n",
    "                )\n",
    "                # Any Other Candidate Names is Mentioned in Topic\n",
    "                or any(\n",
    "                    (\n",
    "                        name and ngramed_keyword\n",
    "                        and f' {name.strip().lower()} ' in f' {ngramed_keyword.strip().lower()} '\n",
    "                    ) or (\n",
    "                        name and word\n",
    "                        and name.strip().lower() == word.strip().lower()\n",
    "                    )\n",
    "                    for name in names\n",
    "                    for ngramed_keyword in topic_ngramed_keywords\n",
    "                    for word in ngramed_keyword.split(\" \")\n",
    "                )\n",
    "            ):\n",
    "                # Add The Candidate Mentioned\n",
    "                presidential_candidate_mentions.add(presidential_candidate)\n",
    "        # Return the Candidate If It's the Only 1 Mentioned\n",
    "        if len(presidential_candidate_mentions) == 1:\n",
    "            return presidential_candidate_mentions.pop()\n",
    "        else:\n",
    "            return None\n",
    "    def get_if_no_other_state_mentioned_in_topic_different_from_possible_state(topic_ngramed_keywords: list[str], sentence: str) ->  str | None:\n",
    "        # Get Possible State for the Sentence\n",
    "        possible_state = sentences_possible_state[sentence]\n",
    "        if possible_state not in state_cities: raise ValueError(f'This Sentence has Invalid Possible State ({possible_state}): \"{sentence}\"')\n",
    "        # Filter Sentence with Topic of [Other State] Not in [Arizona, Michigan, Pennsylvania]\n",
    "        if possible_state not in original_state_cities: return None\n",
    "        # Filter Sentence with Topic of [Other State] Different from its [Possible State]\n",
    "        other_states = [state for state in state_cities if state is not possible_state]\n",
    "        if any(\n",
    "            f' {other_state.strip().lower()} ' in f' {ngramed_keyword.strip().lower()} '\n",
    "            or (\n",
    "                word\n",
    "                and other_state.strip().lower() == word.strip().lower()\n",
    "            )\n",
    "            for other_state in other_states\n",
    "            for ngramed_keyword in topic_ngramed_keywords\n",
    "            for word in ngramed_keyword.split(\" \")\n",
    "        ): return None\n",
    "        # Filter Sentence with Topics of [Other States' Cities] Different from its [Possible State Cities]\n",
    "        other_state_cities = [\n",
    "            other_city\n",
    "            for other_cities in {\n",
    "                state: state_cities[state]\n",
    "                for state in state_cities\n",
    "                if state is not possible_state\n",
    "            }.values()\n",
    "            for other_city in other_cities\n",
    "            if other_city\n",
    "        ]\n",
    "        if any(\n",
    "            f' {other_city.strip().lower()} ' in f' {ngramed_keyword.strip().lower()} '\n",
    "            or (\n",
    "                word\n",
    "                and other_city.strip().lower() == word.strip().lower()\n",
    "            )\n",
    "            for other_city in other_state_cities\n",
    "            for ngramed_keyword in topic_ngramed_keywords\n",
    "            for word in ngramed_keyword.split(\" \")\n",
    "        ): return None\n",
    "        # Return the Possible State\n",
    "        return possible_state\n",
    "    def sentence_has_word_count_greater_than_n(sentence: str, min_number_of_word_in_relevant_sentence: int = min_number_of_word_in_relevant_sentence) -> bool:\n",
    "        # Only include word tags\n",
    "        word_tags = {\n",
    "            \"CC\",  # conjunctions (and, or, but)\n",
    "            \"CD\",  # cardinal numbers\n",
    "            \"DT\",  # determiners (the, a, this)\n",
    "            \"EX\",  # existential there\n",
    "            \"FW\",  # foreign words\n",
    "            \"IN\",  # prepositions\n",
    "            \"JJ\", \"JJR\", \"JJS\",  # adjectives\n",
    "            \"LS\",  # List markers (First, Second, One, Two, A, B, etc.)\n",
    "            \"MD\",  # modals (can, should)\n",
    "            \"NN\", \"NNP\", \"NNPS\", \"NNS\",  # nouns\n",
    "            \"PDT\",  # pre-determiners\n",
    "            \"PRP\", \"PRP$\",  # pronouns\n",
    "            \"RB\", \"RBR\", \"RBS\",  # adverbs\n",
    "            \"RP\",  # particles\n",
    "            \"TO\",  # to\n",
    "            \"UH\",  # interjections\n",
    "            \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\",  # verbs\n",
    "            \"WDT\", \"WP\", \"WP$\", \"WRB\"  # wh-words\n",
    "        }\n",
    "        pos_tags = pos_tag(word_tokenize(sentence)) # POS Tagging\n",
    "        word_count = sum(1 for word, pos in pos_tags if pos in word_tags)\n",
    "        return word_count >= min_number_of_word_in_relevant_sentence\n",
    "\n",
    "    # Get Relevant Sentences\n",
    "    for _, row in topic_info.iterrows():\n",
    "        topic_id = row[\"Topic\"]\n",
    "        if topic_id == -1: continue # Skip Outlier\n",
    "    \n",
    "        # Get List of Topics and their Sentences\n",
    "        topic_ngramed_keywords = [\n",
    "            ngramed_keyword \n",
    "            for ngramed_keyword in row[\"Representation\"]\n",
    "            if ngramed_keyword\n",
    "        ]\n",
    "        topic_sentences = topics_and_documents[topics_and_documents[\"Topic\"] == topic_id][\"Representative_Docs\"].tolist()\n",
    "        \n",
    "        for sentence in topic_sentences:\n",
    "            # Check and Get 1 Candidate from Topics\n",
    "            presidential_candidate = get_only_if_1_candidate_mentioned_in_the_topic(topic_ngramed_keywords)\n",
    "            if presidential_candidate is None: continue\n",
    "            \n",
    "            # Check and Get 1 State from Topics and [Possible State assigned in Sentence] \n",
    "            state = get_if_no_other_state_mentioned_in_topic_different_from_possible_state(topic_ngramed_keywords, sentence)\n",
    "            if state is None: continue\n",
    "            \n",
    "            # Check if sentence has word count greater than N (default: 5)\n",
    "            if not sentence_has_word_count_greater_than_n(sentence): continue\n",
    "            \n",
    "            # Add Relevant Sentence with their Respective Candidate and State\n",
    "            list_of_relevant_sentences.append({\n",
    "                \"Sentence\": sentence,\n",
    "                \"Presidential_Candidate\": presidential_candidate,\n",
    "                \"State\": state,\n",
    "                \"Topic_Keywords\": topic_ngramed_keywords\n",
    "            })\n",
    "    \n",
    "    # Save List of All Relevant Sentences into CSV file\n",
    "    df = pd.DataFrame(list_of_relevant_sentences)\n",
    "    df.to_csv(relevant_transcript_sentences_filename, index=False, errors=\"ignore\")\n",
    "    return df, topic_model\n",
    "\n",
    "list_of_relevant_sentences, bertopic_model = filter_relevant_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T00:14:46.016257Z",
     "start_time": "2024-11-09T00:14:45.981201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>24902</td>\n",
       "      <td>-1_harris_trump_states_people</td>\n",
       "      <td>[harris, trump, states, people, election, stat...</td>\n",
       "      <td>[Some are opting for Trump., But they're not g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>61</td>\n",
       "      <td>2190</td>\n",
       "      <td>61_interview_policy_momentum_speech</td>\n",
       "      <td>[interview, policy, momentum, speech, question...</td>\n",
       "      <td>[So in addition to sort of becoming more aggre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>304</td>\n",
       "      <td>1139</td>\n",
       "      <td>304_biden_joe biden_joe_president biden</td>\n",
       "      <td>[biden, joe biden, joe, president biden, biden...</td>\n",
       "      <td>[And in 2020, Joe Biden lost it by 2.3%., Why ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>423</td>\n",
       "      <td>597</td>\n",
       "      <td>423_democrats_party_republicans_democrat</td>\n",
       "      <td>[democrats, party, republicans, democrat, demo...</td>\n",
       "      <td>[How many of you identify as Democrats?, And i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>489</td>\n",
       "      <td>574</td>\n",
       "      <td>489_character_guy_man_job</td>\n",
       "      <td>[character, guy, man, job, part appeal, things...</td>\n",
       "      <td>[It's about character., Then you then I think ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>trump</td>\n",
       "      <td>[trump trump, trump, , , , , , , , ]</td>\n",
       "      <td>[Trump., Trump?, Trump.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>trump donald</td>\n",
       "      <td>[trump trump, donald trump, donald, trump, , ,...</td>\n",
       "      <td>[Um, Donald Trump., Donald Trump?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>d. kamala</td>\n",
       "      <td>[kamala kamala, kamala, , , , , , , , ]</td>\n",
       "      <td>[Kamala, kamala, kamala, kamala.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>j. trump</td>\n",
       "      <td>[trump, , , , , , , , , ]</td>\n",
       "      <td>[That is Donald J. Trump.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>john trump</td>\n",
       "      <td>[trump businessman, businessman, president tru...</td>\n",
       "      <td>[President Trump, he's a businessman.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>628 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Count                                      Name  \\\n",
       "0       -1  24902             -1_harris_trump_states_people   \n",
       "62      61   2190       61_interview_policy_momentum_speech   \n",
       "305    304   1139   304_biden_joe biden_joe_president biden   \n",
       "424    423    597  423_democrats_party_republicans_democrat   \n",
       "490    489    574                 489_character_guy_man_job   \n",
       "..     ...    ...                                       ...   \n",
       "3        2      3                                     trump   \n",
       "7        6      2                              trump donald   \n",
       "12      11      1                                 d. kamala   \n",
       "9        8      1                                  j. trump   \n",
       "8        7      1                                john trump   \n",
       "\n",
       "                                        Representation  \\\n",
       "0    [harris, trump, states, people, election, stat...   \n",
       "62   [interview, policy, momentum, speech, question...   \n",
       "305  [biden, joe biden, joe, president biden, biden...   \n",
       "424  [democrats, party, republicans, democrat, demo...   \n",
       "490  [character, guy, man, job, part appeal, things...   \n",
       "..                                                 ...   \n",
       "3                 [trump trump, trump, , , , , , , , ]   \n",
       "7    [trump trump, donald trump, donald, trump, , ,...   \n",
       "12             [kamala kamala, kamala, , , , , , , , ]   \n",
       "9                            [trump, , , , , , , , , ]   \n",
       "8    [trump businessman, businessman, president tru...   \n",
       "\n",
       "                                   Representative_Docs  \n",
       "0    [Some are opting for Trump., But they're not g...  \n",
       "62   [So in addition to sort of becoming more aggre...  \n",
       "305  [And in 2020, Joe Biden lost it by 2.3%., Why ...  \n",
       "424  [How many of you identify as Democrats?, And i...  \n",
       "490  [It's about character., Then you then I think ...  \n",
       "..                                                 ...  \n",
       "3                             [Trump., Trump?, Trump.]  \n",
       "7                   [Um, Donald Trump., Donald Trump?]  \n",
       "12                   [Kamala, kamala, kamala, kamala.]  \n",
       "9                           [That is Donald J. Trump.]  \n",
       "8               [President Trump, he's a businessman.]  \n",
       "\n",
       "[628 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertopic_model.get_topic_info().sort_values(by=\"Count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T00:14:46.100346Z",
     "start_time": "2024-11-09T00:14:46.084310Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Presidential_Candidate</th>\n",
       "      <th>State</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There's a Trump Trump.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[president trump, trump president, trump trump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm excited for Kamala.</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>[kamala kamala, kind person, kamala, kamala ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kamala want to do that.</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>[kamala kamala, kind person, kamala, kamala ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>But you look at Kamala and I'm thinking to mys...</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>[kamala kamala, kind person, kamala, kamala ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number one, what's really driving my vote is K...</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>[kamala kamala, kind person, kamala, kamala ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5164</th>\n",
       "      <td>So pretty much everything I've seen the past f...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[trump trump, trump president, trump, people t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5165</th>\n",
       "      <td>I guess I feel like bad most of the time if y...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>[trump trump, trump president, trump, people t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5166</th>\n",
       "      <td>Yeah, so lots of Trump love.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[trump trump, trump president, trump, people t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5167</th>\n",
       "      <td>Donald Trump does what he says.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[trump trump, trump president, trump, people t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5168</th>\n",
       "      <td>But now it's, it's a harder call just because ...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[trump trump, trump president, trump, people t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5169 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence  \\\n",
       "0                                There's a Trump Trump.   \n",
       "1                               I'm excited for Kamala.   \n",
       "2                               Kamala want to do that.   \n",
       "3     But you look at Kamala and I'm thinking to mys...   \n",
       "4     Number one, what's really driving my vote is K...   \n",
       "...                                                 ...   \n",
       "5164  So pretty much everything I've seen the past f...   \n",
       "5165   I guess I feel like bad most of the time if y...   \n",
       "5166                       Yeah, so lots of Trump love.   \n",
       "5167                    Donald Trump does what he says.   \n",
       "5168  But now it's, it's a harder call just because ...   \n",
       "\n",
       "     Presidential_Candidate         State  \\\n",
       "0              Donald Trump  Pennsylvania   \n",
       "1             Kamala Harris       Arizona   \n",
       "2             Kamala Harris       Arizona   \n",
       "3             Kamala Harris       Arizona   \n",
       "4             Kamala Harris      Michigan   \n",
       "...                     ...           ...   \n",
       "5164           Donald Trump  Pennsylvania   \n",
       "5165           Donald Trump       Arizona   \n",
       "5166           Donald Trump  Pennsylvania   \n",
       "5167           Donald Trump  Pennsylvania   \n",
       "5168           Donald Trump  Pennsylvania   \n",
       "\n",
       "                                         Topic_Keywords  \n",
       "0     [president trump, trump president, trump trump...  \n",
       "1     [kamala kamala, kind person, kamala, kamala ki...  \n",
       "2     [kamala kamala, kind person, kamala, kamala ki...  \n",
       "3     [kamala kamala, kind person, kamala, kamala ki...  \n",
       "4     [kamala kamala, kind person, kamala, kamala ki...  \n",
       "...                                                 ...  \n",
       "5164  [trump trump, trump president, trump, people t...  \n",
       "5165  [trump trump, trump president, trump, people t...  \n",
       "5166  [trump trump, trump president, trump, people t...  \n",
       "5167  [trump trump, trump president, trump, people t...  \n",
       "5168  [trump trump, trump president, trump, people t...  \n",
       "\n",
       "[5169 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_relevant_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T00:14:46.361934Z",
     "start_time": "2024-11-09T00:14:46.259140Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_df941\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_df941_level0_col0\" class=\"col_heading level0 col0\" >Presidential_Candidate</th>\n",
       "      <th id=\"T_df941_level0_col1\" class=\"col_heading level0 col1\" >State</th>\n",
       "      <th id=\"T_df941_level0_col2\" class=\"col_heading level0 col2\" >count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_df941_row0_col0\" class=\"data row0 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_df941_row0_col1\" class=\"data row0 col1\" >Arizona</td>\n",
       "      <td id=\"T_df941_row0_col2\" class=\"data row0 col2\" >876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_df941_row1_col0\" class=\"data row1 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_df941_row1_col1\" class=\"data row1 col1\" >Michigan</td>\n",
       "      <td id=\"T_df941_row1_col2\" class=\"data row1 col2\" >1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_df941_row2_col0\" class=\"data row2 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_df941_row2_col1\" class=\"data row2 col1\" >Pennsylvania</td>\n",
       "      <td id=\"T_df941_row2_col2\" class=\"data row2 col2\" >1801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_df941_row3_col0\" class=\"data row3 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_df941_row3_col1\" class=\"data row3 col1\" >Arizona</td>\n",
       "      <td id=\"T_df941_row3_col2\" class=\"data row3 col2\" >198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_df941_row4_col0\" class=\"data row4 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_df941_row4_col1\" class=\"data row4 col1\" >Michigan</td>\n",
       "      <td id=\"T_df941_row4_col2\" class=\"data row4 col2\" >364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_df941_row5_col0\" class=\"data row5 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_df941_row5_col1\" class=\"data row5 col1\" >Pennsylvania</td>\n",
       "      <td id=\"T_df941_row5_col2\" class=\"data row5 col2\" >581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_df941_row6_col0\" class=\"data row6 col0\" ></td>\n",
       "      <td id=\"T_df941_row6_col1\" class=\"data row6 col1\" >Total</td>\n",
       "      <td id=\"T_df941_row6_col2\" class=\"data row6 col2\" >5169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x22ffd273710>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sa tingin ko need natin 5k sentences minimum for Relevant Sentences di lang for gathered.\n",
    "Kasi mamaya 5k Random Sentences nakuha natin tas 100 lang dun Relevant with candidate & state.\n",
    "\n",
    "Ang naiisip ko since meron 6 Combinations = 3 candidate * 2 state\n",
    "Gawin natin 5000/6 = 834 Relevant Sentences required set natin as minimum per Combination\n",
    "\n",
    "Trump  - Arizona      = 834 Relevant Sentences\n",
    "Harris - Arizona      = 834 Relevant Sentences\n",
    "Trump  - Michigan     = 834 Relevant Sentences\n",
    "Harris - Michigan     = 834 Relevant Sentences\n",
    "Trump  - Pennsylvania = 834 Relevant Sentences\n",
    "Harris - Pennsylvania = 834 Relevant Sentences\n",
    "               -------------------------------\n",
    "               Total: ~5000 Relevant Sentences\n",
    "\"\"\"\n",
    "def print_statistics():\n",
    "    try:\n",
    "        grouped_df = (\n",
    "            list_of_relevant_sentences\n",
    "            .groupby([\"Presidential_Candidate\", \"State\"])\n",
    "            .size()\n",
    "            .reset_index(name=\"count\")\n",
    "        )\n",
    "        total_count = grouped_df[\"count\"].sum()\n",
    "        total_row = pd.DataFrame({\"Presidential_Candidate\": [\"\"], \"State\": [\"Total\"], \"count\": [total_count]})\n",
    "        grouped_df = pd.concat([grouped_df, total_row], ignore_index=True)\n",
    "        return grouped_df.style.hide(axis=\"index\")\n",
    "    except: return \"No Relevant Sentences\"\n",
    "print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T00:14:46.439334Z",
     "start_time": "2024-11-09T00:14:46.435975Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
