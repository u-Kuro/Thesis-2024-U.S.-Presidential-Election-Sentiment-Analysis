{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T00:09:37.046049Z",
     "start_time": "2024-11-09T00:09:35.691753Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\---\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "import os, re, nltk\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Additional Downloads\n",
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T00:09:37.064679Z",
     "start_time": "2024-11-09T00:09:37.051553Z"
    }
   },
   "outputs": [],
   "source": [
    "def sanitize_filename(filename: str) -> str:\n",
    "    # Escape Double Quotes\n",
    "    filename = filename.replace('\"', '\\\\\"')\n",
    "\n",
    "    # Replace Invalid Characters with \"_\"\n",
    "    invalid_chars = re.compile(r'[<>:\"/\\\\|?*]')\n",
    "    sanitized_filename = invalid_chars.sub(\"_\", filename)\n",
    "\n",
    "    return sanitized_filename\n",
    "    \n",
    "def read_unique_items_from_file(file: str) -> list:\n",
    "    if os.path.exists(file):\n",
    "        with open(file, \"r\", errors=\"ignore\") as f:\n",
    "            return list(set(e.strip() for e in f.readlines() if e.strip()))\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T00:17:16.273357Z",
     "start_time": "2024-11-09T00:17:16.252982Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# File Names\n",
    "transcript_sentences_filename = \"transcript_sentences.csv\"\n",
    "relevant_transcript_sentences_filename = \"relevant_transcript_sentences.csv\"\n",
    "\n",
    "# Folder Names\n",
    "transcription_path = \"Transcription\"\n",
    "cities_transcription_paths = {\n",
    "    \"Michigan\": os.path.join(transcription_path, \"Michigan\"),\n",
    "    \"Arizona\": os.path.join(transcription_path, \"Arizona\"),\n",
    "    \"Pennsylvania\": os.path.join(transcription_path, \"Pennsylvania\"),\n",
    "}\n",
    "cities_path = \"State Cities\"\n",
    "\n",
    "# Numeric Constants \n",
    "max_pair_of_words_for_topic = 2\n",
    "\"\"\"\n",
    "    > Maximum words to consider for topic extraction\n",
    "        1: Unigram (e.g., \"Donald\")\n",
    "        2: Bigram (e.g., \"Donald Trump\")\n",
    "\"\"\"\n",
    "\n",
    "min_number_of_word_in_relevant_sentence = 5\n",
    "\"\"\"\n",
    "    > Minimum word count required for a sentence to be considered relevant\n",
    "    Example: \"This is a nice place\" = 5 words\n",
    "\"\"\"\n",
    "\n",
    "min_similarity_of_topic_modeling = 0.7\n",
    "\"\"\"\n",
    "    > Minimum similarity threshold for topic matching\n",
    "        Range: [0.1, 1.0]\n",
    "    Note: Higher values require closer matches\n",
    "    Example: 0.7 = 70% similarity required\n",
    "\"\"\"\n",
    "\n",
    "max_topic_count = None\n",
    "\"\"\"\n",
    "    Topic count limiter for dimensionality reduction\n",
    "        None: No reduction, keep all discovered topics\n",
    "        \"auto\": Automatically Reduces Topic Count\n",
    "        Number: Force reduce to specified number of topics\n",
    "    Note: (1) Using Number for Numeric reduction may merge unrelated topics together\n",
    "          (2) Lower Number may increase precision but risk missing relevant topics\n",
    "\"\"\"\n",
    "\n",
    "# Sentence Categories\n",
    "presidential_candidates = {\n",
    "    \"Donald Trump\": [\n",
    "        \"Donald\", \"Trump\",\n",
    "        \"Trump Donald\", \"Donald John\", \"John Trump\",\n",
    "        \"Donald J\", \"J. Donald\", \"J. Trump\", \"Trump J\",\n",
    "        \"Trump D\", \"D. Trump\", \"John D\", \"D. John\",\n",
    "        \"Donald T\", \"T. Donald\", \"John T\", \"T. John\",\n",
    "        \"Donald John Trump\", \"Donald J Trump\", \"D. J. Trump\", \n",
    "        \"President Donald\", \"President Trump\",\n",
    "        \"President Donald Trump\"\n",
    "    ],\n",
    "    \"Kamala Harris\": [\n",
    "        \"Kamala\", \"Harris\",\n",
    "        \"Harris Kamala\", \"Kamala Devi\", \"Devi Harris\",\n",
    "        \"Kamala D\", \"D. Kamala\", \"D. Harris\", \"Harris D\",\n",
    "        \"Harris K\", \"K. Harris\", \"Devi K\", \"K. Devi\",\n",
    "        \"Kamala H\", \"H. Kamala\", \"Devi H\", \"H. Devi\",\n",
    "        \"Kamala Devi Harris\", \"Kamala D Harris\", \"K. D. Harris\",  \n",
    "        \"President Kamala\", \"President Harris\",\n",
    "        \"President Kamala Harris\"\n",
    "    ]\n",
    "}\n",
    "original_state_cities = [\"Arizona\", \"Michigan\", \"Pennsylvania\"]\n",
    "state_cities = {\n",
    "    \"Arizona\": read_unique_items_from_file(os.path.join(cities_path, \"arizona-cities.txt\")),\n",
    "    \"Michigan\": read_unique_items_from_file(os.path.join(cities_path, \"michigan-cities.txt\")),\n",
    "    \"Pennsylvania\": read_unique_items_from_file(os.path.join(cities_path, \"pennsylvania-cities.txt\")),\n",
    "    \"Alabama\": [\"AL\", \"A.L\"],\n",
    "    \"Alaska\": [\"AK\", \"A.K\"],\n",
    "    \"Arkansas\": [\"AR\", \"A.R\"],\n",
    "    \"California\": [\"CA\", \"C.A\"],\n",
    "    \"Colorado\": [\"CO\", \"C.O\"],\n",
    "    \"Connecticut\": [\"CT\", \"C.T\"],\n",
    "    \"Delaware\": [\"DE\", \"D.E\"],\n",
    "    \"Florida\": [\"FL\", \"F.L\"],\n",
    "    \"Georgia\": [\"GA\", \"G.A\"],\n",
    "    \"Hawaii\": [\"HI\", \"H.I\"],\n",
    "    \"Idaho\": [\"ID\", \"I.D\"],\n",
    "    \"Illinois\": [\"IL\", \"I.L\"],\n",
    "    \"Indiana\": [\"IN\", \"I.N\"],\n",
    "    \"Iowa\": [\"IA\", \"I.A\"],\n",
    "    \"Kansas\": [\"KS\", \"K.S\"],\n",
    "    \"Kentucky\": [\"KY\", \"K.Y\"],\n",
    "    \"Louisiana\": [\"LA\", \"L.A\"],\n",
    "    \"Maine\": [\"ME\", \"M.E\"],\n",
    "    \"Maryland\": [\"MD\", \"M.D\"],\n",
    "    \"Massachusetts\": [\"MA\", \"M.A\"],\n",
    "    \"Minnesota\": [\"MN\", \"M.N\"],\n",
    "    \"Mississippi\": [\"MS\", \"M.S\"],\n",
    "    \"Missouri\": [\"MO\", \"M.O\"],\n",
    "    \"Montana\": [\"MT\", \"M.T\"],\n",
    "    \"Nebraska\": [\"NE\", \"N.E\"],\n",
    "    \"Nevada\": [\"NV\", \"N.V\"],\n",
    "    \"New Hampshire\": [\"NH\", \"N.H\"],\n",
    "    \"New Jersey\": [\"NJ\", \"N.J\"],\n",
    "    \"New Mexico\": [\"NM\", \"N.M\"],\n",
    "    \"New York\": [\"NY\", \"N.Y\"],\n",
    "    \"North Carolina\": [\"NC\", \"N.C\"],\n",
    "    \"North Dakota\": [\"ND\", \"N.D\"],\n",
    "    \"Ohio\": [\"OH\", \"O.H\"],\n",
    "    \"Oklahoma\": [\"OK\", \"O.K\"],\n",
    "    \"Oregon\": [\"OR\", \"O.R\"],\n",
    "    \"Rhode Island\": [\"RI\", \"R.I\"],\n",
    "    \"South Carolina\": [\"SC\", \"S.C\"],\n",
    "    \"South Dakota\": [\"SD\", \"S.D\"],\n",
    "    \"Tennessee\": [\"TN\", \"T.N\"],\n",
    "    \"Texas\": [\"TX\", \"T.X\"],\n",
    "    \"Utah\": [\"UT\", \"U.T\"],\n",
    "    \"Vermont\": [\"VT\", \"V.T\"],\n",
    "    \"Virginia\": [\"VA\", \"V.A\"],\n",
    "    \"Washington\": [\"WA\", \"W.A\"],\n",
    "    \"West Virginia\": [\"WV\", \"W.V\"],\n",
    "    \"Wisconsin\": [\"WI\", \"W.I\"],\n",
    "    \"Wyoming\": [\"WY\", \"W.Y\"],\n",
    "}\n",
    "\n",
    "# Words for Sentence Filtering\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Additional Preprocessing of Configurations\n",
    "presidential_candidates = {presidential_candidate: list(set(names)) for presidential_candidate, names in presidential_candidates.items()}\n",
    "presidential_candidates_combinations = [\n",
    "    name.lower()\n",
    "    for full_name, names in presidential_candidates.items()\n",
    "    for name in ([full_name] if len(full_name.split()) <= max_pair_of_words_for_topic else []) + [\n",
    "        name for name in names\n",
    "        if len(name.split()) <= max_pair_of_words_for_topic\n",
    "    ]\n",
    "]\n",
    "presidential_candidates_combinations_in_2d = [\n",
    "    ([full_name.lower()] if len(full_name.split()) <= max_pair_of_words_for_topic else []) + [\n",
    "        name.lower()\n",
    "        for name in names\n",
    "        if len(name.split()) <= max_pair_of_words_for_topic\n",
    "    ]\n",
    "    for full_name, names in presidential_candidates.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Extraction (Transcripts to CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T00:09:38.969010Z",
     "start_time": "2024-11-09T00:09:37.125594Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99fd9b8e4af44ca1a5df09b8e2992fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Sentences for Michigan [0/260 Transcript]:   0%|          | 0/260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9ae9403a03421f8fe1e6ab5f2fcd63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Sentences for Arizona [0/182 Transcript]:   0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5190fc5ecb104e5097b0f6962f0df13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Sentences for Pennsylvania [0/268 Transcript]:   0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences: 57876\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Possible_State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>These guys come home from guys, females, I'm s...</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ain't nobody here voting for Kamala Harris.</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cheap Chinese imports.</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What happened to Mike Pence wasn't an isolated...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm asking you to be excited about the future ...</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57871</th>\n",
       "      <td>All right Garrett joining us now Garrett what ...</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57872</th>\n",
       "      <td>Democrats talk about challenging in Florida.</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57873</th>\n",
       "      <td>From the night of the election, the stop the s...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57874</th>\n",
       "      <td>I was right about the crime stats going way up.</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57875</th>\n",
       "      <td>And we actually heard Donald Trump criticizing...</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57876 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence Possible_State\n",
       "0      These guys come home from guys, females, I'm s...       Michigan\n",
       "1            Ain't nobody here voting for Kamala Harris.   Pennsylvania\n",
       "2                                 Cheap Chinese imports.       Michigan\n",
       "3      What happened to Mike Pence wasn't an isolated...        Arizona\n",
       "4      I'm asking you to be excited about the future ...       Michigan\n",
       "...                                                  ...            ...\n",
       "57871  All right Garrett joining us now Garrett what ...       Michigan\n",
       "57872       Democrats talk about challenging in Florida.   Pennsylvania\n",
       "57873  From the night of the election, the stop the s...        Arizona\n",
       "57874    I was right about the crime stats going way up.   Pennsylvania\n",
       "57875  And we actually heard Donald Trump criticizing...       Michigan\n",
       "\n",
       "[57876 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_transcripts_into_csv_of_sentences() -> pd.DataFrame:\n",
    "    # Initialize list of sentences and possible states\n",
    "    list_of_sentences = []\n",
    "\n",
    "    # Collect sentences from each state's transcription files\n",
    "    for state, path in cities_transcription_paths.items():\n",
    "        transcription_files = os.listdir(path)\n",
    "        total_transcription_files = len(transcription_files)\n",
    "\n",
    "        with tqdm(total=total_transcription_files, desc=f'Collecting Sentences for {state} [0/{total_transcription_files} Transcript]') as pbar:\n",
    "            for index, filename in enumerate(transcription_files):\n",
    "                current = f'{index + 1}/{total_transcription_files}'\n",
    "                if filename == \".ipynb_checkpoints\":\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                pbar.set_description(f'Collecting Sentences for {state} [{current} Transcript]')\n",
    "\n",
    "                # Open transcription file\n",
    "                file_path = os.path.join(path, filename)\n",
    "                with open(file_path, \"r\", errors=\"ignore\") as file:\n",
    "                    transcription = file.read()\n",
    "\n",
    "                    # Split transcript into sentences\n",
    "                    sentences = sent_tokenize(transcription)\n",
    "\n",
    "                    # Remove consecutive duplicates\n",
    "                    sentences = [sentence for i, sentence in enumerate(sentences) if i == 0 or sentence != sentences[i - 1]]\n",
    "\n",
    "                    # Append each sentence with the state name\n",
    "                    list_of_sentences.extend([(sentence, state) for sentence in sentences])\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Convert the list of sentences and states into a DataFrame\n",
    "    df = pd.DataFrame(list(set(list_of_sentences)), columns=[\"Sentence\", \"Possible_State\"])\n",
    "    df.to_csv(transcript_sentences_filename, index=False, errors=\"ignore\")\n",
    "    return df\n",
    "\n",
    "# Run the function and print summary\n",
    "list_of_sentences = process_transcripts_into_csv_of_sentences()\n",
    "print(f'Number of Sentences: {len(list_of_sentences)}')\n",
    "list_of_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic: Relevant Sentence Filtering (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T00:14:45.691850Z",
     "start_time": "2024-11-09T00:09:39.005027Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 01:13:34,031 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a3008ad8784698936ca7b75805bdfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1809 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 01:15:31,936 - BERTopic - Embedding - Completed ✓\n",
      "2024-11-21 01:15:31,938 - BERTopic - Guided - Find embeddings highly related to seeded topics.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c54c11da12423095d101ed65d2ff2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 01:15:32,170 - BERTopic - Guided - Completed ✓\n",
      "2024-11-21 01:15:32,171 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-11-21 01:16:23,173 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-11-21 01:16:23,175 - BERTopic - Zeroshot Step 1 - Finding documents that could be assigned to either one of the zero-shot topics\n",
      "2024-11-21 01:16:23,482 - BERTopic - Zeroshot Step 1 - Completed ✓\n",
      "2024-11-21 01:17:02,510 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-11-21 01:17:06,932 - BERTopic - Cluster - Completed ✓\n",
      "2024-11-21 01:17:06,932 - BERTopic - Zeroshot Step 2 - Combining topics from zero-shot topic modeling with topics from clustering...\n",
      "2024-11-21 01:17:07,021 - BERTopic - Zeroshot Step 2 - Completed ✓\n",
      "2024-11-21 01:17:07,025 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-11-21 01:17:38,129 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "source": [
    "def filter_relevant_sentences() -> tuple[DataFrame, BERTopic]:\n",
    "    # Get All Collected Sentences from Transcript and a Map with their Respective Possible State\n",
    "    df = pd.read_csv(transcript_sentences_filename, encoding_errors=\"ignore\")\n",
    "    sentences_possible_state = pd.Series(df['Possible_State'].values, index=df['Sentence']).to_dict()\n",
    "    sentences = df[\"Sentence\"].tolist()\n",
    "    \n",
    "    # Define Filter for Words as Possible Topics\n",
    "    def filter_possible_topics(text: str) -> list:\n",
    "        \"\"\"\n",
    "            Filter Words If it's a Possible Topic:\n",
    "                1) Only Nouns and Proper Nouns (e.g. Dollars, Currency)\n",
    "                2) No Stop Words (e.g. in, to)\n",
    "                3) Minimum of Two-Letter Words (e.g. Ox)\n",
    "                4) Exclude Numbers\n",
    "        \"\"\"\n",
    "        pos_tags = pos_tag(word_tokenize(text)) # POS Tagging\n",
    "        # Return Possible Topics\n",
    "        return [\n",
    "            token.lower() for token, pos in pos_tags\n",
    "            if pos in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"] # Nouns / Proper Nouns\n",
    "            and token.lower() not in stop_words # Exclude Stop Words\n",
    "            and len(token) > 1 # Exclude One-Letter Words (e.g. Included: Ox)\n",
    "            and not token.isnumeric() # Exclude Numbers\n",
    "        ]\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        ngram_range=(1, max_pair_of_words_for_topic),\n",
    "        tokenizer=filter_possible_topics\n",
    "    )\n",
    "\n",
    "    # Train BERTopic model\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        n_gram_range=(1, max_pair_of_words_for_topic),\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        seed_topic_list=presidential_candidates_combinations_in_2d,\n",
    "        zeroshot_topic_list=presidential_candidates_combinations,\n",
    "        zeroshot_min_similarity=min_similarity_of_topic_modeling,\n",
    "        nr_topics=None if max_topic_count is None else \"auto\" if max_topic_count == \"auto\" else max(len(presidential_candidates_combinations), max_topic_count),\n",
    "        verbose=True\n",
    "    )\n",
    "    topic_ids, _ = topic_model.fit_transform(sentences)\n",
    "    \n",
    "    # Get BERTopic Results\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    topics_and_documents = pd.DataFrame({\"Topic\": topic_ids, \"Representative_Docs\": sentences})\n",
    "\n",
    "    # Initialize Lists for Relevant Sentences\n",
    "    list_of_relevant_sentences = []\n",
    "    \n",
    "    # Define Filters for Relevant Sentences\n",
    "    \"\"\"\n",
    "        Add Relevant Sentences Only If:\n",
    "            1) Only 1 Candidate is Mentioned in the Topic\n",
    "            2) No Other State is Mentioned in the Topic Different from Possible State\n",
    "            3) Sentence has Word Count Greater than N or 5\n",
    "    \"\"\"\n",
    "    def get_only_if_1_candidate_mentioned_in_the_topic(topic_ngramed_keywords: list[str]) ->  str | None:\n",
    "        # Collect Candidate Mentions in Topics\n",
    "        presidential_candidate_mentions = set() # Avoid Duplicates\n",
    "        for presidential_candidate, names in presidential_candidates.items():\n",
    "            if (\n",
    "                # Any Candidate is Mentioned in Topic\n",
    "                any(\n",
    "                    (\n",
    "                        presidential_candidate and ngramed_keyword\n",
    "                        and f' {presidential_candidate.strip().lower()} ' in f' {ngramed_keyword.strip().lower()} '\n",
    "                    ) or (\n",
    "                        presidential_candidate and word\n",
    "                        and presidential_candidate.strip().lower() == word.strip().lower()\n",
    "                    )\n",
    "                    for ngramed_keyword in topic_ngramed_keywords\n",
    "                    for word in ngramed_keyword.split(\" \")\n",
    "                )\n",
    "                # Any Other Candidate Names is Mentioned in Topic\n",
    "                or any(\n",
    "                    (\n",
    "                        name and ngramed_keyword\n",
    "                        and f' {name.strip().lower()} ' in f' {ngramed_keyword.strip().lower()} '\n",
    "                    ) or (\n",
    "                        name and word\n",
    "                        and name.strip().lower() == word.strip().lower()\n",
    "                    )\n",
    "                    for name in names\n",
    "                    for ngramed_keyword in topic_ngramed_keywords\n",
    "                    for word in ngramed_keyword.split(\" \")\n",
    "                )\n",
    "            ):\n",
    "                # Add The Candidate Mentioned\n",
    "                presidential_candidate_mentions.add(presidential_candidate)\n",
    "        # Return the Candidate If It's the Only 1 Mentioned\n",
    "        if len(presidential_candidate_mentions) == 1:\n",
    "            return presidential_candidate_mentions.pop()\n",
    "        else:\n",
    "            return None\n",
    "    def get_if_no_other_state_mentioned_in_topic_different_from_possible_state(topic_ngramed_keywords: list[str], sentence: str) ->  str | None:\n",
    "        # Get Possible State for the Sentence\n",
    "        possible_state = sentences_possible_state[sentence]\n",
    "        if possible_state not in state_cities: raise ValueError(f'This Sentence has Invalid Possible State ({possible_state}): \"{sentence}\"')\n",
    "        # Filter Sentence with Topic of [Other State] Not in [Arizona, Michigan, Pennsylvania]\n",
    "        if possible_state not in original_state_cities: return None\n",
    "        # Filter Sentence with Topic of [Other State] Different from its [Possible State]\n",
    "        other_states = [state for state in state_cities if state is not possible_state]\n",
    "        if any(\n",
    "            f' {other_state.strip().lower()} ' in f' {ngramed_keyword.strip().lower()} '\n",
    "            or (\n",
    "                word\n",
    "                and other_state.strip().lower() == word.strip().lower()\n",
    "            )\n",
    "            for other_state in other_states\n",
    "            for ngramed_keyword in topic_ngramed_keywords\n",
    "            for word in ngramed_keyword.split(\" \")\n",
    "        ): return None\n",
    "        # Filter Sentence with Topics of [Other States' Cities] Different from its [Possible State Cities]\n",
    "        other_state_cities = [\n",
    "            other_city\n",
    "            for other_cities in {\n",
    "                state: state_cities[state]\n",
    "                for state in state_cities\n",
    "                if state is not possible_state\n",
    "            }.values()\n",
    "            for other_city in other_cities\n",
    "            if other_city\n",
    "        ]\n",
    "        if any(\n",
    "            f' {other_city.strip().lower()} ' in f' {ngramed_keyword.strip().lower()} '\n",
    "            or (\n",
    "                word\n",
    "                and other_city.strip().lower() == word.strip().lower()\n",
    "            )\n",
    "            for other_city in other_state_cities\n",
    "            for ngramed_keyword in topic_ngramed_keywords\n",
    "            for word in ngramed_keyword.split(\" \")\n",
    "        ): return None\n",
    "        # Return the Possible State\n",
    "        return possible_state\n",
    "    def sentence_has_word_count_greater_than_n(sentence: str, min_number_of_word_in_relevant_sentence: int = min_number_of_word_in_relevant_sentence) -> bool:\n",
    "        # Only include word tags\n",
    "        word_tags = {\n",
    "            \"CC\",  # conjunctions (and, or, but)\n",
    "            \"CD\",  # cardinal numbers\n",
    "            \"DT\",  # determiners (the, a, this)\n",
    "            \"EX\",  # existential there\n",
    "            \"FW\",  # foreign words\n",
    "            \"IN\",  # prepositions\n",
    "            \"JJ\", \"JJR\", \"JJS\",  # adjectives\n",
    "            \"LS\",  # List markers (First, Second, One, Two, A, B, etc.)\n",
    "            \"MD\",  # modals (can, should)\n",
    "            \"NN\", \"NNP\", \"NNPS\", \"NNS\",  # nouns\n",
    "            \"PDT\",  # pre-determiners\n",
    "            \"PRP\", \"PRP$\",  # pronouns\n",
    "            \"RB\", \"RBR\", \"RBS\",  # adverbs\n",
    "            \"RP\",  # particles\n",
    "            \"TO\",  # to\n",
    "            \"UH\",  # interjections\n",
    "            \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\",  # verbs\n",
    "            \"WDT\", \"WP\", \"WP$\", \"WRB\"  # wh-words\n",
    "        }\n",
    "        pos_tags = pos_tag(word_tokenize(sentence)) # POS Tagging\n",
    "        word_count = sum(1 for word, pos in pos_tags if pos in word_tags)\n",
    "        return word_count >= min_number_of_word_in_relevant_sentence\n",
    "\n",
    "    # Get Relevant Sentences\n",
    "    for _, row in topic_info.iterrows():\n",
    "        topic_id = row[\"Topic\"]\n",
    "        if topic_id == -1: continue # Skip Outlier\n",
    "    \n",
    "        # Get List of Topics and their Sentences\n",
    "        topic_ngramed_keywords = [\n",
    "            ngramed_keyword \n",
    "            for ngramed_keyword in row[\"Representation\"]\n",
    "            if ngramed_keyword\n",
    "        ]\n",
    "        topic_sentences = topics_and_documents[topics_and_documents[\"Topic\"] == topic_id][\"Representative_Docs\"].tolist()\n",
    "        \n",
    "        for sentence in topic_sentences:\n",
    "            # Check and Get 1 Candidate from Topics\n",
    "            presidential_candidate = get_only_if_1_candidate_mentioned_in_the_topic(topic_ngramed_keywords)\n",
    "            if presidential_candidate is None: continue\n",
    "            \n",
    "            # Check and Get 1 State from Topics and [Possible State assigned in Sentence] \n",
    "            state = get_if_no_other_state_mentioned_in_topic_different_from_possible_state(topic_ngramed_keywords, sentence)\n",
    "            if state is None: continue\n",
    "            \n",
    "            # Check if sentence has word count greater than N (default: 5)\n",
    "            if not sentence_has_word_count_greater_than_n(sentence): continue\n",
    "            \n",
    "            # Add Relevant Sentence with their Respective Candidate and State\n",
    "            list_of_relevant_sentences.append({\n",
    "                \"Sentence\": sentence,\n",
    "                \"Presidential_Candidate\": presidential_candidate,\n",
    "                \"State\": state,\n",
    "                \"Topic_Keywords\": topic_ngramed_keywords\n",
    "            })\n",
    "    \n",
    "    # Save List of All Relevant Sentences into CSV file\n",
    "    df = pd.DataFrame(list_of_relevant_sentences)\n",
    "    df.to_csv(relevant_transcript_sentences_filename, index=False, errors=\"ignore\")\n",
    "    return df, topic_model\n",
    "\n",
    "list_of_relevant_sentences, bertopic_model = filter_relevant_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T00:14:46.016257Z",
     "start_time": "2024-11-09T00:14:45.981201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>25609</td>\n",
       "      <td>-1_trump_harris_people_election</td>\n",
       "      <td>[trump, harris, people, election, states, stat...</td>\n",
       "      <td>[The most important battleground state is Penn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>307</td>\n",
       "      <td>1144</td>\n",
       "      <td>307_biden_joe biden_joe_president biden</td>\n",
       "      <td>[biden, joe biden, joe, president biden, biden...</td>\n",
       "      <td>[Why are you voting for Joe Biden?, I was unco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>392</td>\n",
       "      <td>882</td>\n",
       "      <td>392_character_guy_man_blah</td>\n",
       "      <td>[character, guy, man, blah, ego, blah blah, bl...</td>\n",
       "      <td>[He's brain damaged, blah, blah, blah, blah, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>524</td>\n",
       "      <td>619</td>\n",
       "      <td>524_michigan_state michigan_michigan michigan_...</td>\n",
       "      <td>[michigan, state michigan, michigan michigan, ...</td>\n",
       "      <td>[Michigan is very odd as well., As far as Mich...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>274</td>\n",
       "      <td>542</td>\n",
       "      <td>274_thank_name_thanks_john</td>\n",
       "      <td>[thank, name, thanks, john, ruben ruben, j.d, ...</td>\n",
       "      <td>[Thank you so much for joining us., Thank you,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>president harris</td>\n",
       "      <td>[today vice, harris today, president harris, h...</td>\n",
       "      <td>[Harris or Trump., What about vice president H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>j. trump</td>\n",
       "      <td>[j. trump, donald j., j., trump donald, donald...</td>\n",
       "      <td>[That is Donald J. Trump., Absolutely 100% Don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>trump donald</td>\n",
       "      <td>[donald trump, donald, trump, , , , , , , ]</td>\n",
       "      <td>[Donald Trump?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>d. kamala</td>\n",
       "      <td>[kamala kamala, kamala, , , , , , , , ]</td>\n",
       "      <td>[Kamala, kamala, kamala, kamala.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>john trump</td>\n",
       "      <td>[trump businessman, businessman, president tru...</td>\n",
       "      <td>[President Trump, he's a businessman.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>621 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Count                                               Name  \\\n",
       "0       -1  25609                    -1_trump_harris_people_election   \n",
       "308    307   1144            307_biden_joe biden_joe_president biden   \n",
       "393    392    882                         392_character_guy_man_blah   \n",
       "525    524    619  524_michigan_state michigan_michigan michigan_...   \n",
       "275    274    542                         274_thank_name_thanks_john   \n",
       "..     ...    ...                                                ...   \n",
       "12      11      3                                   president harris   \n",
       "8        7      3                                           j. trump   \n",
       "9        8      1                                       trump donald   \n",
       "14      13      1                                          d. kamala   \n",
       "3        2      1                                         john trump   \n",
       "\n",
       "                                        Representation  \\\n",
       "0    [trump, harris, people, election, states, stat...   \n",
       "308  [biden, joe biden, joe, president biden, biden...   \n",
       "393  [character, guy, man, blah, ego, blah blah, bl...   \n",
       "525  [michigan, state michigan, michigan michigan, ...   \n",
       "275  [thank, name, thanks, john, ruben ruben, j.d, ...   \n",
       "..                                                 ...   \n",
       "12   [today vice, harris today, president harris, h...   \n",
       "8    [j. trump, donald j., j., trump donald, donald...   \n",
       "9          [donald trump, donald, trump, , , , , , , ]   \n",
       "14             [kamala kamala, kamala, , , , , , , , ]   \n",
       "3    [trump businessman, businessman, president tru...   \n",
       "\n",
       "                                   Representative_Docs  \n",
       "0    [The most important battleground state is Penn...  \n",
       "308  [Why are you voting for Joe Biden?, I was unco...  \n",
       "393  [He's brain damaged, blah, blah, blah, blah, b...  \n",
       "525  [Michigan is very odd as well., As far as Mich...  \n",
       "275  [Thank you so much for joining us., Thank you,...  \n",
       "..                                                 ...  \n",
       "12   [Harris or Trump., What about vice president H...  \n",
       "8    [That is Donald J. Trump., Absolutely 100% Don...  \n",
       "9                                      [Donald Trump?]  \n",
       "14                   [Kamala, kamala, kamala, kamala.]  \n",
       "3               [President Trump, he's a businessman.]  \n",
       "\n",
       "[621 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertopic_model.get_topic_info().sort_values(by=\"Count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T00:14:46.100346Z",
     "start_time": "2024-11-09T00:14:46.084310Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Presidential_Candidate</th>\n",
       "      <th>State</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>And I think Donald Trump, I'm going to call it...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>[trump america, america trump, trump trump, tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump all the way, 100%.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>[trump america, america trump, trump trump, tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>That's not what Harris needed.</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[harris harris, problem harris, harris problem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Let's say Harris holds onto that.</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>[harris harris, problem harris, harris problem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm trying to keep an open mind about Harris.</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[harris harris, problem harris, harris problem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6014</th>\n",
       "      <td>So he's out his polling is outperforming his l...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>[trump polls, polls trump, polls, vote polls, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6015</th>\n",
       "      <td>We're going to look at the election polling in...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>[trump polls, polls trump, polls, vote polls, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6016</th>\n",
       "      <td>They are up significantly in most polling from...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>[trump polls, polls trump, polls, vote polls, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6017</th>\n",
       "      <td>The larger polls I'm seeing has Trump up by tw...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[trump polls, polls trump, polls, vote polls, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6018</th>\n",
       "      <td>But if anything, he's like slipped up a little...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[trump polls, polls trump, polls, vote polls, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6019 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence  \\\n",
       "0     And I think Donald Trump, I'm going to call it...   \n",
       "1                       Donald Trump all the way, 100%.   \n",
       "2                        That's not what Harris needed.   \n",
       "3                     Let's say Harris holds onto that.   \n",
       "4         I'm trying to keep an open mind about Harris.   \n",
       "...                                                 ...   \n",
       "6014  So he's out his polling is outperforming his l...   \n",
       "6015  We're going to look at the election polling in...   \n",
       "6016  They are up significantly in most polling from...   \n",
       "6017  The larger polls I'm seeing has Trump up by tw...   \n",
       "6018  But if anything, he's like slipped up a little...   \n",
       "\n",
       "     Presidential_Candidate         State  \\\n",
       "0              Donald Trump      Michigan   \n",
       "1              Donald Trump       Arizona   \n",
       "2             Kamala Harris  Pennsylvania   \n",
       "3             Kamala Harris      Michigan   \n",
       "4             Kamala Harris  Pennsylvania   \n",
       "...                     ...           ...   \n",
       "6014           Donald Trump      Michigan   \n",
       "6015           Donald Trump      Michigan   \n",
       "6016           Donald Trump       Arizona   \n",
       "6017           Donald Trump  Pennsylvania   \n",
       "6018           Donald Trump  Pennsylvania   \n",
       "\n",
       "                                         Topic_Keywords  \n",
       "0     [trump america, america trump, trump trump, tr...  \n",
       "1     [trump america, america trump, trump trump, tr...  \n",
       "2     [harris harris, problem harris, harris problem...  \n",
       "3     [harris harris, problem harris, harris problem...  \n",
       "4     [harris harris, problem harris, harris problem...  \n",
       "...                                                 ...  \n",
       "6014  [trump polls, polls trump, polls, vote polls, ...  \n",
       "6015  [trump polls, polls trump, polls, vote polls, ...  \n",
       "6016  [trump polls, polls trump, polls, vote polls, ...  \n",
       "6017  [trump polls, polls trump, polls, vote polls, ...  \n",
       "6018  [trump polls, polls trump, polls, vote polls, ...  \n",
       "\n",
       "[6019 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_relevant_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T00:14:46.361934Z",
     "start_time": "2024-11-09T00:14:46.259140Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_98c17\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_98c17_level0_col0\" class=\"col_heading level0 col0\" >Presidential_Candidate</th>\n",
       "      <th id=\"T_98c17_level0_col1\" class=\"col_heading level0 col1\" >State</th>\n",
       "      <th id=\"T_98c17_level0_col2\" class=\"col_heading level0 col2\" >count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_98c17_row0_col0\" class=\"data row0 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_98c17_row0_col1\" class=\"data row0 col1\" >Arizona</td>\n",
       "      <td id=\"T_98c17_row0_col2\" class=\"data row0 col2\" >1059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_98c17_row1_col0\" class=\"data row1 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_98c17_row1_col1\" class=\"data row1 col1\" >Michigan</td>\n",
       "      <td id=\"T_98c17_row1_col2\" class=\"data row1 col2\" >1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_98c17_row2_col0\" class=\"data row2 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_98c17_row2_col1\" class=\"data row2 col1\" >Pennsylvania</td>\n",
       "      <td id=\"T_98c17_row2_col2\" class=\"data row2 col2\" >2063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_98c17_row3_col0\" class=\"data row3 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_98c17_row3_col1\" class=\"data row3 col1\" >Arizona</td>\n",
       "      <td id=\"T_98c17_row3_col2\" class=\"data row3 col2\" >242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_98c17_row4_col0\" class=\"data row4 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_98c17_row4_col1\" class=\"data row4 col1\" >Michigan</td>\n",
       "      <td id=\"T_98c17_row4_col2\" class=\"data row4 col2\" >372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_98c17_row5_col0\" class=\"data row5 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_98c17_row5_col1\" class=\"data row5 col1\" >Pennsylvania</td>\n",
       "      <td id=\"T_98c17_row5_col2\" class=\"data row5 col2\" >670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_98c17_row6_col0\" class=\"data row6 col0\" ></td>\n",
       "      <td id=\"T_98c17_row6_col1\" class=\"data row6 col1\" >Total</td>\n",
       "      <td id=\"T_98c17_row6_col2\" class=\"data row6 col2\" >6019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14878fc1890>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sa tingin ko need natin 5k sentences minimum for Relevant Sentences di lang for gathered.\n",
    "Kasi mamaya 5k Random Sentences nakuha natin tas 100 lang dun Relevant with candidate & state.\n",
    "\n",
    "Ang naiisip ko since meron 6 Combinations = 3 candidate * 2 state\n",
    "Gawin natin 5000/6 = 834 Relevant Sentences required set natin as minimum per Combination\n",
    "\n",
    "Trump  - Arizona      = 834 Relevant Sentences\n",
    "Harris - Arizona      = 834 Relevant Sentences\n",
    "Trump  - Michigan     = 834 Relevant Sentences\n",
    "Harris - Michigan     = 834 Relevant Sentences\n",
    "Trump  - Pennsylvania = 834 Relevant Sentences\n",
    "Harris - Pennsylvania = 834 Relevant Sentences\n",
    "               -------------------------------\n",
    "               Total: ~5000 Relevant Sentences\n",
    "\"\"\"\n",
    "def print_statistics():\n",
    "    try:\n",
    "        grouped_df = (\n",
    "            list_of_relevant_sentences\n",
    "            .groupby([\"Presidential_Candidate\", \"State\"])\n",
    "            .size()\n",
    "            .reset_index(name=\"count\")\n",
    "        )\n",
    "        total_count = grouped_df[\"count\"].sum()\n",
    "        total_row = pd.DataFrame({\"Presidential_Candidate\": [\"\"], \"State\": [\"Total\"], \"count\": [total_count]})\n",
    "        grouped_df = pd.concat([grouped_df, total_row], ignore_index=True)\n",
    "        return grouped_df.style.hide(axis=\"index\")\n",
    "    except: return \"No Relevant Sentences\"\n",
    "print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T00:14:46.439334Z",
     "start_time": "2024-11-09T00:14:46.435975Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
