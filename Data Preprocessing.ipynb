{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-11-09T00:09:37.046049Z",
     "start_time": "2024-11-09T00:09:35.691753Z"
    }
   },
   "source": [
    "# Import Dependencies\n",
    "import os, re, nltk\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Additional Downloads\n",
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Define Utilities"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-11-09T00:09:37.064679Z",
     "start_time": "2024-11-09T00:09:37.051553Z"
    }
   },
   "source": [
    "def sanitize_filename(filename: str) -> str:\n",
    "    # Escape Double Quotes\n",
    "    filename = filename.replace('\"', '\\\\\"')\n",
    "\n",
    "    # Replace Invalid Characters with \"_\"\n",
    "    invalid_chars = re.compile(r'[<>:\"/\\\\|?*]')\n",
    "    sanitized_filename = invalid_chars.sub(\"_\", filename)\n",
    "\n",
    "    return sanitized_filename\n",
    "    \n",
    "def read_unique_items_from_file(file: str) -> list:\n",
    "    if os.path.exists(file):\n",
    "        with open(file, \"r\", errors=\"ignore\") as f:\n",
    "            return list(set(e.strip() for e in f.readlines() if e.strip()))\n",
    "    return []"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Set Configurations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-11-09T00:17:16.273357Z",
     "start_time": "2024-11-09T00:17:16.252982Z"
    }
   },
   "source": [
    "# File Names\n",
    "transcript_sentences_filename = \"transcript_sentences.csv\"\n",
    "relevant_transcript_sentences_filename = \"relevant_transcript_sentences.csv\"\n",
    "\n",
    "# Folder Names\n",
    "transcription_path = \"Transcription\"\n",
    "cities_transcription_paths = {\n",
    "    \"Michigan\": os.path.join(transcription_path, \"Michigan\"),\n",
    "    \"Arizona\": os.path.join(transcription_path, \"Arizona\"),\n",
    "    \"Pennsylvania\": os.path.join(transcription_path, \"Pennsylvania\"),\n",
    "}\n",
    "cities_path = \"State Cities\"\n",
    "\n",
    "# Numeric Constants \n",
    "max_pair_of_words_for_topic = 2\n",
    "\"\"\"\n",
    "    > Maximum words to consider for topic extraction\n",
    "        1: Unigram (e.g., \"Donald\")\n",
    "        2: Bigram (e.g., \"Donald Trump\")\n",
    "\"\"\"\n",
    "\n",
    "min_number_of_word_in_relevant_sentence = 5\n",
    "\"\"\"\n",
    "    > Minimum word count required for a sentence to be considered relevant\n",
    "    Example: \"This is a nice place\" = 5 words\n",
    "\"\"\"\n",
    "\n",
    "min_similarity_of_topic_modeling = 0.7\n",
    "\"\"\"\n",
    "    > Minimum similarity threshold for topic matching\n",
    "        Range: [0.1, 1.0]\n",
    "    Note: Higher values require closer matches\n",
    "    Example: 0.7 = 70% similarity required\n",
    "\"\"\"\n",
    "\n",
    "max_topic_count = None\n",
    "\"\"\"\n",
    "    Topic count limiter for dimensionality reduction\n",
    "        None: No reduction, keep all discovered topics\n",
    "        \"auto\": Automatically Reduces Topic Count\n",
    "        Number: Force reduce to specified number of topics\n",
    "    Note: (1) Using Number for Numeric reduction may merge unrelated topics together\n",
    "          (2) Lower Number may increase precision but risk missing relevant topics\n",
    "\"\"\"\n",
    "\n",
    "# Sentence Categories\n",
    "presidential_candidates = {\n",
    "    \"Donald Trump\": [\n",
    "        \"Donald\", \"Trump\",\n",
    "        \"Trump Donald\", \"Donald John\", \"John Trump\",\n",
    "        \"Donald J\", \"J. Donald\", \"J. Trump\", \"Trump J\",\n",
    "        \"Trump D\", \"D. Trump\", \"John D\", \"D. John\",\n",
    "        \"Donald T\", \"T. Donald\", \"John T\", \"T. John\",\n",
    "        \"Donald John Trump\", \"Donald J Trump\", \"D. J. Trump\", \n",
    "        \"President Donald\", \"President Trump\",\n",
    "        \"President Donald Trump\"\n",
    "    ],\n",
    "    \"Kamala Harris\": [\n",
    "        \"Kamala\", \"Harris\",\n",
    "        \"Harris Kamala\", \"Kamala Devi\", \"Devi Harris\",\n",
    "        \"Kamala D\", \"D. Kamala\", \"D. Harris\", \"Harris D\",\n",
    "        \"Harris K\", \"K. Harris\", \"Devi K\", \"K. Devi\",\n",
    "        \"Kamala H\", \"H. Kamala\", \"Devi H\", \"H. Devi\",\n",
    "        \"Kamala Devi Harris\", \"Kamala D Harris\", \"K. D. Harris\",  \n",
    "        \"President Kamala\", \"President Harris\",\n",
    "        \"President Kamala Harris\"\n",
    "    ]\n",
    "}\n",
    "original_state_cities = [\"Arizona\", \"Michigan\", \"Pennsylvania\"]\n",
    "state_cities = {\n",
    "    \"Arizona\": read_unique_items_from_file(os.path.join(cities_path, \"arizona-cities.txt\")),\n",
    "    \"Michigan\": read_unique_items_from_file(os.path.join(cities_path, \"michigan-cities.txt\")),\n",
    "    \"Pennsylvania\": read_unique_items_from_file(os.path.join(cities_path, \"pennsylvania-cities.txt\")),\n",
    "    \"Alabama\": [\"AL\", \"A.L\"],\n",
    "    \"Alaska\": [\"AK\", \"A.K\"],\n",
    "    \"Arkansas\": [\"AR\", \"A.R\"],\n",
    "    \"California\": [\"CA\", \"C.A\"],\n",
    "    \"Colorado\": [\"CO\", \"C.O\"],\n",
    "    \"Connecticut\": [\"CT\", \"C.T\"],\n",
    "    \"Delaware\": [\"DE\", \"D.E\"],\n",
    "    \"Florida\": [\"FL\", \"F.L\"],\n",
    "    \"Georgia\": [\"GA\", \"G.A\"],\n",
    "    \"Hawaii\": [\"HI\", \"H.I\"],\n",
    "    \"Idaho\": [\"ID\", \"I.D\"],\n",
    "    \"Illinois\": [\"IL\", \"I.L\"],\n",
    "    \"Indiana\": [\"IN\", \"I.N\"],\n",
    "    \"Iowa\": [\"IA\", \"I.A\"],\n",
    "    \"Kansas\": [\"KS\", \"K.S\"],\n",
    "    \"Kentucky\": [\"KY\", \"K.Y\"],\n",
    "    \"Louisiana\": [\"LA\", \"L.A\"],\n",
    "    \"Maine\": [\"ME\", \"M.E\"],\n",
    "    \"Maryland\": [\"MD\", \"M.D\"],\n",
    "    \"Massachusetts\": [\"MA\", \"M.A\"],\n",
    "    \"Minnesota\": [\"MN\", \"M.N\"],\n",
    "    \"Mississippi\": [\"MS\", \"M.S\"],\n",
    "    \"Missouri\": [\"MO\", \"M.O\"],\n",
    "    \"Montana\": [\"MT\", \"M.T\"],\n",
    "    \"Nebraska\": [\"NE\", \"N.E\"],\n",
    "    \"Nevada\": [\"NV\", \"N.V\"],\n",
    "    \"New Hampshire\": [\"NH\", \"N.H\"],\n",
    "    \"New Jersey\": [\"NJ\", \"N.J\"],\n",
    "    \"New Mexico\": [\"NM\", \"N.M\"],\n",
    "    \"New York\": [\"NY\", \"N.Y\"],\n",
    "    \"North Carolina\": [\"NC\", \"N.C\"],\n",
    "    \"North Dakota\": [\"ND\", \"N.D\"],\n",
    "    \"Ohio\": [\"OH\", \"O.H\"],\n",
    "    \"Oklahoma\": [\"OK\", \"O.K\"],\n",
    "    \"Oregon\": [\"OR\", \"O.R\"],\n",
    "    \"Rhode Island\": [\"RI\", \"R.I\"],\n",
    "    \"South Carolina\": [\"SC\", \"S.C\"],\n",
    "    \"South Dakota\": [\"SD\", \"S.D\"],\n",
    "    \"Tennessee\": [\"TN\", \"T.N\"],\n",
    "    \"Texas\": [\"TX\", \"T.X\"],\n",
    "    \"Utah\": [\"UT\", \"U.T\"],\n",
    "    \"Vermont\": [\"VT\", \"V.T\"],\n",
    "    \"Virginia\": [\"VA\", \"V.A\"],\n",
    "    \"Washington\": [\"WA\", \"W.A\"],\n",
    "    \"West Virginia\": [\"WV\", \"W.V\"],\n",
    "    \"Wisconsin\": [\"WI\", \"W.I\"],\n",
    "    \"Wyoming\": [\"WY\", \"W.Y\"],\n",
    "}\n",
    "\n",
    "# Words for Sentence Filtering\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Additional Preprocessing of Configurations\n",
    "presidential_candidates = {presidential_candidate: list(set(names)) for presidential_candidate, names in presidential_candidates.items()}\n",
    "presidential_candidates_combinations = [\n",
    "    name.lower()\n",
    "    for full_name, names in presidential_candidates.items()\n",
    "    for name in ([full_name] if len(full_name.split()) <= max_pair_of_words_for_topic else []) + [\n",
    "        name for name in names\n",
    "        if len(name.split()) <= max_pair_of_words_for_topic\n",
    "    ]\n",
    "]\n",
    "presidential_candidates_combinations_in_2d = [\n",
    "    ([full_name.lower()] if len(full_name.split()) <= max_pair_of_words_for_topic else []) + [\n",
    "        name.lower()\n",
    "        for name in names\n",
    "        if len(name.split()) <= max_pair_of_words_for_topic\n",
    "    ]\n",
    "    for full_name, names in presidential_candidates.items()\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Sentence Extraction (Transcripts to CSV)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-11-09T00:09:38.969010Z",
     "start_time": "2024-11-09T00:09:37.125594Z"
    }
   },
   "source": [
    "def process_transcripts_into_csv_of_sentences() -> pd.DataFrame:\n",
    "    # Initialize list of sentences and possible states\n",
    "    list_of_sentences = []\n",
    "\n",
    "    # Collect sentences from each state's transcription files\n",
    "    for state, path in cities_transcription_paths.items():\n",
    "        transcription_files = os.listdir(path)\n",
    "        total_transcription_files = len(transcription_files)\n",
    "\n",
    "        with tqdm(total=total_transcription_files, desc=f'Collecting Sentences for {state} [0/{total_transcription_files} Transcript]') as pbar:\n",
    "            for index, filename in enumerate(transcription_files):\n",
    "                current = f'{index + 1}/{total_transcription_files}'\n",
    "                if filename == \".ipynb_checkpoints\":\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                pbar.set_description(f'Collecting Sentences for {state} [{current} Transcript]')\n",
    "\n",
    "                # Open transcription file\n",
    "                file_path = os.path.join(path, filename)\n",
    "                with open(file_path, \"r\", errors=\"ignore\") as file:\n",
    "                    transcription = file.read()\n",
    "\n",
    "                    # Split transcript into sentences\n",
    "                    sentences = sent_tokenize(transcription)\n",
    "\n",
    "                    # Remove consecutive duplicates\n",
    "                    sentences = [sentence for i, sentence in enumerate(sentences) if i == 0 or sentence != sentences[i - 1]]\n",
    "\n",
    "                    # Append each sentence with the state name\n",
    "                    list_of_sentences.extend([(sentence, state) for sentence in sentences])\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Convert the list of sentences and states into a DataFrame\n",
    "    df = pd.DataFrame(list(set(list_of_sentences)), columns=[\"Sentence\", \"Possible_State\"])\n",
    "    df.to_csv(transcript_sentences_filename, index=False, errors=\"ignore\")\n",
    "    return df\n",
    "\n",
    "# Run the function and print summary\n",
    "list_of_sentences = process_transcripts_into_csv_of_sentences()\n",
    "print(f'Number of Sentences: {len(list_of_sentences)}')\n",
    "list_of_sentences"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting Sentences for Michigan [0/260 Transcript]:   0%|          | 0/260 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "625381f4c3974f938848e1c31b62c0dc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Collecting Sentences for Arizona [0/168 Transcript]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "928ed5337e46471485bb559dad37ae0b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Collecting Sentences for Pennsylvania [0/268 Transcript]:   0%|          | 0/268 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "37ec4146bcb5401f8eae0f77f8e61070"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences: 56797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                Sentence Possible_State\n",
       "0      Ladies and gentlemen, this is a remarkable tim...       Michigan\n",
       "1             And I think there's pushback against that.       Michigan\n",
       "2      Yeah, so I agree with Schmidt that it's danger...        Arizona\n",
       "3      You had libertarians on the ballot and yeah, t...        Arizona\n",
       "4      Clean energy focus keeps Washington reliably b...        Arizona\n",
       "...                                                  ...            ...\n",
       "56792  There are thousands and thousands of illegal i...       Michigan\n",
       "56793                                         Off by 12.   Pennsylvania\n",
       "56794  Karl yesterday Michael Watley who has the RNC ...        Arizona\n",
       "56795  I think it's going to be the most important el...       Michigan\n",
       "56796         I am not saying that she will win Arizona.        Arizona\n",
       "\n",
       "[56797 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Possible_State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ladies and gentlemen, this is a remarkable tim...</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>And I think there's pushback against that.</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yeah, so I agree with Schmidt that it's danger...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You had libertarians on the ballot and yeah, t...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clean energy focus keeps Washington reliably b...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56792</th>\n",
       "      <td>There are thousands and thousands of illegal i...</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56793</th>\n",
       "      <td>Off by 12.</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56794</th>\n",
       "      <td>Karl yesterday Michael Watley who has the RNC ...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56795</th>\n",
       "      <td>I think it's going to be the most important el...</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56796</th>\n",
       "      <td>I am not saying that she will win Arizona.</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56797 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# BERTopic: Relevant Sentence Filtering (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-11-09T00:14:45.691850Z",
     "start_time": "2024-11-09T00:09:39.005027Z"
    }
   },
   "source": [
    "def filter_relevant_sentences() -> tuple[DataFrame, BERTopic]:\n",
    "    # Get All Collected Sentences from Transcript and a Map with their Respective Possible State\n",
    "    df = pd.read_csv(transcript_sentences_filename, encoding_errors=\"ignore\")\n",
    "    sentences_possible_state = pd.Series(df['Possible_State'].values, index=df['Sentence']).to_dict()\n",
    "    sentences = df[\"Sentence\"].tolist()\n",
    "    \n",
    "    # Define Filter for Words as Possible Topics\n",
    "    def filter_possible_topics(text: str) -> list:\n",
    "        \"\"\"\n",
    "            Filter Words If it's a Possible Topic:\n",
    "                1) Only Nouns and Proper Nouns (e.g. Dollars, Currency)\n",
    "                2) No Stop Words (e.g. in, to)\n",
    "                3) Minimum of Two-Letter Words (e.g. Ox)\n",
    "                4) Exclude Numbers\n",
    "        \"\"\"\n",
    "        pos_tags = pos_tag(word_tokenize(text)) # POS Tagging\n",
    "        # Return Possible Topics\n",
    "        return [\n",
    "            token.lower() for token, pos in pos_tags\n",
    "            if pos in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"] # Nouns / Proper Nouns\n",
    "            and token.lower() not in stop_words # Exclude Stop Words\n",
    "            and len(token) > 1 # Exclude One-Letter Words (e.g. Included: Ox)\n",
    "            and not token.isnumeric() # Exclude Numbers\n",
    "        ]\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        ngram_range=(1, max_pair_of_words_for_topic),\n",
    "        tokenizer=filter_possible_topics\n",
    "    )\n",
    "\n",
    "    # Train BERTopic model\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        n_gram_range=(1, max_pair_of_words_for_topic),\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        seed_topic_list=presidential_candidates_combinations_in_2d,\n",
    "        zeroshot_topic_list=presidential_candidates_combinations,\n",
    "        zeroshot_min_similarity=min_similarity_of_topic_modeling,\n",
    "        nr_topics=None if max_topic_count is None else \"auto\" if max_topic_count == \"auto\" else max(len(presidential_candidates_combinations), max_topic_count),\n",
    "        verbose=True\n",
    "    )\n",
    "    topic_ids, _ = topic_model.fit_transform(sentences)\n",
    "    \n",
    "    # Get BERTopic Results\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    topics_and_documents = pd.DataFrame({\"Topic\": topic_ids, \"Representative_Docs\": sentences})\n",
    "\n",
    "    # Initialize Lists for Relevant Sentences\n",
    "    list_of_relevant_sentences = []\n",
    "    \n",
    "    # Define Filters for Relevant Sentences\n",
    "    \"\"\"\n",
    "        Add Relevant Sentences Only If:\n",
    "            1) Only 1 Candidate is Mentioned in the Topic\n",
    "            2) No Other State is Mentioned in the Topic Different from Possible State\n",
    "            3) Sentence has Word Count Greater than N or 5\n",
    "    \"\"\"\n",
    "    def get_only_if_1_candidate_mentioned_in_the_topic(topic_ngramed_keywords: list[str]) ->  str | None:\n",
    "        # Collect Candidate Mentions in Topics\n",
    "        presidential_candidate_mentions = set() # Avoid Duplicates\n",
    "        for presidential_candidate, names in presidential_candidates.items():\n",
    "            if (\n",
    "                # Any Candidate is Mentioned in Topic\n",
    "                any(\n",
    "                    (\n",
    "                        presidential_candidate and ngramed_keyword\n",
    "                        and f' {presidential_candidate.strip().lower()} ' in f' {ngramed_keyword.strip().lower()} '\n",
    "                    ) or (\n",
    "                        presidential_candidate and word\n",
    "                        and presidential_candidate.strip().lower() == word.strip().lower()\n",
    "                    )\n",
    "                    for ngramed_keyword in topic_ngramed_keywords\n",
    "                    for word in ngramed_keyword.split(\" \")\n",
    "                )\n",
    "                # Any Other Candidate Names is Mentioned in Topic\n",
    "                or any(\n",
    "                    (\n",
    "                        name and ngramed_keyword\n",
    "                        and f' {name.strip().lower()} ' in f' {ngramed_keyword.strip().lower()} '\n",
    "                    ) or (\n",
    "                        name and word\n",
    "                        and name.strip().lower() == word.strip().lower()\n",
    "                    )\n",
    "                    for name in names\n",
    "                    for ngramed_keyword in topic_ngramed_keywords\n",
    "                    for word in ngramed_keyword.split(\" \")\n",
    "                )\n",
    "            ):\n",
    "                # Add The Candidate Mentioned\n",
    "                presidential_candidate_mentions.add(presidential_candidate)\n",
    "        # Return the Candidate If It's the Only 1 Mentioned\n",
    "        if len(presidential_candidate_mentions) == 1:\n",
    "            return presidential_candidate_mentions.pop()\n",
    "        else:\n",
    "            return None\n",
    "    def get_if_no_other_state_mentioned_in_topic_different_from_possible_state(topic_ngramed_keywords: list[str], sentence: str) ->  str | None:\n",
    "        # Get Possible State for the Sentence\n",
    "        possible_state = sentences_possible_state[sentence]\n",
    "        if possible_state not in state_cities: raise ValueError(f'This Sentence has Invalid Possible State ({possible_state}): \"{sentence}\"')\n",
    "        # Filter Sentence with Topic of [Other State] Not in [Arizona, Michigan, Pennsylvania]\n",
    "        if possible_state not in original_state_cities: return None\n",
    "        # Filter Sentence with Topic of [Other State] Different from its [Possible State]\n",
    "        other_states = [state for state in state_cities if state is not possible_state]\n",
    "        if any(\n",
    "            f' {other_state.strip().lower()} ' in f' {ngramed_keyword.strip().lower()} '\n",
    "            or (\n",
    "                word\n",
    "                and other_state.strip().lower() == word.strip().lower()\n",
    "            )\n",
    "            for other_state in other_states\n",
    "            for ngramed_keyword in topic_ngramed_keywords\n",
    "            for word in ngramed_keyword.split(\" \")\n",
    "        ): return None\n",
    "        # Filter Sentence with Topics of [Other States' Cities] Different from its [Possible State Cities]\n",
    "        other_state_cities = [\n",
    "            other_city\n",
    "            for other_cities in {\n",
    "                state: state_cities[state]\n",
    "                for state in state_cities\n",
    "                if state is not possible_state\n",
    "            }.values()\n",
    "            for other_city in other_cities\n",
    "            if other_city\n",
    "        ]\n",
    "        if any(\n",
    "            f' {other_city.strip().lower()} ' in f' {ngramed_keyword.strip().lower()} '\n",
    "            or (\n",
    "                word\n",
    "                and other_city.strip().lower() == word.strip().lower()\n",
    "            )\n",
    "            for other_city in other_state_cities\n",
    "            for ngramed_keyword in topic_ngramed_keywords\n",
    "            for word in ngramed_keyword.split(\" \")\n",
    "        ): return None\n",
    "        # Return the Possible State\n",
    "        return possible_state\n",
    "    def sentence_has_word_count_greater_than_n(sentence: str, min_number_of_word_in_relevant_sentence: int = min_number_of_word_in_relevant_sentence) -> bool:\n",
    "        # Only include word tags\n",
    "        word_tags = {\n",
    "            \"CC\",  # conjunctions (and, or, but)\n",
    "            \"CD\",  # cardinal numbers\n",
    "            \"DT\",  # determiners (the, a, this)\n",
    "            \"EX\",  # existential there\n",
    "            \"FW\",  # foreign words\n",
    "            \"IN\",  # prepositions\n",
    "            \"JJ\", \"JJR\", \"JJS\",  # adjectives\n",
    "            \"LS\",  # List markers (First, Second, One, Two, A, B, etc.)\n",
    "            \"MD\",  # modals (can, should)\n",
    "            \"NN\", \"NNP\", \"NNPS\", \"NNS\",  # nouns\n",
    "            \"PDT\",  # pre-determiners\n",
    "            \"PRP\", \"PRP$\",  # pronouns\n",
    "            \"RB\", \"RBR\", \"RBS\",  # adverbs\n",
    "            \"RP\",  # particles\n",
    "            \"TO\",  # to\n",
    "            \"UH\",  # interjections\n",
    "            \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\",  # verbs\n",
    "            \"WDT\", \"WP\", \"WP$\", \"WRB\"  # wh-words\n",
    "        }\n",
    "        pos_tags = pos_tag(word_tokenize(sentence)) # POS Tagging\n",
    "        word_count = sum(1 for word, pos in pos_tags if pos in word_tags)\n",
    "        return word_count >= min_number_of_word_in_relevant_sentence\n",
    "\n",
    "    # Get Relevant Sentences\n",
    "    for _, row in topic_info.iterrows():\n",
    "        topic_id = row[\"Topic\"]\n",
    "        if topic_id == -1: continue # Skip Outlier\n",
    "    \n",
    "        # Get List of Topics and their Sentences\n",
    "        topic_ngramed_keywords = [\n",
    "            ngramed_keyword \n",
    "            for ngramed_keyword in row[\"Representation\"]\n",
    "            if ngramed_keyword\n",
    "        ]\n",
    "        topic_sentences = topics_and_documents[topics_and_documents[\"Topic\"] == topic_id][\"Representative_Docs\"].tolist()\n",
    "        \n",
    "        for sentence in topic_sentences:\n",
    "            # Check and Get 1 Candidate from Topics\n",
    "            presidential_candidate = get_only_if_1_candidate_mentioned_in_the_topic(topic_ngramed_keywords)\n",
    "            if presidential_candidate is None: continue\n",
    "            \n",
    "            # Check and Get 1 State from Topics and [Possible State assigned in Sentence] \n",
    "            state = get_if_no_other_state_mentioned_in_topic_different_from_possible_state(topic_ngramed_keywords, sentence)\n",
    "            if state is None: continue\n",
    "            \n",
    "            # Check if sentence has word count greater than N (default: 5)\n",
    "            if not sentence_has_word_count_greater_than_n(sentence): continue\n",
    "            \n",
    "            # Add Relevant Sentence with their Respective Candidate and State\n",
    "            list_of_relevant_sentences.append({\n",
    "                \"Sentence\": sentence,\n",
    "                \"Presidential_Candidate\": presidential_candidate,\n",
    "                \"State\": state,\n",
    "                \"Topic_Keywords\": topic_ngramed_keywords\n",
    "            })\n",
    "    \n",
    "    # Save List of All Relevant Sentences into CSV file\n",
    "    df = pd.DataFrame(list_of_relevant_sentences)\n",
    "    df.to_csv(relevant_transcript_sentences_filename, index=False, errors=\"ignore\")\n",
    "    return df, topic_model\n",
    "\n",
    "list_of_relevant_sentences, bertopic_model = filter_relevant_sentences()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 08:09:39,519 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1775 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49861016efae41e3ad6d0a916aca11e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 08:12:01,102 - BERTopic - Embedding - Completed ✓\n",
      "2024-11-09 08:12:01,102 - BERTopic - Guided - Find embeddings highly related to seeded topics.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc2dfa2ce7ff4fa19bb3ea3eb7638a3f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 08:12:01,442 - BERTopic - Guided - Completed ✓\n",
      "2024-11-09 08:12:01,443 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-11-09 08:12:47,271 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-11-09 08:12:47,318 - BERTopic - Zeroshot Step 1 - Finding documents that could be assigned to either one of the zero-shot topics\n",
      "2024-11-09 08:12:52,803 - BERTopic - Zeroshot Step 1 - Completed ✓\n",
      "2024-11-09 08:13:13,420 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-11-09 08:13:18,155 - BERTopic - Cluster - Completed ✓\n",
      "2024-11-09 08:13:18,155 - BERTopic - Zeroshot Step 2 - Combining topics from zero-shot topic modeling with topics from clustering...\n",
      "2024-11-09 08:13:18,288 - BERTopic - Zeroshot Step 2 - Completed ✓\n",
      "2024-11-09 08:13:18,288 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-11-09 08:13:43,788 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-11-09T00:14:46.016257Z",
     "start_time": "2024-11-09T00:14:45.981201Z"
    }
   },
   "source": "bertopic_model.get_topic_info().sort_values(by=\"Count\", ascending=False)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     Topic  Count                                               Name  \\\n",
       "0       -1  24091                      -1_harris_trump_people_states   \n",
       "85      84   2074                84_interview_momentum_policy_border   \n",
       "275    274   1144                274_biden_joe biden_joe_biden biden   \n",
       "338    337    812                         337_guy_character_man_blah   \n",
       "249    248    635  248_arizona_state arizona_arizona arizona_ariz...   \n",
       "..     ...    ...                                                ...   \n",
       "6        5      1                             president donald trump   \n",
       "15      14      1                                    kamala d harris   \n",
       "18      17      1                                          kamala d.   \n",
       "20      19      1                                          d. kamala   \n",
       "9        8      1                                     donald j trump   \n",
       "\n",
       "                                        Representation  \\\n",
       "0    [harris, trump, people, states, state, electio...   \n",
       "85   [interview, momentum, policy, border, intervie...   \n",
       "275  [biden, joe biden, joe, biden biden, president...   \n",
       "338  [guy, character, man, blah, tangent, blah blah...   \n",
       "249  [arizona, state arizona, arizona arizona, ariz...   \n",
       "..                                                 ...   \n",
       "6                            [trump, , , , , , , , , ]   \n",
       "15             [harris harris, harris, , , , , , , , ]   \n",
       "18   [benefit population, kamala benefit, populatio...   \n",
       "20             [kamala kamala, kamala, , , , , , , , ]   \n",
       "9                            [trump, , , , , , , , , ]   \n",
       "\n",
       "                                   Representative_Docs  \n",
       "0    [However, Trump has turned this down., They we...  \n",
       "85   [Does she have the momentum that she needs to ...  \n",
       "275  [You know, we talk about Joe Biden coming., An...  \n",
       "338  [That's a great guy., I think he's a good guy....  \n",
       "249  [And I will add, I'm in Arizona., And that sta...  \n",
       "..                                                 ...  \n",
       "6                             [There's a Trump Trump.]  \n",
       "15   [If I were Kamala Harris, I would have stuck w...  \n",
       "18   [I think Kamala would be a benefit to my Nativ...  \n",
       "20                   [Kamala, kamala, kamala, kamala.]  \n",
       "9                           [That is Donald J. Trump.]  \n",
       "\n",
       "[600 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>24091</td>\n",
       "      <td>-1_harris_trump_people_states</td>\n",
       "      <td>[harris, trump, people, states, state, electio...</td>\n",
       "      <td>[However, Trump has turned this down., They we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>84</td>\n",
       "      <td>2074</td>\n",
       "      <td>84_interview_momentum_policy_border</td>\n",
       "      <td>[interview, momentum, policy, border, intervie...</td>\n",
       "      <td>[Does she have the momentum that she needs to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>274</td>\n",
       "      <td>1144</td>\n",
       "      <td>274_biden_joe biden_joe_biden biden</td>\n",
       "      <td>[biden, joe biden, joe, biden biden, president...</td>\n",
       "      <td>[You know, we talk about Joe Biden coming., An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>337</td>\n",
       "      <td>812</td>\n",
       "      <td>337_guy_character_man_blah</td>\n",
       "      <td>[guy, character, man, blah, tangent, blah blah...</td>\n",
       "      <td>[That's a great guy., I think he's a good guy....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>248</td>\n",
       "      <td>635</td>\n",
       "      <td>248_arizona_state arizona_arizona arizona_ariz...</td>\n",
       "      <td>[arizona, state arizona, arizona arizona, ariz...</td>\n",
       "      <td>[And I will add, I'm in Arizona., And that sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>president donald trump</td>\n",
       "      <td>[trump, , , , , , , , , ]</td>\n",
       "      <td>[There's a Trump Trump.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>kamala d harris</td>\n",
       "      <td>[harris harris, harris, , , , , , , , ]</td>\n",
       "      <td>[If I were Kamala Harris, I would have stuck w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>kamala d.</td>\n",
       "      <td>[benefit population, kamala benefit, populatio...</td>\n",
       "      <td>[I think Kamala would be a benefit to my Nativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>d. kamala</td>\n",
       "      <td>[kamala kamala, kamala, , , , , , , , ]</td>\n",
       "      <td>[Kamala, kamala, kamala, kamala.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>donald j trump</td>\n",
       "      <td>[trump, , , , , , , , , ]</td>\n",
       "      <td>[That is Donald J. Trump.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-11-09T00:14:46.100346Z",
     "start_time": "2024-11-09T00:14:46.084310Z"
    }
   },
   "source": "list_of_relevant_sentences",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               Sentence  \\\n",
       "0               Donald Trump and his running mate, J.D.   \n",
       "1                  President Trump, he's a businessman.   \n",
       "2                                There's a Trump Trump.   \n",
       "3                              That is Donald J. Trump.   \n",
       "4     So in this next election, Kamala Harris, she's...   \n",
       "...                                                 ...   \n",
       "5425                          You don't agree with him.   \n",
       "5426                         I'm not going to disagree.   \n",
       "5427         I don't think I agree with a single thing.   \n",
       "5428                        I don't agree with my kids.   \n",
       "5429                             I don't agree with it.   \n",
       "\n",
       "     Presidential_Candidate         State  \\\n",
       "0              Donald Trump       Arizona   \n",
       "1              Donald Trump      Michigan   \n",
       "2              Donald Trump  Pennsylvania   \n",
       "3              Donald Trump       Arizona   \n",
       "4             Kamala Harris  Pennsylvania   \n",
       "...                     ...           ...   \n",
       "5425           Donald Trump      Michigan   \n",
       "5426           Donald Trump      Michigan   \n",
       "5427           Donald Trump      Michigan   \n",
       "5428           Donald Trump  Pennsylvania   \n",
       "5429           Donald Trump      Michigan   \n",
       "\n",
       "                                         Topic_Keywords  \n",
       "0     [j.d j.d, j.d, mate j.d, trump running, runnin...  \n",
       "1     [trump businessman, businessman, president tru...  \n",
       "2                                               [trump]  \n",
       "3                                               [trump]  \n",
       "4     [harris harris, harris kamala, harris, kamala ...  \n",
       "...                                                 ...  \n",
       "5425  [eyes percent, thing kids, opinion ace, look e...  \n",
       "5426  [eyes percent, thing kids, opinion ace, look e...  \n",
       "5427  [eyes percent, thing kids, opinion ace, look e...  \n",
       "5428  [eyes percent, thing kids, opinion ace, look e...  \n",
       "5429  [eyes percent, thing kids, opinion ace, look e...  \n",
       "\n",
       "[5430 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Presidential_Candidate</th>\n",
       "      <th>State</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump and his running mate, J.D.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>[j.d j.d, j.d, mate j.d, trump running, runnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>President Trump, he's a businessman.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>[trump businessman, businessman, president tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There's a Trump Trump.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[trump]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>That is Donald J. Trump.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>[trump]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So in this next election, Kamala Harris, she's...</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[harris harris, harris kamala, harris, kamala ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5425</th>\n",
       "      <td>You don't agree with him.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>[eyes percent, thing kids, opinion ace, look e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5426</th>\n",
       "      <td>I'm not going to disagree.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>[eyes percent, thing kids, opinion ace, look e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5427</th>\n",
       "      <td>I don't think I agree with a single thing.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>[eyes percent, thing kids, opinion ace, look e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5428</th>\n",
       "      <td>I don't agree with my kids.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[eyes percent, thing kids, opinion ace, look e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5429</th>\n",
       "      <td>I don't agree with it.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>[eyes percent, thing kids, opinion ace, look e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5430 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-11-09T00:14:46.361934Z",
     "start_time": "2024-11-09T00:14:46.259140Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Sa tingin ko need natin 5k sentences minimum for Relevant Sentences di lang for gathered.\n",
    "Kasi mamaya 5k Random Sentences nakuha natin tas 100 lang dun Relevant with candidate & state.\n",
    "\n",
    "Ang naiisip ko since meron 6 Combinations = 3 candidate * 2 state\n",
    "Gawin natin 5000/6 = 834 Relevant Sentences required set natin as minimum per Combination\n",
    "\n",
    "Trump  - Arizona      = 834 Relevant Sentences\n",
    "Harris - Arizona      = 834 Relevant Sentences\n",
    "Trump  - Michigan     = 834 Relevant Sentences\n",
    "Harris - Michigan     = 834 Relevant Sentences\n",
    "Trump  - Pennsylvania = 834 Relevant Sentences\n",
    "Harris - Pennsylvania = 834 Relevant Sentences\n",
    "               -------------------------------\n",
    "               Total: ~5000 Relevant Sentences\n",
    "\"\"\"\n",
    "def print_statistics():\n",
    "    try:\n",
    "        grouped_df = (\n",
    "            list_of_relevant_sentences\n",
    "            .groupby([\"Presidential_Candidate\", \"State\"])\n",
    "            .size()\n",
    "            .reset_index(name=\"count\")\n",
    "        )\n",
    "        total_count = grouped_df[\"count\"].sum()\n",
    "        total_row = pd.DataFrame({\"Presidential_Candidate\": [\"\"], \"State\": [\"Total\"], \"count\": [total_count]})\n",
    "        grouped_df = pd.concat([grouped_df, total_row], ignore_index=True)\n",
    "        return grouped_df.style.hide(axis=\"index\")\n",
    "    except: return \"No Relevant Sentences\"\n",
    "print_statistics()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1f4a8b04fd0>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_6b905\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_6b905_level0_col0\" class=\"col_heading level0 col0\" >Presidential_Candidate</th>\n",
       "      <th id=\"T_6b905_level0_col1\" class=\"col_heading level0 col1\" >State</th>\n",
       "      <th id=\"T_6b905_level0_col2\" class=\"col_heading level0 col2\" >count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_6b905_row0_col0\" class=\"data row0 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_6b905_row0_col1\" class=\"data row0 col1\" >Arizona</td>\n",
       "      <td id=\"T_6b905_row0_col2\" class=\"data row0 col2\" >957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6b905_row1_col0\" class=\"data row1 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_6b905_row1_col1\" class=\"data row1 col1\" >Michigan</td>\n",
       "      <td id=\"T_6b905_row1_col2\" class=\"data row1 col2\" >1437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6b905_row2_col0\" class=\"data row2 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_6b905_row2_col1\" class=\"data row2 col1\" >Pennsylvania</td>\n",
       "      <td id=\"T_6b905_row2_col2\" class=\"data row2 col2\" >1706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6b905_row3_col0\" class=\"data row3 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_6b905_row3_col1\" class=\"data row3 col1\" >Arizona</td>\n",
       "      <td id=\"T_6b905_row3_col2\" class=\"data row3 col2\" >235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6b905_row4_col0\" class=\"data row4 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_6b905_row4_col1\" class=\"data row4 col1\" >Michigan</td>\n",
       "      <td id=\"T_6b905_row4_col2\" class=\"data row4 col2\" >420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6b905_row5_col0\" class=\"data row5 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_6b905_row5_col1\" class=\"data row5 col1\" >Pennsylvania</td>\n",
       "      <td id=\"T_6b905_row5_col2\" class=\"data row5 col2\" >675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6b905_row6_col0\" class=\"data row6 col0\" ></td>\n",
       "      <td id=\"T_6b905_row6_col1\" class=\"data row6 col1\" >Total</td>\n",
       "      <td id=\"T_6b905_row6_col2\" class=\"data row6 col2\" >5430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T00:14:46.439334Z",
     "start_time": "2024-11-09T00:14:46.435975Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
