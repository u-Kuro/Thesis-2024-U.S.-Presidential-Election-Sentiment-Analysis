{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MSI Laptop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\MSI\n",
      "[nltk_data]     Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\MSI Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\MSI\n",
      "[nltk_data]     Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Main Dependencies\n",
    "import os, re, nltk\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import Other Dependencies\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Additional Downloads\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename: str) -> str:\n",
    "    # Escape Double Quotes\n",
    "    filename = filename.replace('\"', '\\\\\"')\n",
    "\n",
    "    # Replace Invalid Characters with \"_\"\n",
    "    invalid_chars = re.compile(r'[<>:\"/\\\\|?*]')\n",
    "    sanitized_filename = invalid_chars.sub(\"_\", filename)\n",
    "\n",
    "    return sanitized_filename\n",
    "    \n",
    "def read_unique_items_from_file(file: str) -> list:\n",
    "    with open(file, \"r\") as f:\n",
    "        return list(set(url.strip() for url in f.readlines() if url.strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Names\n",
    "transcript_sentences_filename = \"transcript_sentences.csv\"\n",
    "related_transcript_sentences_filename = \"related_transcript_sentences.csv\"\n",
    "\n",
    "# Folder Names\n",
    "transcription_output_path = \"Transcription\"\n",
    "cities_path = \"State Cities\"\n",
    "\n",
    "# Boolean Flags\n",
    "remove_video = True\n",
    "remove_audio = True\n",
    "\n",
    "# Numeric Constants \n",
    "max_consecutive_words_for_topic = 3 # e.g. Unigram: \"Donald\" | Bigram: \"Donald Trump\" | Trigram: \"President Donald Trump\"\n",
    "\n",
    "# Sentence Categories\n",
    "presidential_candidates = {\n",
    "    \"Donald Trump\": [\n",
    "        \"Donald\", \"Trump\"\n",
    "    ],\n",
    "    \"Kamala Harris\": [\n",
    "        \"Kamala\", \"Harris\"\n",
    "    ]\n",
    "}\n",
    "state_cities = {\n",
    "    \"Michigan\": read_unique_items_from_file(os.path.join(cities_path, \"michigan-cities.txt\")),\n",
    "    \"Arizona\": read_unique_items_from_file(os.path.join(cities_path, \"arizona-cities.txt\")),\n",
    "    \"Pennsylvania\": read_unique_items_from_file(os.path.join(cities_path, \"pennsylvania-cities.txt\"))\n",
    "}\n",
    "\n",
    "# Words for Sentence Filtering\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "generic_abstract_nouns = {\n",
    "    \"thing\", \"stuff\", \"event\",\n",
    "    \"aspect\", \"issue\", \"place\",\n",
    "    \"person\"\n",
    "}\n",
    "\n",
    "# Additional Preprocessing of Configurations\n",
    "presidential_candidates = {presidential_candidate: list(set(names)) for presidential_candidate, names in presidential_candidates.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Extraction (Transcripts to CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69851966b6ec4e9ca2f4474194472907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Sentences from Transcripts:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences: 5242\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harris' policies on expanding healthcare acces...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>And so I think that's what you're hearing from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Every time since 2016, Donald Trump, or one of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I think it's dead, but you can never say it be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>And then he comes over with the hundred dollar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence\n",
       "0  Harris' policies on expanding healthcare acces...\n",
       "1  And so I think that's what you're hearing from...\n",
       "2  Every time since 2016, Donald Trump, or one of...\n",
       "3  I think it's dead, but you can never say it be...\n",
       "4  And then he comes over with the hundred dollar..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_transcripts_into_csv_of_sentences() -> DataFrame:\n",
    "    # Initialize List of Sentences\n",
    "    list_of_sentences = []\n",
    "    \n",
    "    def is_sentence_complete(sentence: str) -> bool:\n",
    "        \"\"\"\n",
    "        Its a Proper Sentence If:\n",
    "            1) It has Atleast 1 Noun or Pronoun\n",
    "            2) It has Atleast 1 Verb\n",
    "        \"\"\"\n",
    "        pos_tags = pos_tag(word_tokenize(sentence)) # POS Tagging\n",
    "        has_subject = any(tag in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"] for _, tag in pos_tags) # Exclude Sentence w/out Noun and Pronoun\n",
    "        has_verb = any(tag.startswith(\"VB\") for _, tag in pos_tags) # Exclude Sentence w/out Verb\n",
    "    \n",
    "        return has_subject and has_verb\n",
    "    \n",
    "    # Collect List of All Sentences from Transcripts\n",
    "    transcription_files = os.listdir(transcription_output_path)\n",
    "    with tqdm(total=len(transcription_files), desc=\"Collecting Sentences from Transcripts\") as pbar:\n",
    "        for filename in transcription_files:\n",
    "            file_path = os.path.join(transcription_output_path, filename)\n",
    "            \n",
    "            with open(file_path, \"r\") as file:\n",
    "                text = file.read()\n",
    "                \n",
    "                # Split into Sentences\n",
    "                sentences = list(set(sent_tokenize(text)))\n",
    "    \n",
    "                # Filter Proper Sentences (With Noun/Proper-Noun and Verb)\n",
    "                sentences = [sentence for sentence in sentences if is_sentence_complete(sentence)]\n",
    "                \n",
    "                # Add Sentence to the List\n",
    "                list_of_sentences.extend(sentences)\n",
    "                \n",
    "            pbar.update(1)\n",
    "\n",
    "    # Save List of All Sentences into CSV file\n",
    "    df = pd.DataFrame(list(set(list_of_sentences)), columns=[\"Sentence\"])\n",
    "    df.to_csv(transcript_sentences_filename, index=False)\n",
    "    return df\n",
    "\n",
    "list_of_sentences = process_transcripts_into_csv_of_sentences()\n",
    "print(f'Number of Sentences: {len(list_of_sentences)}')\n",
    "list_of_sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic: Relevant Sentence Filtering (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 16:37:25,446 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8c991842cb4a24940bd25141603b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 16:37:55,997 - BERTopic - Embedding - Completed ✓\n",
      "2024-10-20 16:37:55,997 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-10-20 16:38:07,188 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-10-20 16:38:07,202 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-10-20 16:38:07,623 - BERTopic - Cluster - Completed ✓\n",
      "2024-10-20 16:38:07,670 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-10-20 16:38:13,939 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Related Sentences: 6\n",
      "Number of Topic Clusters: 92\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>2086</td>\n",
       "      <td>-1_trump_election_people_state</td>\n",
       "      <td>[trump, election, people, state, voters, penns...</td>\n",
       "      <td>[You have you have some terrific people., We'v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>233</td>\n",
       "      <td>0_questions_loomer_part_morelos</td>\n",
       "      <td>[questions, loomer, part, morelos, debate, tru...</td>\n",
       "      <td>[And that's why they're expecting to see lots ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>1_trump_president_grievances_president trump</td>\n",
       "      <td>[trump, president, grievances, president trump...</td>\n",
       "      <td>[ We want Trump!, And while most Trump support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>131</td>\n",
       "      <td>2_economy_money_store_inflation</td>\n",
       "      <td>[economy, money, store, inflation, grocery, me...</td>\n",
       "      <td>[Whether it's the economy., What do you think ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>127</td>\n",
       "      <td>3_harris_campaign_harris campaign_favor</td>\n",
       "      <td>[harris, campaign, harris campaign, favor, vic...</td>\n",
       "      <td>[Harris campaign is trying to address that as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>86</td>\n",
       "      <td>11</td>\n",
       "      <td>86_conspiracy_conspiracy theories_theories_con...</td>\n",
       "      <td>[conspiracy, conspiracy theories, theories, co...</td>\n",
       "      <td>[If you talk about it, oh, it's a conspiracy.,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>87</td>\n",
       "      <td>11</td>\n",
       "      <td>87_report_reports_reporting oops report_end re...</td>\n",
       "      <td>[report, reports, reporting oops report, end r...</td>\n",
       "      <td>[So we we call it the end of the report., Oops...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>88</td>\n",
       "      <td>11</td>\n",
       "      <td>88_ryan_tim ryan_tim_democrat</td>\n",
       "      <td>[ryan, tim ryan, tim, democrat, j.d, congressm...</td>\n",
       "      <td>[Tim Ryan, the Democrat nominee, congressman, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>89</td>\n",
       "      <td>11</td>\n",
       "      <td>89_election day_day_election_days election</td>\n",
       "      <td>[election day, day, election, days election, o...</td>\n",
       "      <td>[What happens on Election Day is the most impo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>90_politico_information_cyber_cyber security</td>\n",
       "      <td>[politico, information, cyber, cyber security,...</td>\n",
       "      <td>[The campaign has not provided any other speci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                               Name  \\\n",
       "0      -1   2086                     -1_trump_election_people_state   \n",
       "1       0    233                    0_questions_loomer_part_morelos   \n",
       "2       1    141       1_trump_president_grievances_president trump   \n",
       "3       2    131                    2_economy_money_store_inflation   \n",
       "4       3    127            3_harris_campaign_harris campaign_favor   \n",
       "..    ...    ...                                                ...   \n",
       "87     86     11  86_conspiracy_conspiracy theories_theories_con...   \n",
       "88     87     11  87_report_reports_reporting oops report_end re...   \n",
       "89     88     11                      88_ryan_tim ryan_tim_democrat   \n",
       "90     89     11         89_election day_day_election_days election   \n",
       "91     90     11       90_politico_information_cyber_cyber security   \n",
       "\n",
       "                                       Representation  \\\n",
       "0   [trump, election, people, state, voters, penns...   \n",
       "1   [questions, loomer, part, morelos, debate, tru...   \n",
       "2   [trump, president, grievances, president trump...   \n",
       "3   [economy, money, store, inflation, grocery, me...   \n",
       "4   [harris, campaign, harris campaign, favor, vic...   \n",
       "..                                                ...   \n",
       "87  [conspiracy, conspiracy theories, theories, co...   \n",
       "88  [report, reports, reporting oops report, end r...   \n",
       "89  [ryan, tim ryan, tim, democrat, j.d, congressm...   \n",
       "90  [election day, day, election, days election, o...   \n",
       "91  [politico, information, cyber, cyber security,...   \n",
       "\n",
       "                                  Representative_Docs  \n",
       "0   [You have you have some terrific people., We'v...  \n",
       "1   [And that's why they're expecting to see lots ...  \n",
       "2   [ We want Trump!, And while most Trump support...  \n",
       "3   [Whether it's the economy., What do you think ...  \n",
       "4   [Harris campaign is trying to address that as ...  \n",
       "..                                                ...  \n",
       "87  [If you talk about it, oh, it's a conspiracy.,...  \n",
       "88  [So we we call it the end of the report., Oops...  \n",
       "89  [Tim Ryan, the Democrat nominee, congressman, ...  \n",
       "90  [What happens on Election Day is the most impo...  \n",
       "91  [The campaign has not provided any other speci...  \n",
       "\n",
       "[92 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_related_sentences() -> tuple[DataFrame, BERTopic]:\n",
    "    # Get All Sentences from Transcript\n",
    "    df = pd.read_csv(transcript_sentences_filename)\n",
    "    sentences = df[\"Sentence\"].tolist()\n",
    "    \n",
    "    # Set Filter for Words as Possible Topics\n",
    "    def filter_possible_topics(text: str) -> list:\n",
    "        \"\"\"\n",
    "        Filter Words If its a Possible Topic:\n",
    "            1) Only Nouns and Proper Nouns (e.g. Dollars, Currency)\n",
    "            2) No Stop Words (e.g. in, to)\n",
    "            3) No Generic Abstract Nouns (e.g. thing, stuff)\n",
    "            4) Minumum of Three Letter Words (e.g. USA)\n",
    "            5) Exclude Numbers\n",
    "        \"\"\"\n",
    "        \n",
    "        pos_tags = pos_tag(word_tokenize(text)) # POS Tagging\n",
    "        possible_topics = [\n",
    "            token.lower() for token, pos in pos_tags\n",
    "            if pos in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"] # Nouns / Proper Nouns\n",
    "            and token.lower() not in stop_words # Exclude Stop Words\n",
    "            and token.lower() not in generic_abstract_nouns # Exclude Generic Abstract Nouns\n",
    "            and len(token) > 2 # Exclude One/Two Letter Words\n",
    "            and not token.isnumeric() # Exclude Numbers\n",
    "        ]\n",
    "        \n",
    "        return possible_topics\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        ngram_range=(1, max_consecutive_words_for_topic),\n",
    "        tokenizer=filter_possible_topics\n",
    "    )\n",
    "\n",
    "    # Train BERTopic model\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        n_gram_range=(1, max_consecutive_words_for_topic),\n",
    "        vectorizer_model=vectorizer_model,        \n",
    "        verbose=True\n",
    "    )\n",
    "    topic_model.fit_transform(sentences)\n",
    "    \n",
    "    # Get BERTopic Results\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    \n",
    "    # Initialize Lists for our filtered results\n",
    "    list_of_related_sentences = []\n",
    "    \n",
    "    # Analyze each topic row in topic_info\n",
    "    for _, row in topic_info.iterrows():\n",
    "        if row[\"Topic\"] == -1: continue # Skip Outlier\n",
    "\n",
    "        # Get List of Topics and its Related Sentences\n",
    "        topic_keywords = row[\"Representation\"]\n",
    "        related_sentences = row[\"Representative_Docs\"]\n",
    "        \n",
    "        # Check Candidate Mentions in Topics\n",
    "        presidential_candidate_mentions = set() # Avoid Duplicates\n",
    "        for presidential_candidate, names in presidential_candidates.items():\n",
    "            if (\n",
    "                any(name.lower() in keyword.lower() for name in names for keyword in topic_keywords) \n",
    "                or any(presidential_candidate.lower() in keyword.lower() for keyword in topic_keywords)\n",
    "            ): \n",
    "                presidential_candidate_mentions.add(presidential_candidate)\n",
    "        \n",
    "        # Make Sure Only 1 Candidate is Mentioned\n",
    "        if len(presidential_candidate_mentions) != 1: continue\n",
    "\n",
    "        # Check State Mentions in Topics (Including Cities)\n",
    "        state_mentions = set() # Avoid Duplicates\n",
    "        for state, cities in state_cities.items():\n",
    "            if (\n",
    "                any(city.lower() in keyword.lower() for city in cities for keyword in topic_keywords) \n",
    "                or any(state.lower() in keyword.lower() for keyword in topic_keywords)\n",
    "            ): \n",
    "                state_mentions.add(state)\n",
    "\n",
    "        # Make Sure Only 1 State is Mentioned\n",
    "        if len(presidential_candidate_mentions) != 1: continue\n",
    "        \n",
    "        \"\"\"\n",
    "        Add Related Sentences Only If:\n",
    "            1) Only 1 Candidate is Mentioned\n",
    "            2) Only 1 State is Mentioned\n",
    "        \"\"\"\n",
    "        if len(presidential_candidate_mentions) == 1 and len(state_mentions) == 1:\n",
    "            presidential_candidate = presidential_candidate_mentions.pop()\n",
    "            state = state_mentions.pop()\n",
    "\n",
    "            # Add All Related Sentences with Corresponding Presidential Candidate, State, and Topic Keywords\n",
    "            for sentence in related_sentences:\n",
    "                list_of_related_sentences.append({\n",
    "                    \"Sentence\": sentence,\n",
    "                    \"Presidential_Candidate\": presidential_candidate,\n",
    "                    \"State\": state,\n",
    "                    \"Topic_Keywords\": topic_keywords\n",
    "                })\n",
    "    \n",
    "    # Save List of All Related Sentences into CSV file\n",
    "    df = pd.DataFrame(list_of_related_sentences)\n",
    "    df.to_csv(related_transcript_sentences_filename, index=False)\n",
    "    return df, topic_model\n",
    "\n",
    "list_of_related_sentences, bertopic_model = filter_related_sentences()\n",
    "print(f'Number of Related Sentences: {len(list_of_related_sentences)}')\n",
    "print(f'Number of Topic Clusters: {len(bertopic_model.get_topic_info())}')\n",
    "bertopic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Presidential_Candidate</th>\n",
       "      <th>State</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>At the same time, Trump is such a unique figur...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[pennsylvania, trump pennsylvania, town, every...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Every other pollster, Emerson, Insider Advanta...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[pennsylvania, trump pennsylvania, town, every...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is part of a swing that Donald Trump did ...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[pennsylvania, trump pennsylvania, town, every...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The pushing with Kamala with more current thin...</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[kamala, bit bit, bit bit bit, kamala face, bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So some are even pointing fingers at her campa...</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[kamala, bit bit, bit bit bit, kamala face, bi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Presidential_Candidate  \\\n",
       "0  At the same time, Trump is such a unique figur...           Donald Trump   \n",
       "1  Every other pollster, Emerson, Insider Advanta...           Donald Trump   \n",
       "2  This is part of a swing that Donald Trump did ...           Donald Trump   \n",
       "3  The pushing with Kamala with more current thin...          Kamala Harris   \n",
       "4  So some are even pointing fingers at her campa...          Kamala Harris   \n",
       "\n",
       "          State                                     Topic_Keywords  \n",
       "0  Pennsylvania  [pennsylvania, trump pennsylvania, town, every...  \n",
       "1  Pennsylvania  [pennsylvania, trump pennsylvania, town, every...  \n",
       "2  Pennsylvania  [pennsylvania, trump pennsylvania, town, every...  \n",
       "3  Pennsylvania  [kamala, bit bit, bit bit bit, kamala face, bi...  \n",
       "4  Pennsylvania  [kamala, bit bit, bit bit bit, kamala face, bi...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_related_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_fed3b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_fed3b_level0_col0\" class=\"col_heading level0 col0\" >Presidential_Candidate</th>\n",
       "      <th id=\"T_fed3b_level0_col1\" class=\"col_heading level0 col1\" >State</th>\n",
       "      <th id=\"T_fed3b_level0_col2\" class=\"col_heading level0 col2\" >count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_fed3b_row0_col0\" class=\"data row0 col0\" >Donald Trump</td>\n",
       "      <td id=\"T_fed3b_row0_col1\" class=\"data row0 col1\" >Pennsylvania</td>\n",
       "      <td id=\"T_fed3b_row0_col2\" class=\"data row0 col2\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fed3b_row1_col0\" class=\"data row1 col0\" >Kamala Harris</td>\n",
       "      <td id=\"T_fed3b_row1_col1\" class=\"data row1 col1\" >Pennsylvania</td>\n",
       "      <td id=\"T_fed3b_row1_col2\" class=\"data row1 col2\" >3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x244d642c250>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sa tingin ko need natin 5k sentences minimum for Related di lang for gathered.\n",
    "Kasi mamaya 5k Unrelated Sentences nakuha natin tas 100 lang dun Related with candidate & state.\n",
    "\n",
    "Ang naiisip ko since 6 Combination = 3 candidate * 2 state\n",
    "Gawin natin 5000/6 = 834 Related Sentences required set natin as minimum per Combination\n",
    "\n",
    "Trump  - Arizona      = 834 Related Sentences\n",
    "Harris - Arizona      = 834 Related Sentences\n",
    "Trump  - Michigan     = 834 Related Sentences\n",
    "Harris - Michigan     = 834 Related Sentences\n",
    "Trump  - Pennsylvania = 834 Related Sentences\n",
    "Harris - Pennsylvania = 834 Related Sentences\n",
    "                     --------------------------\n",
    "                      ~5000 Related Sentences\n",
    "\"\"\"\n",
    "(\n",
    "    pd\n",
    "    .read_csv(related_transcript_sentences_filename)\n",
    "    .groupby([\"Presidential_Candidate\", \"State\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    "    .style.hide(axis=\"index\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
