{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "39484717-540e-48c1-8b54-60ffaf3bb002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports and Initial Setup\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "# Name to use for saving the model\n",
    "model_path = 'BERT Sentiment Model'\n",
    "\n",
    "# Dataset (CSV) Column Names\n",
    "sentence_column_name = \"Sentence\"\n",
    "sentiment_column_name = \"Final_Sent\"\n",
    "\n",
    "# To Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "754d0227-4c7d-4b02-bf92-0ca020f88aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                               Sentence Presidential_Candidate  ... Vote_2  Final_Sent\n",
       " 1292  As the Democratic National Convention approach...          Kamala Harris  ...      1           1\n",
       " 2200                    He's like fighter, fighter man.           Donald Trump  ...      1           1\n",
       " 2780  Donald Trump over performed by almost 5% accor...           Donald Trump  ...      0           1\n",
       " 2960  If Kamala wants my vote, I want her to take a ...          Kamala Harris  ...     -1           0\n",
       " 2870  Around 80% of the African Americans here in Mi...           Donald Trump  ...     -1          -1\n",
       " ...                                                 ...                    ...  ...    ...         ...\n",
       " 2748     If Michigan goes to Donald Trump, he will win.           Donald Trump  ...      1           1\n",
       " 636   And Harris's ability to appeal to them could p...          Kamala Harris  ...      1           1\n",
       " 861   Donald Trump, on the other hand, in a town hal...           Donald Trump  ...      1           0\n",
       " 1763  You see Obama won Eaton County twice so did Tr...           Donald Trump  ...      1           1\n",
       " 1177  It's all there publicly, but Donald Trump's go...           Donald Trump  ...      0          -1\n",
       " \n",
       " [2646 rows x 6 columns],\n",
       "                                                Sentence Presidential_Candidate  ... Vote_2  Final_Sent\n",
       " 1638                                 She's a communist.          Kamala Harris  ...      0          -1\n",
       " 2174  There was he was never going to get over forty...           Donald Trump  ...      0          -1\n",
       " 2209  And now the Trump campaign is practically maki...           Donald Trump  ...      0           1\n",
       " 3487  The other thing you hear, at least for some of...          Kamala Harris  ...      1           1\n",
       " 304   More of these said groups are going to move, s...          Kamala Harris  ...      1           1\n",
       " ...                                                 ...                    ...  ...    ...         ...\n",
       " 1410  Now, after their Pennsylvania debut, Harris an...          Kamala Harris  ...     -1           0\n",
       " 2796                           He insulted a lot of us.           Donald Trump  ...     -1          -1\n",
       " 497   Ever, and he sees Kamala Harris as more libera...          Kamala Harris  ...      1           0\n",
       " 3612  She's got receipts on the issues that matter t...          Kamala Harris  ...      0           1\n",
       " 2793  Obviously, there's context and different facto...           Donald Trump  ...     -1           1\n",
       " \n",
       " [378 rows x 6 columns],\n",
       "                                                Sentence Presidential_Candidate  ... Vote_2  Final_Sent\n",
       " 838   The Harris Waltz campaign is doing a Labor Day...          Kamala Harris  ...      1           0\n",
       " 2615  Do you think he's going to win the state of Mi...           Donald Trump  ...      1           0\n",
       " 871   This energy and your black filling, look at th...           Donald Trump  ...      1           1\n",
       " 2802      Hey hey, ho ho, harrison Trump has got to go.           Donald Trump  ...      0          -1\n",
       " 1188  She rallied voters in Erie, telling the crowd ...          Kamala Harris  ...      1           0\n",
       " ...                                                 ...                    ...  ...    ...         ...\n",
       " 3502    She says she's not different from Biden at all.          Kamala Harris  ...      0          -1\n",
       " 1426             She does seem to be an outside figure.          Kamala Harris  ...      0           0\n",
       " 1765  I know Trump went down some very seemingly rid...           Donald Trump  ...     -1          -1\n",
       " 108   It's crazy, he was probably waiting outside Wh...           Donald Trump  ...      1           1\n",
       " 3326  Kamala has tried to reassure this aspect of th...          Kamala Harris  ...      1           0\n",
       " \n",
       " [756 rows x 6 columns])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Splitting\n",
    "# Load the dataset from CSV file\n",
    "df = pd.read_csv('annotated_dataset.csv')\n",
    "# Split data into 80% training+validation and 20% test\n",
    "remaining, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# Split the remaining 80% into 70% training and 10% validation (0.125 of 80% = 10% overall)\n",
    "train, val = train_test_split(remaining, test_size=0.125, random_state=42)\n",
    "\n",
    "train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e93974fb-7a73-40ce-86f1-ec6a718ef6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0569, 1.0804, 0.8626])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the model and compute class weights\n",
    "def compute_class_weights(labels):\n",
    "    \"\"\"\n",
    "    Calculate weights for each class to handle imbalanced data\n",
    "    For example, if we have 100 positive but only 10 negative samples,\n",
    "    negative samples will get higher weight to balance their importance\n",
    "    \"\"\"\n",
    "    # Shift labels for model [-1, 0, 1] to [0, 1, 2]\n",
    "    mapped_labels = labels + 1\n",
    "    # Count how many samples we have of each class\n",
    "    class_counts = np.bincount(mapped_labels)\n",
    "    # Give higher weights to classes with fewer samples\n",
    "    weights = 1. / class_counts\n",
    "    # Normalize weights to sum to number of classes\n",
    "    weights = weights * len(class_counts) / weights.sum()\n",
    "    return torch.FloatTensor(weights)\n",
    "\n",
    "# Calculate weights for each class from training data\n",
    "class_weights = compute_class_weights(train[sentiment_column_name].values)\n",
    "class_weights = class_weights.to(device)  # Move weights to GPU if available\n",
    "\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3fe13256-c790-4a57-834e-a8585c1a2d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base BERT model to use\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# Create a custom BERT model that can handle weighted loss\n",
    "class BertWithWeightedLoss(BertForSequenceClassification):\n",
    "    \"\"\"\n",
    "    Custom BERT model that applies different weights to each class\n",
    "    This helps handle imbalanced datasets better\n",
    "    \"\"\"\n",
    "    def __init__(self, config, class_weights):\n",
    "        super().__init__(config)\n",
    "        self.class_weights = class_weights # Store class weights for loss calculation\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Get model outputs without computing loss\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=None # Set to None to prevent automatic loss calculation\n",
    "        )\n",
    "        \n",
    "        # Calculate weighted loss if labels are provided (training phase)\n",
    "        if labels is not None:\n",
    "            # Create loss function with class weights\n",
    "            loss_fct = CrossEntropyLoss(weight=self.class_weights)\n",
    "            # Calculate loss using model predictions and true labels\n",
    "            loss = loss_fct(\n",
    "                outputs.logits.view(-1, self.num_labels),  # Reshape predictions\n",
    "                labels.view(-1)                            # Reshape labels\n",
    "            )\n",
    "            outputs.loss = loss  # Add loss to outputs\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# Initialize the tokenizer that will convert text to numbers\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "af4a89e5-97b1-4659-ba1f-bcf5d1d28cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation Functions\n",
    "def create_data_loader(data, tokenizer, batch_size):\n",
    "    \"\"\"\n",
    "    Convert text data into a format BERT can understand and create batches\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame containing text and labels\n",
    "        tokenizer: BERT tokenizer to convert text to numbers\n",
    "        batch_size: How many samples to process at once\n",
    "    \n",
    "    Returns:\n",
    "        DataLoader that yields batches of processed data\n",
    "    \"\"\"\n",
    "    # Convert text to BERT input format with progress bar\n",
    "    encodings = tokenizer(\n",
    "        data[sentence_column_name].tolist(), # Convert sentences to list\n",
    "        truncation=True, # Cut texts longer than max_length\n",
    "        padding=True, # Pad texts shorter than max_length\n",
    "        max_length=128, # Maximum sequence length\n",
    "        return_tensors='pt', # Return PyTorch tensors\n",
    "        verbose=True # Show progress\n",
    "    )\n",
    "\n",
    "    # Create dataset by combining inputs and labels\n",
    "    dataset = torch.utils.data.TensorDataset(\n",
    "        encodings['input_ids'], # Tokenized text\n",
    "        encodings['attention_mask'], # Attention mask for padding\n",
    "        torch.tensor(data[sentiment_column_name].tolist()) # Labels\n",
    "    )\n",
    "    \n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9ac60c60-8317-4241-9c80-745a5d9ef956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate model performance using various metrics\n",
    "    \n",
    "    Args:\n",
    "        model: The BERT model to evaluate\n",
    "        data_loader: DataLoader containing validation or test data\n",
    "        device: CPU or GPU\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing various performance metrics\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad(): # Don't compute gradients during evaluation\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluation\"):\n",
    "            # Move batch to GPU if available\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            adjusted_labels = labels + 1 # Shift labels for model\n",
    "\n",
    "            # Get model predictions\n",
    "            outputs = model(input_ids, attention_mask, labels=adjusted_labels)\n",
    "            val_loss += outputs.loss.item()\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            all_preds.extend((predicted - 1).cpu().numpy())\n",
    "            all_labels.extend((adjusted_labels - 1).cpu().numpy())\n",
    "\n",
    "    # Calculate various performance metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'loss': val_loss / len(data_loader),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a16dc4b9-41cd-4383-b480-d9d5cb303c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, device, epochs, learning_rate):\n",
    "    \"\"\"\n",
    "    Train the model and periodically evaluate its performance\n",
    "    \n",
    "    Args:\n",
    "        model: The BERT model to train\n",
    "        train_loader: DataLoader with training data\n",
    "        val_loader: DataLoader with validation data\n",
    "        device: CPU or GPU\n",
    "        epochs: Number of times to process all training data\n",
    "        learning_rate: How quickly the model should learn\n",
    "    \n",
    "    Returns:\n",
    "        Trained model and its best validation metrics\n",
    "    \"\"\"\n",
    "    # Initialize optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    best_metrics = None\n",
    "    best_model = None\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # Set model to training mode\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Process each batch\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs} Training\")\n",
    "        for batch in pbar:\n",
    "            # Move batch to GPU if available\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            adjusted_labels = labels + 1 # Shift labels for model [-1, 0, 1] to [0, 1, 2]\n",
    "\n",
    "            # Training step\n",
    "            optimizer.zero_grad() # Clear previous gradients\n",
    "            outputs = model(input_ids, attention_mask, labels=adjusted_labels) # Forward pass\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item() # Accumulate loss\n",
    "\n",
    "            # Update model weights\n",
    "            loss.backward() # Backward pass\n",
    "            optimizer.step() # Update weights\n",
    "\n",
    "            # Update progress bar with current loss\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Calculate average loss for this epoch\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Average training loss: {avg_train_loss:.4f}')\n",
    "        \n",
    "        val_metrics = evaluate_model(model, val_loader, device)\n",
    "        print(f'Validation metrics: {val_metrics}')\n",
    "        \n",
    "         # Update best parameters if accuracy score improves\n",
    "        if best_metrics == None or val_metrics['accuracy'] > best_metrics['accuracy']:\n",
    "            best_metrics = val_metrics\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model, best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9f662c91-5ce7-4d94-aa27-c4f1c9e60e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different values to try for each parameter\n",
    "param_grid = {\n",
    "    'learning_rate': [2e-5, 5e-5], # BERT is best on -5\n",
    "    'batch_size': [32], # If the max length is 128 or 256, then 32 would be a good number.\n",
    "    'epochs': [5] # Amount of epochs required were small 3 to 5\n",
    "}\n",
    "\n",
    "# Hyperparameter Tuning Function\n",
    "def hyperparameter_tuning(train_data, val_data, device, class_weights):\n",
    "    \"\"\"\n",
    "    Try different combinations of hyperparameters to find the best ones\n",
    "    \n",
    "    Args:\n",
    "        train_data: Training data DataFrame\n",
    "        val_data: Validation data DataFrame\n",
    "        device: CPU or GPU\n",
    "        class_weights: Weights for each class\n",
    "    \n",
    "    Returns:\n",
    "        Best parameters and their corresponding metrics\n",
    "    \"\"\"    \n",
    "\n",
    "    # Create all possible combinations of parameters\n",
    "    param_combinations = [\n",
    "        dict(zip(param_grid.keys(), v)) \n",
    "        for v in itertools.product(*param_grid.values())\n",
    "    ]\n",
    "\n",
    "    best_model = None\n",
    "    best_metrics = None\n",
    "    best_params = None\n",
    "\n",
    "    # Try each combination of parameters\n",
    "    for params in param_combinations:\n",
    "        print(f\"\\nTrying parameters: {params}\")\n",
    "\n",
    "        # Create data loaders with current batch size\n",
    "        train_loader = create_data_loader(train_data, tokenizer, params['batch_size'])\n",
    "        val_loader = create_data_loader(val_data, tokenizer, params['batch_size'])\n",
    "\n",
    "        # Initialize the custom BERT model\n",
    "        model = BertWithWeightedLoss.from_pretrained(\n",
    "            model_name,\n",
    "            # Configure BERT for binary classification\n",
    "            config=BertForSequenceClassification.from_pretrained(\n",
    "                model_name,\n",
    "                num_labels=3,\n",
    "                output_attentions=False, # Don't output attention weights\n",
    "                output_hidden_states=False, # Don't output hidden states\n",
    "            ).config,\n",
    "            class_weights=class_weights\n",
    "        )\n",
    "        # Move model to GPU if available\n",
    "        model.to(device)\n",
    "\n",
    "        # Train model with current parameters\n",
    "        model, val_metrics = train_model(\n",
    "            model, \n",
    "            train_loader, \n",
    "            val_loader,\n",
    "            device,\n",
    "            params['epochs'],\n",
    "            params['learning_rate']\n",
    "        )\n",
    "\n",
    "        # Update best parameters if accuracy score improves\n",
    "        if best_metrics == None or val_metrics['accuracy'] > best_metrics['accuracy']:\n",
    "            best_model = model\n",
    "            best_params = params\n",
    "            best_metrics = val_metrics\n",
    "    \n",
    "    return best_model, best_params, best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fc5670fc-34ee-43dc-82ed-ed41223315e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying parameters: {'learning_rate': 2e-05, 'batch_size': 32, 'epochs': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertWithWeightedLoss were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\MSI Laptop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/5 Training: 100%|██████████████████████████████████████████████████| 83/83 [14:32<00:00, 10.52s/it, loss=0.686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Average training loss: 0.9529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████| 12/12 [00:22<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'loss': 0.7400287439425787, 'accuracy': 0.6851851851851852, 'precision': 0.6833075654504226, 'recall': 0.6851851851851852, 'f1': 0.6770897761219201}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 Training: 100%|███████████████████████████████████████████████████| 83/83 [14:56<00:00, 10.80s/it, loss=0.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Average training loss: 0.6529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████| 12/12 [00:24<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'loss': 0.6706100727121035, 'accuracy': 0.701058201058201, 'precision': 0.694312524913797, 'recall': 0.701058201058201, 'f1': 0.6949520620027532}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 Training: 100%|██████████████████████████████████████████████████| 83/83 [15:48<00:00, 11.43s/it, loss=0.355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Average training loss: 0.4043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████| 12/12 [00:23<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'loss': 0.7281094739834467, 'accuracy': 0.7328042328042328, 'precision': 0.7349320873130397, 'recall': 0.7328042328042328, 'f1': 0.7300415897933018}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 Training: 100%|██████████████████████████████████████████████████| 83/83 [14:42<00:00, 10.64s/it, loss=0.126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Average training loss: 0.2370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████| 12/12 [00:26<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'loss': 0.849938211341699, 'accuracy': 0.716931216931217, 'precision': 0.7167796288215753, 'recall': 0.716931216931217, 'f1': 0.7161571734685311}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 Training: 100%|██████████████████████████████████████████████████| 83/83 [33:41<00:00, 24.35s/it, loss=0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Average training loss: 0.1594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████| 12/12 [00:58<00:00,  4.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'loss': 0.9804117580254873, 'accuracy': 0.6851851851851852, 'precision': 0.6996588736431473, 'recall': 0.6851851851851852, 'f1': 0.6860579958692612}\n",
      "\n",
      "Trying parameters: {'learning_rate': 5e-05, 'batch_size': 32, 'epochs': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertWithWeightedLoss were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\MSI Laptop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/5 Training: 100%|██████████████████████████████████████████████████| 83/83 [30:34<00:00, 22.10s/it, loss=0.525]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Average training loss: 0.8809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████| 12/12 [00:24<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'loss': 0.7059457004070282, 'accuracy': 0.701058201058201, 'precision': 0.692152787491736, 'recall': 0.701058201058201, 'f1': 0.6858338149795068}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 Training: 100%|██████████████████████████████████████████████████| 83/83 [15:26<00:00, 11.16s/it, loss=0.696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Average training loss: 0.5485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████| 12/12 [00:24<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'loss': 0.6784906660517057, 'accuracy': 0.701058201058201, 'precision': 0.6975855201708459, 'recall': 0.701058201058201, 'f1': 0.6988953916187093}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 Training: 100%|██████████████████████████████████████████████████| 83/83 [15:53<00:00, 11.49s/it, loss=0.194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Average training loss: 0.3132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████| 12/12 [00:25<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'loss': 0.8028487414121628, 'accuracy': 0.7116402116402116, 'precision': 0.7235556827930305, 'recall': 0.7116402116402116, 'f1': 0.7082303887486773}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 Training: 100%|██████████████████████████████████████████████████| 83/83 [15:32<00:00, 11.24s/it, loss=0.134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Average training loss: 0.1640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████| 12/12 [00:24<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'loss': 1.0634979108969371, 'accuracy': 0.6904761904761905, 'precision': 0.6969949654216449, 'recall': 0.6904761904761905, 'f1': 0.6917576179342783}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 Training: 100%|██████████████████████████████████████████████████| 83/83 [16:57<00:00, 12.26s/it, loss=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Average training loss: 0.0980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████| 12/12 [00:24<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'loss': 1.1518813719352086, 'accuracy': 0.6746031746031746, 'precision': 0.6781348531832091, 'recall': 0.6746031746031746, 'f1': 0.6758853744255204}\n",
      "\n",
      "Best parameters: {'learning_rate': 2e-05, 'batch_size': 32, 'epochs': 5}\n",
      "Best validation metrics: {'loss': 0.7281094739834467, 'accuracy': 0.7328042328042328, 'precision': 0.7349320873130397, 'recall': 0.7328042328042328, 'f1': 0.7300415897933018}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertWithWeightedLoss(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Hyperparameter Tuning\n",
    "best_model, best_params, best_metrics = hyperparameter_tuning(train, val, device, class_weights)\n",
    "print(f\"\\nBest parameters: {best_params}\")\n",
    "print(f\"Best validation metrics: {best_metrics}\")\n",
    "\n",
    "# Save the best model\n",
    "best_model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a213a98-c7de-483f-98b9-4d122345a271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Time\n",
    "# 14:32 00:22\n",
    "# 14:56 00:24\n",
    "# 15:48 00:23\n",
    "# 14:42 00:26\n",
    "# 33:41 00:58\n",
    "\n",
    "# 30:34 00:24\n",
    "# 15:26 00:24\n",
    "# 15:53 00:25\n",
    "# 15:32 00:24\n",
    "# 16:57 00:24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "00df7a16-c782-4f75-842e-0e1ba82cdc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating final model on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████| 24/24 [01:06<00:00,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set metrics: {'loss': 0.9251180763045946, 'accuracy': 0.6838624338624338, 'precision': 0.6979576767654118, 'recall': 0.6838624338624338, 'f1': 0.6873617524080348}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Final Evaluation on Test Set\n",
    "test_loader = create_data_loader(test, tokenizer, best_params['batch_size'])\n",
    "print(\"\\nEvaluating final model on test set...\")\n",
    "test_metrics = evaluate_model(best_model, test_loader, device)\n",
    "print(f\"Test set metrics: {test_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "405c5b46-59a1-4689-a24f-eeb867d07bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class (-1, 0, 1): 1\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "best_model.eval()\n",
    "\n",
    "# Example text for prediction\n",
    "example_text = \"harris leads by 1%\"\n",
    "\n",
    "# Tokenize the input text\n",
    "encoded_input = tokenizer(\n",
    "    example_text,\n",
    "    return_tensors=\"pt\", # Return PyTorch tensors\n",
    "    truncation=True,\n",
    "    padding=True\n",
    ")\n",
    "# Remove token_type_ids if not used\n",
    "encoded_input.pop(\"token_type_ids\", None)\n",
    "\n",
    "# Perform prediction without gradient computation\n",
    "with torch.no_grad():\n",
    "    outputs = best_model(**encoded_input)\n",
    "\n",
    "# Get the logits from the model's output\n",
    "logits = outputs.logits\n",
    "\n",
    "# Get the predicted class (0, 1, or 2)\n",
    "predicted_class = torch.argmax(logits, dim=1).item() - 1\n",
    "print(\"Predicted class (-1, 0, 1):\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139abbee-c804-46c0-9868-66940a2217d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
