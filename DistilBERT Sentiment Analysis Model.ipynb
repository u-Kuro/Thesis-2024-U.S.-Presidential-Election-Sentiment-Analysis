{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44d6e654-7786-4a8d-9db3-791f06a065f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports and Initial Setup\n",
    "import os, copy, torch, itertools\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Name to use for saving the model\n",
    "model_path = 'DistilBERT Sentiment Model'\n",
    "\n",
    "# Dataset (CSV) Column Names\n",
    "sentence_column_name = \"Sentences\"\n",
    "sentiment_column_name = \"Final_Sent\"\n",
    "dataset_split_folder_name = \"Annotated Dataset Split\"\n",
    "model_evaluation_result_folder_name = os.path.join(\"Model Evaluation Results\", \"DistilBERT\")\n",
    "os.makedirs(model_evaluation_result_folder_name, exist_ok=True)\n",
    "\n",
    "# To Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c103e29f-dc5a-4762-9450-04238b6f2981",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                              Sentences  \\\n",
       " 0     Nate Silver's Bolton has Kamala Harris leading...   \n",
       " 1     This razor-thin margin demonstrates that Penns...   \n",
       " 2                               Trump leads by just 1%.   \n",
       " 3     How critical is winning the state of Michigan ...   \n",
       " 4     Am I saying Trump's going to win it by 21 poin...   \n",
       " ...                                                 ...   \n",
       " 4016       Kamala Harris, I believe, fits all of those.   \n",
       " 4017  This idea that she's going to magically do bet...   \n",
       " 4018  It holds 15 electoral votes and it is very muc...   \n",
       " 4019  Harris holds narrow leads in four critical bat...   \n",
       " 4020  And so, I refuse a meeting with Donald Trump o...   \n",
       " \n",
       "      Presidential_Candidate         State  Vote_1  Vote_2  Final_Sent  \n",
       " 0             Kamala Harris      Michigan       1       0           1  \n",
       " 1              Donald Trump  Pennsylvania       1       1           1  \n",
       " 2              Donald Trump      Michigan       1       0           1  \n",
       " 3             Kamala Harris      Michigan       0      -1           0  \n",
       " 4              Donald Trump  Pennsylvania       0       1           0  \n",
       " ...                     ...           ...     ...     ...         ...  \n",
       " 4016          Kamala Harris       Arizona       1       0           1  \n",
       " 4017          Kamala Harris  Pennsylvania      -1       0           0  \n",
       " 4018          Kamala Harris      Michigan       0       0           0  \n",
       " 4019          Kamala Harris      Michigan       1       1           1  \n",
       " 4020           Donald Trump      Michigan      -1      -1          -1  \n",
       " \n",
       " [4021 rows x 6 columns],\n",
       "                                              Sentences Presidential_Candidate  \\\n",
       " 0    And if Trump is really winning Pennsylvania, d...           Donald Trump   \n",
       " 1    Republican voters are increasingly expressing ...           Donald Trump   \n",
       " 2                          They know what he has done.           Donald Trump   \n",
       " 3           You do have Trump sitting plus point five.           Donald Trump   \n",
       " 4    And she is actually changing her mind on a lot...          Kamala Harris   \n",
       " ..                                                 ...                    ...   \n",
       " 570  So I see a lot of signs that polling is not pi...          Kamala Harris   \n",
       " 571                    Right now, Trump might get 50%.           Donald Trump   \n",
       " 572  So you think Trump probably does have an advan...           Donald Trump   \n",
       " 573  Trump has a lot of issues that I wouldn't pref...           Donald Trump   \n",
       " 574  Trump, a master of using personal narratives t...           Donald Trump   \n",
       " \n",
       "             State  Vote_1  Vote_2  Final_Sent  \n",
       " 0    Pennsylvania      -1      -1          -1  \n",
       " 1        Michigan      -1       0          -1  \n",
       " 2        Michigan       1       0           1  \n",
       " 3        Michigan       1       0           1  \n",
       " 4    Pennsylvania       0      -1           0  \n",
       " ..            ...     ...     ...         ...  \n",
       " 570      Michigan      -1       0          -1  \n",
       " 571      Michigan       1       0           0  \n",
       " 572       Arizona       1       1           1  \n",
       " 573       Arizona       0       0           0  \n",
       " 574  Pennsylvania      -1       0          -1  \n",
       " \n",
       " [575 rows x 6 columns],\n",
       "                                               Sentences  \\\n",
       " 0     And the campaign has had a robust public relat...   \n",
       " 1                 Concrete plans, that's who Kamala is.   \n",
       " 2     What Kamala and her radical left cronies have ...   \n",
       " 3     Also, essentially, they see Kamala Harris as s...   \n",
       " 4                     She's she's behind by two points.   \n",
       " ...                                                 ...   \n",
       " 1144  So and also I do think Harris has some room to...   \n",
       " 1145  She's probably done because that's supposed to...   \n",
       " 1146  Does the Harris walls ticket benefit from that...   \n",
       " 1147  I will never be a Republican, but if I were he...   \n",
       " 1148  So I think people are waking up more so than w...   \n",
       " \n",
       "      Presidential_Candidate         State  Vote_1  Vote_2  Final_Sent  \n",
       " 0             Kamala Harris  Pennsylvania       0       1           0  \n",
       " 1             Kamala Harris       Arizona       1       0           1  \n",
       " 2             Kamala Harris       Arizona       1      -1          -1  \n",
       " 3             Kamala Harris      Michigan      -1       0          -1  \n",
       " 4             Kamala Harris       Arizona       1      -1          -1  \n",
       " ...                     ...           ...     ...     ...         ...  \n",
       " 1144          Kamala Harris       Arizona       1       0           1  \n",
       " 1145          Kamala Harris  Pennsylvania       0       1           0  \n",
       " 1146          Kamala Harris       Arizona       1       1           1  \n",
       " 1147           Donald Trump       Arizona      -1       0          -1  \n",
       " 1148          Kamala Harris       Arizona       1       0           0  \n",
       " \n",
       " [1149 rows x 6 columns],\n",
       "                                               Sentences  \\\n",
       " 0     Nate Silver's Bolton has Kamala Harris leading...   \n",
       " 1     This razor-thin margin demonstrates that Penns...   \n",
       " 2                               Trump leads by just 1%.   \n",
       " 3     How critical is winning the state of Michigan ...   \n",
       " 4     Am I saying Trump's going to win it by 21 poin...   \n",
       " ...                                                 ...   \n",
       " 5740  So and also I do think Harris has some room to...   \n",
       " 5741  She's probably done because that's supposed to...   \n",
       " 5742  Does the Harris walls ticket benefit from that...   \n",
       " 5743  I will never be a Republican, but if I were he...   \n",
       " 5744  So I think people are waking up more so than w...   \n",
       " \n",
       "      Presidential_Candidate         State  Vote_1  Vote_2  Final_Sent  \n",
       " 0             Kamala Harris      Michigan       1       0           1  \n",
       " 1              Donald Trump  Pennsylvania       1       1           1  \n",
       " 2              Donald Trump      Michigan       1       0           1  \n",
       " 3             Kamala Harris      Michigan       0      -1           0  \n",
       " 4              Donald Trump  Pennsylvania       0       1           0  \n",
       " ...                     ...           ...     ...     ...         ...  \n",
       " 5740          Kamala Harris       Arizona       1       0           1  \n",
       " 5741          Kamala Harris  Pennsylvania       0       1           0  \n",
       " 5742          Kamala Harris       Arizona       1       1           1  \n",
       " 5743           Donald Trump       Arizona      -1       0          -1  \n",
       " 5744          Kamala Harris       Arizona       1       0           0  \n",
       " \n",
       " [5745 rows x 6 columns])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Splitting\n",
    "def split_data():\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(dataset_split_folder_name, exist_ok=True)\n",
    "    \n",
    "    # Define file paths\n",
    "    train_path = os.path.join(dataset_split_folder_name, 'train.csv')\n",
    "    val_path = os.path.join(dataset_split_folder_name, 'validation.csv')\n",
    "    test_path = os.path.join(dataset_split_folder_name, 'test.csv')\n",
    "    \n",
    "    # Check if split files already exist\n",
    "    if all(os.path.exists(f) for f in [train_path, val_path, test_path]):\n",
    "        train = pd.read_csv(train_path)\n",
    "        val = pd.read_csv(val_path)\n",
    "        test = pd.read_csv(test_path)\n",
    "\n",
    "        # Combine DataFrames vertically\n",
    "        df = pd.concat([train, val, test], axis=0).reset_index(drop=True)\n",
    "                \n",
    "        return train, val, test, df\n",
    "    else:\n",
    "        # Load the dataset\n",
    "        df = pd.read_csv('annotated_dataset.csv')\n",
    "        \n",
    "        # Split data into 80% training+validation and 20% test\n",
    "        remaining, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "        # Split the remaining 80% into 70% training and 10% validation (0.125 of 80% = 10% overall)\n",
    "        train, val = train_test_split(remaining, test_size=0.125, random_state=42)\n",
    "        \n",
    "        # Save splits\n",
    "        train.to_csv(train_path, index=False)\n",
    "        val.to_csv(val_path, index=False)\n",
    "        test.to_csv(test_path, index=False)\n",
    "    \n",
    "        return train, val, test, df\n",
    "\n",
    "train, val, test, full = split_data()\n",
    "\n",
    "train, val, test, full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ad121e53-d2dc-4afe-bf3a-25dfa335307f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9941, 1.1636, 0.8422])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the model and compute class weights\n",
    "def compute_class_weights(labels):\n",
    "    \"\"\"\n",
    "    Calculate weights for each class to handle imbalanced data\n",
    "    For example, if we have 100 positive but only 10 negative samples,\n",
    "    negative samples will get higher weight to balance their importance\n",
    "    \"\"\"\n",
    "    # Shift labels for model [-1, 0, 1] to [0, 1, 2]\n",
    "    mapped_labels = labels + 1\n",
    "    # Count how many samples we have of each class\n",
    "    class_counts = np.bincount(mapped_labels)\n",
    "    # Give higher weights to classes with fewer samples\n",
    "    weights = 1. / class_counts\n",
    "    # Normalize weights to sum to number of classes\n",
    "    weights = weights * len(class_counts) / weights.sum()\n",
    "    return torch.FloatTensor(weights)\n",
    "\n",
    "# Calculate weights for each class from training data\n",
    "class_weights = compute_class_weights(train[sentiment_column_name].values)\n",
    "class_weights = class_weights.to(device)  # Move weights to GPU if available\n",
    "\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de75a40e-3da8-4256-b57b-deb4fe2a1d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizer(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base DistilBERT model to use\n",
    "model_name = 'distilbert-base-uncased'\n",
    "\n",
    "# Create a custom DistilBERT model that can handle weighted loss\n",
    "class DistilBertWithWeightedLoss(DistilBertForSequenceClassification):\n",
    "    \"\"\"\n",
    "    Custom DistilBERT model that applies different weights to each class\n",
    "    This helps handle imbalanced datasets better\n",
    "    \"\"\"\n",
    "    def __init__(self, config, class_weights):\n",
    "        super().__init__(config)\n",
    "        self.class_weights = class_weights # Store class weights for loss calculation\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Get model outputs without computing loss\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=None # Set to None to prevent automatic loss calculation\n",
    "        )\n",
    "        \n",
    "        # Calculate weighted loss if labels are provided (training phase)\n",
    "        if labels is not None:\n",
    "            # Create loss function with class weights\n",
    "            loss_fct = CrossEntropyLoss(weight=self.class_weights)\n",
    "            # Calculate loss using model predictions and true labels\n",
    "            loss = loss_fct(\n",
    "                outputs.logits.view(-1, self.num_labels),  # Reshape predictions\n",
    "                labels.view(-1)                            # Reshape labels\n",
    "            )\n",
    "            outputs.loss = loss  # Add loss to outputs\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# Initialize the tokenizer that will convert text to numbers\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8db2eb85-9f69-4ac2-97f0-b5b404ce51ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation Functions\n",
    "def create_data_loader(data, tokenizer, batch_size):\n",
    "    \"\"\"\n",
    "    Convert text data into a format DistilBERT can understand and create batches\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame containing text and labels\n",
    "        tokenizer: DistilBERT tokenizer to convert text to numbers\n",
    "        batch_size: How many samples to process at once\n",
    "\n",
    "    Returns:\n",
    "        DataLoader that yields batches of processed data\n",
    "    \"\"\"\n",
    "    # Create a copy of data to avoid modifying the original\n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    # Convert text to DistilBERT input format with progress bar\n",
    "    encodings = tokenizer(\n",
    "        data_copy[sentence_column_name].tolist(), # Convert sentences to list\n",
    "        truncation=True, # Cut texts longer than max_length\n",
    "        padding=True, # Pad texts shorter than max_length\n",
    "        max_length=128, # Maximum sequence length\n",
    "        return_tensors='pt', # Return PyTorch tensors\n",
    "        verbose=True # Show progress\n",
    "    )\n",
    "\n",
    "    # Create dataset by combining inputs and labels\n",
    "    dataset = torch.utils.data.TensorDataset(\n",
    "        encodings['input_ids'], # Tokenized text\n",
    "        encodings['attention_mask'], # Attention mask for padding\n",
    "        torch.tensor(data_copy[sentiment_column_name].tolist()) # Labels\n",
    "    )\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdcdbb7d-55b2-4d28-8b6e-7cc1b40a6f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "def evaluate_model(model, eval_data, device):\n",
    "    \"\"\"\n",
    "    Evaluate model performance\n",
    "    \n",
    "    Args:\n",
    "        model: The DistilBERT model to evaluate\n",
    "        eval_data: DataFrame containing evaluation data with sentences and labels\n",
    "        tokenizer: DistilBERT tokenizer\n",
    "        device: CPU or GPU\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing various performance metrics and updated DataFrame\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create a copy of eval_data to avoid modifying the original\n",
    "    results_df = eval_data.copy()\n",
    "    # Add new column for predictions\n",
    "    results_df['Predicted_Sent'] = None\n",
    "        \n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, row in tqdm(results_df.iterrows(), total=len(results_df), desc=\"Evaluating\"):\n",
    "            # Tokenize single sentence\n",
    "            encoding = tokenizer(\n",
    "                row[sentence_column_name],\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=128,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Move inputs to device\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "            label = torch.tensor([row[sentiment_column_name]]).to(device)\n",
    "            adjusted_label = label + 1\n",
    "            \n",
    "            # Get model predictions\n",
    "            outputs = model(input_ids, attention_mask, labels=adjusted_label)\n",
    "            val_loss += outputs.loss.item()\n",
    "            \n",
    "            # Get prediction\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            pred = (predicted - 1).cpu().numpy()[0]\n",
    "            \n",
    "            # Store prediction in DataFrame\n",
    "            results_df.at[idx, 'Predicted_Sent'] = int(pred)\n",
    "            \n",
    "            # Store for metrics calculation\n",
    "            all_preds.append(pred)\n",
    "            all_labels.append(row[sentiment_column_name])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    metrics = {\n",
    "        'loss': val_loss / len(results_df),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1-score': f1\n",
    "    }\n",
    "    \n",
    "    return metrics, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "794bb0ab-7b68-408e-830c-60e6ad40c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train_model(model, train_loader, val_data, device, epochs, learning_rate):\n",
    "    # Initialize optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    val_metrics = None\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    val_accuracy = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # Set model to training mode\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Process each batch\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs} Training\")\n",
    "        for batch in pbar:\n",
    "            # Move batch to GPU if available\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            adjusted_labels = labels + 1 # Shift labels for model [-1, 0, 1] to [0, 1, 2]\n",
    "\n",
    "            # Training step\n",
    "            optimizer.zero_grad() # Clear previous gradients\n",
    "            outputs = model(input_ids, attention_mask, labels=adjusted_labels) # Forward pass\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item() # Accumulate loss\n",
    "\n",
    "            # Update model weights\n",
    "            loss.backward() # Backward pass\n",
    "            optimizer.step() # Update weights\n",
    "\n",
    "            # Update progress bar with current loss\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Calculate training loss for this epoch\n",
    "        current_train_loss = total_loss / len(train_loader)\n",
    "        train_loss.append(current_train_loss)\n",
    "\n",
    "        # Calculate validation loss and accuracy for this epoch\n",
    "        val_metrics, _ = evaluate_model(model, val_data, device)\n",
    "        val_loss.append(val_metrics['loss'])\n",
    "        val_accuracy.append(val_metrics['accuracy'])\n",
    "\n",
    "        print(f'Training loss: {current_train_loss}')\n",
    "        print(f'Validation metric: {val_metrics}')\n",
    "    \n",
    "    return model, {\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'val_accuracy': val_accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "833e77d3-a686-4869-a66f-92e62db0c592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model with Parameter Function\n",
    "def train_model_with_parameter(params, train_data, val_data, device, class_weights):\n",
    "    print(f\"Parameters: {params}\")\n",
    "\n",
    "    # Create data loaders with current batch size\n",
    "    train_loader = create_data_loader(train_data, tokenizer, params['batch_size'])\n",
    "\n",
    "    # Initialize the custom DistilBERT model\n",
    "    model = DistilBertWithWeightedLoss.from_pretrained(\n",
    "        model_name,\n",
    "        # Configure DistilBERT for classification\n",
    "        config=DistilBertForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=3,\n",
    "            output_attentions=False, # Don't output attention weights\n",
    "            output_hidden_states=False, # Don't output hidden states\n",
    "        ).config,\n",
    "        class_weights=class_weights\n",
    "    )\n",
    "    # Move model to GPU if available\n",
    "    model.to(device)\n",
    "\n",
    "    # Train model with current parameters\n",
    "    model, train_metric_seq = train_model(\n",
    "        model, \n",
    "        train_loader, \n",
    "        val_data,\n",
    "        device,\n",
    "        params['epochs'],\n",
    "        params['learning_rate']\n",
    "    )\n",
    "    \n",
    "    return model, train_metric_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3ea6f3b5-7c61-4d55-88dc-06d71c3d4db0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'learning_rate': 2e-05, 'epochs': 2, 'batch_size': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertWithWeightedLoss were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\MSI Laptop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/2 Training:   4%|█▊                                                 | 9/252 [00:27<12:34,  3.10s/it, loss=1.12]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train model using different values to try for each parameter\u001b[39;00m\n\u001b[0;32m      2\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2e-5\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m      6\u001b[0m }\n\u001b[1;32m----> 7\u001b[0m model, train_metric_seq \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_with_parameter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(model_path)\n",
      "Cell \u001b[1;32mIn[72], line 24\u001b[0m, in \u001b[0;36mtrain_model_with_parameter\u001b[1;34m(params, train_data, val_data, device, class_weights)\u001b[0m\n\u001b[0;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Train model with current parameters\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m model, train_metric_seq \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, train_metric_seq\n",
      "Cell \u001b[1;32mIn[71], line 30\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_data, device, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     27\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;66;03m# Accumulate loss\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Update model weights\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Update progress bar with current loss\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model using different values to try for each parameter\n",
    "params = {\n",
    "    'learning_rate': 2e-5,\n",
    "    'epochs': 2,\n",
    "    'batch_size': 16\n",
    "}\n",
    "model, train_metric_seq = train_model_with_parameter(params, train, val, device, class_weights)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "train_metric_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766f98c-5bde-44b7-ae75-9574d6cef804",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Parameters: {params}\")\n",
    "\n",
    "# Evaluation on Test Set\n",
    "test_metrics, predicted_labels_df = evaluate_model(model, test, device)\n",
    "_, full_predicted_labels_df = evaluate_model(model, full, device)\n",
    "\n",
    "# Name of Parameters as Filename\n",
    "file_name = f'LR {params[\"learning_rate\"]}, E {params[\"epochs\"]}, BS {params[\"batch_size\"]}'\n",
    "\n",
    "# Save Validation Metrics\n",
    "metric_results_file_name = os.path.join(\n",
    "    model_evaluation_result_folder_name,\n",
    "    f'{file_name} - Validation Metric.csv'\n",
    ")\n",
    "pd.DataFrame(train_metric_seq).to_csv(metric_results_file_name, index=False)\n",
    "\n",
    "# Save Test Metrics\n",
    "metric_results_file_name = os.path.join(\n",
    "    model_evaluation_result_folder_name,\n",
    "    f'{file_name} - Test Metric.csv'\n",
    ")\n",
    "pd.DataFrame([test_metrics]).to_csv(metric_results_file_name, index=False)\n",
    "\n",
    "# Save Predicted Labels\n",
    "sentiment_results_file_name = os.path.join(\n",
    "    model_evaluation_result_folder_name,\n",
    "    f'{file_name} - Predicted Dataset.csv'\n",
    ")\n",
    "predicted_labels_df.to_csv(sentiment_results_file_name, index=False)\n",
    "\n",
    "# Save Full Predicted Labels\n",
    "sentiment_results_file_name = os.path.join(\n",
    "    model_evaluation_result_folder_name,\n",
    "    f'{file_name} - Full Predicted Dataset.csv'\n",
    ")\n",
    "full_predicted_labels_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Test set metrics: {test_metrics}\")\n",
    "predicted_labels_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
