{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d6e654-7786-4a8d-9db3-791f06a065f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports and Initial Setup\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "# Name to use for saving the model\n",
    "model_path = 'DistilBERT Sentiment Model'\n",
    "\n",
    "# Dataset (CSV) Column Names\n",
    "sentence_column_name = \"Sentence\"\n",
    "sentiment_column_name = \"Final_Sent\"\n",
    "\n",
    "# To Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c103e29f-dc5a-4762-9450-04238b6f2981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                               Sentence  \\\n",
       " 1292  As the Democratic National Convention approach...   \n",
       " 2200                    He's like fighter, fighter man.   \n",
       " 2780  Donald Trump over performed by almost 5% accor...   \n",
       " 2960  If Kamala wants my vote, I want her to take a ...   \n",
       " 2870  Around 80% of the African Americans here in Mi...   \n",
       " ...                                                 ...   \n",
       " 2748     If Michigan goes to Donald Trump, he will win.   \n",
       " 636   And Harris's ability to appeal to them could p...   \n",
       " 861   Donald Trump, on the other hand, in a town hal...   \n",
       " 1763  You see Obama won Eaton County twice so did Tr...   \n",
       " 1177  It's all there publicly, but Donald Trump's go...   \n",
       " \n",
       "      Presidential_Candidate         State  Vote_1  Vote_2  Final_Sent  \n",
       " 1292          Kamala Harris  Pennsylvania       1       1           1  \n",
       " 2200           Donald Trump      Michigan       1       1           1  \n",
       " 2780           Donald Trump      Michigan       1       0           1  \n",
       " 2960          Kamala Harris      Michigan       0      -1           0  \n",
       " 2870           Donald Trump      Michigan      -1      -1          -1  \n",
       " ...                     ...           ...     ...     ...         ...  \n",
       " 2748           Donald Trump      Michigan       1       1           1  \n",
       " 636           Kamala Harris  Pennsylvania       1       1           1  \n",
       " 861            Donald Trump  Pennsylvania       0       1           0  \n",
       " 1763           Donald Trump      Michigan       1       1           1  \n",
       " 1177           Donald Trump  Pennsylvania      -1       0          -1  \n",
       " \n",
       " [2646 rows x 6 columns],\n",
       "                                                Sentence  \\\n",
       " 1638                                 She's a communist.   \n",
       " 2174  There was he was never going to get over forty...   \n",
       " 2209  And now the Trump campaign is practically maki...   \n",
       " 3487  The other thing you hear, at least for some of...   \n",
       " 304   More of these said groups are going to move, s...   \n",
       " ...                                                 ...   \n",
       " 1410  Now, after their Pennsylvania debut, Harris an...   \n",
       " 2796                           He insulted a lot of us.   \n",
       " 497   Ever, and he sees Kamala Harris as more libera...   \n",
       " 3612  She's got receipts on the issues that matter t...   \n",
       " 2793  Obviously, there's context and different facto...   \n",
       " \n",
       "      Presidential_Candidate         State  Vote_1  Vote_2  Final_Sent  \n",
       " 1638          Kamala Harris  Pennsylvania      -1       0          -1  \n",
       " 2174           Donald Trump      Michigan      -1       0          -1  \n",
       " 2209           Donald Trump      Michigan       1       0           1  \n",
       " 3487          Kamala Harris      Michigan       1       1           1  \n",
       " 304           Kamala Harris  Pennsylvania       1       1           1  \n",
       " ...                     ...           ...     ...     ...         ...  \n",
       " 1410          Kamala Harris  Pennsylvania       0      -1           0  \n",
       " 2796           Donald Trump      Michigan      -1      -1          -1  \n",
       " 497           Kamala Harris  Pennsylvania       0       1           0  \n",
       " 3612          Kamala Harris      Michigan       1       0           1  \n",
       " 2793           Donald Trump      Michigan       1      -1           1  \n",
       " \n",
       " [378 rows x 6 columns],\n",
       "                                                Sentence  \\\n",
       " 838   The Harris Waltz campaign is doing a Labor Day...   \n",
       " 2615  Do you think he's going to win the state of Mi...   \n",
       " 871   This energy and your black filling, look at th...   \n",
       " 2802      Hey hey, ho ho, harrison Trump has got to go.   \n",
       " 1188  She rallied voters in Erie, telling the crowd ...   \n",
       " ...                                                 ...   \n",
       " 3502    She says she's not different from Biden at all.   \n",
       " 1426             She does seem to be an outside figure.   \n",
       " 1765  I know Trump went down some very seemingly rid...   \n",
       " 108   It's crazy, he was probably waiting outside Wh...   \n",
       " 3326  Kamala has tried to reassure this aspect of th...   \n",
       " \n",
       "      Presidential_Candidate         State  Vote_1  Vote_2  Final_Sent  \n",
       " 838           Kamala Harris  Pennsylvania       0       1           0  \n",
       " 2615           Donald Trump      Michigan       0       1           0  \n",
       " 871            Donald Trump  Pennsylvania       1       1           1  \n",
       " 2802           Donald Trump      Michigan      -1       0          -1  \n",
       " 1188          Kamala Harris  Pennsylvania       0       1           0  \n",
       " ...                     ...           ...     ...     ...         ...  \n",
       " 3502          Kamala Harris      Michigan      -1       0          -1  \n",
       " 1426          Kamala Harris  Pennsylvania       1       0           0  \n",
       " 1765           Donald Trump      Michigan      -1      -1          -1  \n",
       " 108            Donald Trump  Pennsylvania      -1       1           1  \n",
       " 3326          Kamala Harris      Michigan       0       1           0  \n",
       " \n",
       " [756 rows x 6 columns])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Splitting\n",
    "# Load the dataset from CSV file\n",
    "df = pd.read_csv('annotated_dataset.csv')\n",
    "# Split data into 80% training+validation and 20% test\n",
    "remaining, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# Split the remaining 80% into 70% training and 10% validation (0.125 of 80% = 10% overall)\n",
    "train, val = train_test_split(remaining, test_size=0.125, random_state=42)\n",
    "\n",
    "train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad121e53-d2dc-4afe-bf3a-25dfa335307f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0569, 1.0804, 0.8626])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the model and compute class weights\n",
    "def compute_class_weights(labels):\n",
    "    \"\"\"\n",
    "    Calculate weights for each class to handle imbalanced data\n",
    "    For example, if we have 100 positive but only 10 negative samples,\n",
    "    negative samples will get higher weight to balance their importance\n",
    "    \"\"\"\n",
    "    # Shift labels for model [-1, 0, 1] to [0, 1, 2]\n",
    "    mapped_labels = labels + 1\n",
    "    # Count how many samples we have of each class\n",
    "    class_counts = np.bincount(mapped_labels)\n",
    "    # Give higher weights to classes with fewer samples\n",
    "    weights = 1. / class_counts\n",
    "    # Normalize weights to sum to number of classes\n",
    "    weights = weights * len(class_counts) / weights.sum()\n",
    "    return torch.FloatTensor(weights)\n",
    "\n",
    "# Calculate weights for each class from training data\n",
    "class_weights = compute_class_weights(train[sentiment_column_name].values)\n",
    "class_weights = class_weights.to(device)  # Move weights to GPU if available\n",
    "\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de75a40e-3da8-4256-b57b-deb4fe2a1d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152f332f92854a48868f453b22f54e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI Laptop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MSI Laptop\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c741d1596545acb78c50dc419bf165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5157a7045cca4e858fe19bb337349039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c86a56bced643d0a5fd58b40ced5ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizer(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base DistilBERT model to use\n",
    "model_name = 'distilbert-base-uncased'\n",
    "\n",
    "# Create a custom DistilBERT model that can handle weighted loss\n",
    "class DistilBertWithWeightedLoss(DistilBertForSequenceClassification):\n",
    "    \"\"\"\n",
    "    Custom DistilBERT model that applies different weights to each class\n",
    "    This helps handle imbalanced datasets better\n",
    "    \"\"\"\n",
    "    def __init__(self, config, class_weights):\n",
    "        super().__init__(config)\n",
    "        self.class_weights = class_weights # Store class weights for loss calculation\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Get model outputs without computing loss\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=None # Set to None to prevent automatic loss calculation\n",
    "        )\n",
    "        \n",
    "        # Calculate weighted loss if labels are provided (training phase)\n",
    "        if labels is not None:\n",
    "            # Create loss function with class weights\n",
    "            loss_fct = CrossEntropyLoss(weight=self.class_weights)\n",
    "            # Calculate loss using model predictions and true labels\n",
    "            loss = loss_fct(\n",
    "                outputs.logits.view(-1, self.num_labels),  # Reshape predictions\n",
    "                labels.view(-1)                            # Reshape labels\n",
    "            )\n",
    "            outputs.loss = loss  # Add loss to outputs\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# Initialize the tokenizer that will convert text to numbers\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8db2eb85-9f69-4ac2-97f0-b5b404ce51ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation Functions\n",
    "def create_data_loader(data, tokenizer, batch_size):\n",
    "    \"\"\"\n",
    "    Convert text data into a format DistilBERT can understand and create batches\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame containing text and labels\n",
    "        tokenizer: DistilBERT tokenizer to convert text to numbers\n",
    "        batch_size: How many samples to process at once\n",
    "\n",
    "    Returns:\n",
    "        DataLoader that yields batches of processed data\n",
    "    \"\"\"\n",
    "    # Convert text to DistilBERT input format with progress bar\n",
    "    encodings = tokenizer(\n",
    "        data[sentence_column_name].tolist(), # Convert sentences to list\n",
    "        truncation=True, # Cut texts longer than max_length\n",
    "        padding=True, # Pad texts shorter than max_length\n",
    "        max_length=128, # Maximum sequence length\n",
    "        return_tensors='pt', # Return PyTorch tensors\n",
    "        verbose=True # Show progress\n",
    "    )\n",
    "\n",
    "    # Create dataset by combining inputs and labels\n",
    "    dataset = torch.utils.data.TensorDataset(\n",
    "        encodings['input_ids'], # Tokenized text\n",
    "        encodings['attention_mask'], # Attention mask for padding\n",
    "        torch.tensor(data[sentiment_column_name].tolist()) # Labels\n",
    "    )\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdcdbb7d-55b2-4d28-8b6e-7cc1b40a6f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate model performance using various metrics\n",
    "    \n",
    "    Args:\n",
    "        model: The DistilBERT model to evaluate\n",
    "        data_loader: DataLoader containing validation or test data\n",
    "        device: CPU or GPU\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing various performance metrics\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad(): # Don't compute gradients during evaluation\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluation\"):\n",
    "            # Move batch to GPU if available\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            adjusted_labels = labels + 1 # Shift labels for model\n",
    "\n",
    "            # Get model predictions\n",
    "            outputs = model(input_ids, attention_mask, labels=adjusted_labels)\n",
    "            val_loss += outputs.loss.item()\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            all_preds.extend((predicted - 1).cpu().numpy())\n",
    "            all_labels.extend((adjusted_labels - 1).cpu().numpy())\n",
    "\n",
    "    # Calculate various performance metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'loss': val_loss / len(data_loader),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "794bb0ab-7b68-408e-830c-60e6ad40c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, device, epochs, learning_rate):\n",
    "    \"\"\"\n",
    "    Train the model and periodically evaluate its performance\n",
    "    \n",
    "    Args:\n",
    "        model: The DistilBERT model to train\n",
    "        train_loader: DataLoader with training data\n",
    "        val_loader: DataLoader with validation data\n",
    "        device: CPU or GPU\n",
    "        epochs: Number of times to process all training data\n",
    "        learning_rate: How quickly the model should learn\n",
    "    \n",
    "    Returns:\n",
    "        Trained model and its best validation metrics\n",
    "    \"\"\"\n",
    "    # Initialize optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    best_metrics = None\n",
    "    best_model = None\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # Set model to training mode\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Process each batch\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs} Training\")\n",
    "        for batch in pbar:\n",
    "            # Move batch to GPU if available\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            adjusted_labels = labels + 1 # Shift labels for model [-1, 0, 1] to [0, 1, 2]\n",
    "\n",
    "            # Training step\n",
    "            optimizer.zero_grad() # Clear previous gradients\n",
    "            outputs = model(input_ids, attention_mask, labels=adjusted_labels) # Forward pass\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item() # Accumulate loss\n",
    "\n",
    "            # Update model weights\n",
    "            loss.backward() # Backward pass\n",
    "            optimizer.step() # Update weights\n",
    "\n",
    "            # Update progress bar with current loss\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Calculate average loss for this epoch\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Average training loss: {avg_train_loss:.4f}')\n",
    "        \n",
    "        val_metrics = evaluate_model(model, val_loader, device)\n",
    "        print(f'Validation metrics: {val_metrics}')\n",
    "        \n",
    "        # Update best parameters if accuracy score improves\n",
    "        if best_metrics == None or val_metrics['accuracy'] > best_metrics['accuracy']:\n",
    "            best_metrics = val_metrics\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model, best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "833e77d3-a686-4869-a66f-92e62db0c592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different values to try for each parameter\n",
    "param_grid = {\n",
    "    'learning_rate': [2e-5, 5e-5], # DistilBERT works well with similar learning rates as BERT\n",
    "    'batch_size': [32], # Same batch size as BERT works well\n",
    "    'epochs': [5] # DistilBERT might converge faster due to its smaller size\n",
    "}\n",
    "\n",
    "# Hyperparameter Tuning Function\n",
    "def hyperparameter_tuning(train_data, val_data, device, class_weights):\n",
    "    \"\"\"\n",
    "    Try different combinations of hyperparameters to find the best ones\n",
    "    \n",
    "    Args:\n",
    "        train_data: Training data DataFrame\n",
    "        val_data: Validation data DataFrame\n",
    "        device: CPU or GPU\n",
    "        class_weights: Weights for each class\n",
    "    \n",
    "    Returns:\n",
    "        Best parameters and their corresponding metrics\n",
    "    \"\"\"    \n",
    "    # Create all possible combinations of parameters\n",
    "    param_combinations = [\n",
    "        dict(zip(param_grid.keys(), v)) \n",
    "        for v in itertools.product(*param_grid.values())\n",
    "    ]\n",
    "\n",
    "    best_model = None\n",
    "    best_metrics = None\n",
    "    best_params = None\n",
    "\n",
    "    # Try each combination of parameters\n",
    "    for params in param_combinations:\n",
    "        print(f\"\\nTrying parameters: {params}\")\n",
    "\n",
    "        # Create data loaders with current batch size\n",
    "        train_loader = create_data_loader(train_data, tokenizer, params['batch_size'])\n",
    "        val_loader = create_data_loader(val_data, tokenizer, params['batch_size'])\n",
    "\n",
    "        # Initialize the custom DistilBERT model\n",
    "        model = DistilBertWithWeightedLoss.from_pretrained(\n",
    "            model_name,\n",
    "            # Configure DistilBERT for classification\n",
    "            config=DistilBertForSequenceClassification.from_pretrained(\n",
    "                model_name,\n",
    "                num_labels=3,\n",
    "                output_attentions=False, # Don't output attention weights\n",
    "                output_hidden_states=False, # Don't output hidden states\n",
    "            ).config,\n",
    "            class_weights=class_weights\n",
    "        )\n",
    "        # Move model to GPU if available\n",
    "        model.to(device)\n",
    "\n",
    "        # Train model with current parameters\n",
    "        model, val_metrics = train_model(\n",
    "            model, \n",
    "            train_loader, \n",
    "            val_loader,\n",
    "            device,\n",
    "            params['epochs'],\n",
    "            params['learning_rate']\n",
    "        )\n",
    "\n",
    "        # Update best parameters if accuracy score improves\n",
    "        if best_metrics == None or val_metrics['accuracy'] > best_metrics['accuracy']:\n",
    "            best_model = model\n",
    "            best_params = params\n",
    "            best_metrics = val_metrics\n",
    "    \n",
    "    return best_model, best_params, best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ea6f3b5-7c61-4d55-88dc-06d71c3d4db0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying parameters: {'learning_rate': 2e-05, 'batch_size': 32, 'epochs': 5}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d274f889b8492e95003e0557cbb852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertWithWeightedLoss were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\MSI Laptop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/5 Training: 100%|██████████████████████████████████████████████████| 83/83 [06:35<00:00,  4.76s/it, loss=0.617]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Average training loss: 0.9748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████| 12/12 [00:12<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'loss': 0.8230836888154348, 'accuracy': 0.6375661375661376, 'precision': 0.637078456522901, 'recall': 0.6375661375661376, 'f1': 0.6149331550447501}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 Training: 100%|██████████████████████████████████████████████████| 83/83 [06:33<00:00,  4.74s/it, loss=0.555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Average training loss: 0.7155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████| 12/12 [00:11<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'loss': 0.6955294162034988, 'accuracy': 0.6984126984126984, 'precision': 0.6895134641302094, 'recall': 0.6984126984126984, 'f1': 0.6883485616098476}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 Training: 100%|██████████████████████████████████████████████████| 83/83 [06:31<00:00,  4.71s/it, loss=0.518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Average training loss: 0.4995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████| 12/12 [00:11<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'loss': 0.6915995875994364, 'accuracy': 0.7116402116402116, 'precision': 0.707520732256182, 'recall': 0.7116402116402116, 'f1': 0.7088566923039965}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 Training: 100%|██████████████████████████████████████████████████| 83/83 [06:35<00:00,  4.76s/it, loss=0.303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Average training loss: 0.3339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████| 12/12 [00:11<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'loss': 0.8327804307142893, 'accuracy': 0.6984126984126984, 'precision': 0.6931015963259949, 'recall': 0.6984126984126984, 'f1': 0.6896182944526121}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 Training: 100%|█████████████████████████████████████████████████| 83/83 [06:38<00:00,  4.80s/it, loss=0.0448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Average training loss: 0.2051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████| 12/12 [00:11<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'loss': 0.8988261272509893, 'accuracy': 0.6957671957671958, 'precision': 0.6926488675392184, 'recall': 0.6957671957671958, 'f1': 0.6927282976972418}\n",
      "\n",
      "Trying parameters: {'learning_rate': 5e-05, 'batch_size': 32, 'epochs': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertWithWeightedLoss were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\MSI Laptop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/5 Training: 100%|██████████████████████████████████████████████████| 83/83 [06:58<00:00,  5.04s/it, loss=0.559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Average training loss: 0.9082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████| 12/12 [00:11<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'loss': 0.7382264683643976, 'accuracy': 0.6693121693121693, 'precision': 0.6638640465079928, 'recall': 0.6693121693121693, 'f1': 0.6639456513316695}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 Training: 100%|██████████████████████████████████████████████████| 83/83 [06:54<00:00,  4.99s/it, loss=0.595]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Average training loss: 0.5844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████| 12/12 [00:12<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'loss': 0.7595544954140981, 'accuracy': 0.6746031746031746, 'precision': 0.6994809299781407, 'recall': 0.6746031746031746, 'f1': 0.6812595837668226}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 Training: 100%|███████████████████████████████████████████████████| 83/83 [06:54<00:00,  4.99s/it, loss=0.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Average training loss: 0.3508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████| 12/12 [00:11<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'loss': 0.7778427749872208, 'accuracy': 0.716931216931217, 'precision': 0.7128518651080411, 'recall': 0.716931216931217, 'f1': 0.7101108896408577}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 Training: 100%|█████████████████████████████████████████████████| 83/83 [06:56<00:00,  5.02s/it, loss=0.0198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Average training loss: 0.1814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████| 12/12 [00:11<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'loss': 0.9560512825846672, 'accuracy': 0.701058201058201, 'precision': 0.7094646395984178, 'recall': 0.701058201058201, 'f1': 0.7040260695556704}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 Training: 100%|█████████████████████████████████████████████████| 83/83 [06:58<00:00,  5.04s/it, loss=0.0119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Average training loss: 0.1006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████| 12/12 [00:12<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'loss': 1.1578800280888875, 'accuracy': 0.7037037037037037, 'precision': 0.7046887879360371, 'recall': 0.7037037037037037, 'f1': 0.7008976250432833}\n",
      "\n",
      "Best parameters: {'learning_rate': 5e-05, 'batch_size': 32, 'epochs': 5}\n",
      "Best validation metrics: {'loss': 0.7778427749872208, 'accuracy': 0.716931216931217, 'precision': 0.7128518651080411, 'recall': 0.716931216931217, 'f1': 0.7101108896408577}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertWithWeightedLoss(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Hyperparameter Tuning\n",
    "best_model, best_params, best_metrics = hyperparameter_tuning(train, val, device, class_weights)\n",
    "print(f\"\\nBest parameters: {best_params}\")\n",
    "print(f\"Best validation metrics: {best_metrics}\")\n",
    "\n",
    "# Save the best model\n",
    "best_model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa94be6a-7a4d-48c5-b580-801161d8fa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Time\n",
    "# 06:35 00:12\n",
    "# 06:33 00:11\n",
    "# 06:31 00:11\n",
    "# 06:35 00:11\n",
    "# 06:38 00:11\n",
    "\n",
    "# 06:58 00:11\n",
    "# 06:54 00:12\n",
    "# 06:54 00:11\n",
    "# 06:56 00:11\n",
    "# 06:58 00:12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4766f98c-5bde-44b7-ae75-9574d6cef804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating final model on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████| 24/24 [00:34<00:00,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set metrics: {'loss': 1.1459480722745259, 'accuracy': 0.6917989417989417, 'precision': 0.6900590897555074, 'recall': 0.6917989417989417, 'f1': 0.6902151615550367}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Final Evaluation on Test Set\n",
    "test_loader = create_data_loader(test, tokenizer, best_params['batch_size'])\n",
    "print(\"\\nEvaluating final model on test set...\")\n",
    "test_metrics = evaluate_model(best_model, test_loader, device)\n",
    "print(f\"Test set metrics: {test_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60e7e3ce-d3ef-4a0c-b7af-ccf31a631aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class (-1, 0, 1): 1\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "best_model.eval()\n",
    "\n",
    "# Example text for prediction\n",
    "example_text = \"harris leads by 1%\"\n",
    "\n",
    "# Tokenize the input text\n",
    "encoded_input = tokenizer(\n",
    "    example_text,\n",
    "    return_tensors=\"pt\", # Return PyTorch tensors\n",
    "    truncation=True,\n",
    "    padding=True\n",
    ")\n",
    "# Remove token_type_ids if not used\n",
    "encoded_input.pop(\"token_type_ids\", None)\n",
    "\n",
    "# Perform prediction without gradient computation\n",
    "with torch.no_grad():\n",
    "    outputs = best_model(**encoded_input)\n",
    "\n",
    "# Get the logits from the model's output\n",
    "logits = outputs.logits\n",
    "\n",
    "# Get the predicted class (0, 1, or 2)\n",
    "predicted_class = torch.argmax(logits, dim=1).item() - 1\n",
    "print(\"Predicted class (-1, 0, 1):\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b1ca08-6cb4-47ee-b4c9-c1f8992b76bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
